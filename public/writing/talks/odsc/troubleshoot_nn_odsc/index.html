<!DOCTYPE html>
<html lang="en-us">

<head>
    
    

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Trouble shooting Deep Neural Networks</title>
    
    <style>

    html body {
        font-family: 'Ubuntu', sans-serif;
        background-color: white;
    }

    :root {
        --accent: Indigo;
        --border-width:  5px ;
    }

</style>


<link rel="stylesheet" href="https://judahgnewman.netlify.com/css/main.css">





<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu">


 <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"> 


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
 

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/go.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/haskell.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/kotlin.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/swift.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
    
    <script>hljs.initHighlightingOnLoad();</script>






<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<script>$(document).on('click', function() { $('.collapse').collapse('hide'); })</script>
 <meta name="generator" content="Hugo 0.73.0" />
    

    

    
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

    


</head>

<body>
    

    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container">
            <div class="navbar-header">
                <a class="navbar-brand visible-xs" href="#">Trouble shooting Deep Neural Networks</a>
                <button class="navbar-toggle" data-target=".navbar-collapse" data-toggle="collapse">
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
            </div>
            <div class="collapse navbar-collapse">
                
                <ul class="nav navbar-nav">
                    
                    <li><a href="/">Home</a></li>
                    
                    <li><a href="/about/">About</a></li>
                    
                    <li><a href="/writing/">Writing</a></li>
                    
                    <li><a href="/reading/">Reading</a></li>
                    
                    <li><a href="/research/">Research</a></li>
                    
                </ul>
                
                
                <ul class="nav navbar-nav navbar-right">
                    
                    <li class="navbar-icon"><a href="mailto:judah.newman@gmail.com"><i class="fa fa-envelope-o"></i></a></li>
                    
                    <li class="navbar-icon"><a href="https://github.com/jgnewman96/"><i class="fa fa-github"></i></a></li>
                    
                    <li class="navbar-icon"><a href="https://www.linkedin.com/in/jgnewman96/"><i class="fa fa-linkedin"></i></a></li>
                    
                </ul>
                
            </div>
        </div>
    </nav>

<main>

    <div>
        <h2>Trouble shooting Deep Neural Networks</h2>
        <h5></h5>
        <h7 class="section-heading">
            
<a href="https://judahgnewman.netlify.com/tags/deep-learning"><kbd class="item-tag">Deep Learning</kbd></a>


<a href="https://judahgnewman.netlify.com/categories/talks"><kbd class="item-category">Talks</kbd></a>

            
            
            
            <br>
            <i class="far fa-clock"></i> Reading Time: 8 minutes
    </div>




    <div align="start" class="content"><p>Talk given by <a href="http://josh-tobin.com/">Josh Tobin</a> works at Open Ai and <a href="https://fullstackdeeplearning.com/">Full stack Deep Learning</a>. <a href="http://josh-tobin.com/troubleshooting-deep-neural-networks.html">Talk Resources</a></p>
<h2 id="motivation">Motivation</h2>
<p>Troubleshooting neurel nets is hardest part of building a deep model. Even the best practitioners spend a long time trouble shooting. Josh argues that the vast majority of what the best practitioners do to troubleshoot their models can be broken down to a decision tree. During this talk he presents that decision tree. Generally, 80-90% of our time goes into debugging and tuning while only 10-20 percent is deriving math or building implemnations. Troubleshooting is defined as helping your model achieve optimal performance.</p>
<p>But why is troubleshooting Deep Learning so hard? Imagine that you are trying to replicate the results from a paper, but for some reason the lose you are achieving is worse than what the paper reports. It is very difficult to tell if you have a bug and where that bug might be. In software engineering bugs are generally quite loud. You will know immediately when something is breaking. In deep learning though, bugs are essentially silent. There are many possible causes of the error. It could be a case of simply having the wrong hyper parameters. Model performance can be very sensitive to hyper parameters.</p>
<h2 id="strategy-for-deep-learning-troubleshooting">Strategy for Deep Learning Troubleshooting</h2>
<p>Before we delve into the decision tree to follow, there is a certain mindset we need to have in mind when we are troubleshooting Deep Learning. We need to have a pessimistic mindet. It is going to be very difficult to disambiguate errors. We should not assume that things are going to work. By being pessimistic we are going to start as simple as possible, and then slowly ramp up complexity. By starting simple and slowly making changes we are able to understand why is our performance changing.</p>
<h3 id="start-simple">Start Simple</h3>
<p>The first step in starting simple is to choose a simple architecture.</p>
<ul>
<li>If our input is images → start with le net then move to res net</li>
<li>If our input is sequences → start with a LSTM and then slowly move to transformer</li>
<li>If our input is something else → start with a fully connected neural net with one hidden layer, where you go after that is problem dependent</li>
</ul>
<p>But what about multiple inputs? → For example two images and sentence</p>
<ul>
<li>First map each into a lower dimenstional feature space</li>
<li>For images conv net and sentence LSTM</li>
<li>Then you flatten the outputs and concatenate them</li>
<li>Finally send it through a fully connected layer to get an ouput</li>
</ul>
<p>Some other good principles to starting simple</p>
<ul>
<li>
<p>Use sensible defaults</p>
</li>
<li>
<p>Normalize inputs: do not pass raw images, substract the mean and divide by variance</p>
</li>
<li>
<p>Consider simplyfing the problem</p>
<ul>
<li>use a subset of you&rsquo;re data set</li>
<li>if you have many classes just start with a couple</li>
<li>create a simpler synthetic training set</li>
</ul>
</li>
</ul>
<h3 id="implement-and-then-debug">Implement and then Debug</h3>
<p>The first step is really just getting your model to run. Here is a list of the most common bugs that might break your model at the very beginning.</p>
<ul>
<li>Incorrect shapes for tensors: A lot of libraries will silently broadcast tensor without your knowledge</li>
<li>Preprocessing your inputs incorrectly</li>
<li>Incorrect input to your loss function → softmaxed outputs to a loss that expecits logits</li>
<li>Forgetting to set up train mode for the net correctly
<ul>
<li>toggling training / evaluation with batch normalization depedencies</li>
</ul>
</li>
<li>Numerical instability → inf/Nan
<ul>
<li>often stems from using an exp, log or div operation. Use off the shelf softmax rather than writing your own</li>
</ul>
</li>
</ul>
<p>Some general tips</p>
<ul>
<li>Use a light weight implementation: You should have the minimum number of possible line of code for v1. Ideally less than 200 lines of code. Each additional line of code is new chance for bugs</li>
<li>Use of the shelf components when you can</li>
<li>Build complicated data pipelines later in your development process. You should not spend time doing this when you are still debugging. Begin with a dataset that you can load into memory.</li>
</ul>
<p>Try to see if you can overfit a single batch. This will help catch a very large number of bugs. You should be able to the drive the error of your model to zero on a single batch. If you are unable to do this, it will help you find the source of potential bugs.</p>
<p>It is helpful to compare your results to known results. You might have an error if your model is performing worse than the best known results. Ideally you could compare your model with an official model implemntation evaluated on a similar dataset to yours. This would even allow you to compare your code with their code. A less ideal solution would be to compare with an unoffical model implementation on github. But be wary, lots of repos will have bugs in them. Worst case you can compare your results to the results from a paper with no code associated with it. It is also always good to compare with super simple baselines. How does your model compare to the average of outputs, or a linear regression?</p>
<h3 id="evaluate-model-then-decide-what-to-do-next">Evaluate model, then decide what to do next</h3>
<p>A good framework for doing model evaluation is thinking in terms of the bias variance decomposition. This allows you to think about underfitting and overfitting. We can look at the gap between our performance on our training set, our validation set and our test set. The gap between our training set and our validation set is how much our model is overfitting. Test error can be broken down into the following components</p>
<blockquote>
<p>test error = irreducible error + bias + variance + distribution shift + val overfitting</p>
</blockquote>
<p>By breaking error into its parts we can identify what is causing our error and prioritize our improvements. Distribution shift happens when our training or validation data comes from a different distribution than our test set. For example our training data could come from the day time and our test could be from the night time. We can handle this by using two validation sets. One validation set from our training distribution and one from the test data. The difference in our error between these two validation sets is our error due to distribution shift.</p>
<h3 id="improve-model-or-data">Improve model or data</h3>
<p>The first place is to address under-fitting. Here are some things to try ranked from what you should try first to what you should try last.</p>
<ul>
<li>make your model bigger, wider or deeper</li>
<li>reduce regularization</li>
<li>do error anlysis</li>
<li>choose a different model architecture</li>
<li>tune hyper parameters</li>
<li>add features</li>
</ul>
<p>Second you shoud address over-fitting. Once again a list of what you should try from first to last</p>
<ul>
<li>add more training data</li>
<li>add normalization</li>
<li>do data augmentation</li>
<li>add regularization</li>
<li>do an error analysis</li>
<li>choose a different model architecture</li>
<li>tune hyper parameters</li>
<li>Use early stopping</li>
<li>Remove features</li>
<li>Reduce Model size</li>
</ul>
<p>After you have addressed over and underfitting you should move on to distribution shift. One note about over and under fitting, it is possible that combating one will exacerbate the other. You should not just treat them as isolation. You might fix underfitting then do some things to fix overfitting and then need to do some more to fix underfitting. Below is a list for what to try ranked from first to last to address distribution shift.</p>
<ul>
<li>do an error analysis: look at specific errors and prioritize places for more specific data collection</li>
<li>Synthesize more data rather than collecting it</li>
<li>Domain adaptation techniques. This is a new area of research that is not quite advanced enough</li>
</ul>
<p>So how do we do an error analysis? We should look at our errors on the train-validation set and the test-validation set. We can go through a set of errors and manually classify them into different categories. We can then hopefully understand what is driving our errors and how many come from each category. We want to understand how much error does each category introduce and how costly are the possible solutions to fix it. We can then prioritize, if we want to collect more data.</p>
<p>It is important to ocassionaly re-balance your datasets. Switch what is your validation set and every so often check up on the held out test set.</p>
<h3 id="tuning-hyper-parameters">Tuning Hyper Parameters</h3>
<p>There are so many differeny hyper parameters that tuning them can be quite daunting and difficult. You should first start with learning rate and batch size. These have been shown to be two of the more important hyper parameters. If this does not get you where you want then look at the others. Rather than doing a grid search you should do a coarse-to-fine random search. Begin by looking in a large range and then narrow in on smaller regions where you are getting best performance. You can also consider bayesian hyper-parameter optimization but this can be hard to implement and you should only invest in it as your code base matures.</p>
<h3 id="conclusion">Conclusion</h3>
<p>So why is trouble shooting so hard? Because there are many competing sources of error. We have to be pessimistic in our approach. It should be an iterative process where we start as simple as possible and then add complexity. Following the steps that were discussed in this talk should make it easier.</p>
<h2 id="my-thoughts">My thoughts</h2>
<p>Wow, this talk had such a good structure. It was broken down into itemized parts that were easily digestiable. This talk presented not just a good way to troubleshoot deep learning but a good framework for problem solving in general. Start simple and slowly add in complexity. Break the problem down into different parts.</p>
<p>In his presentation he essentially used topic sentences and transition sentences. At the beginning of every section was an introduction and at the end was a summary. He had one underlying example that he returned to trhough out the talk to ground his higher level theory in a concerete example.</p>
<p>This talk and the one before it mafe me reflect that we should have more transparency into the traning process of models. It should be easier to replicate them. People should report common bugs in development and how to solve them.</p>
</div>

    <script src="https://utteranc.es/client.js" repo="jgnewman96/website" issue-term="title" theme="github-light"
    crossorigin="anonymous" async>
    </script>


</main>





<footer>
    <p class="copyright text-muted">
        © 2019 - 2020 Judah Newman,  All opinions are my own. Theme adopted from <a href="https://themes.gohugo.io/minimal/">Minimal</a>. Powered by <a href="https://gohugo.io">Hugo</a>
    </p>
</footer>



</body>


</html>