<!DOCTYPE html>
<html lang="en-us">

<head>
    
    

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Improving Fairness in Machine Learning Systems: What Do Industry Practitioners Need?</title>
    
    <style>

    html body {
        font-family: 'Ubuntu', sans-serif;
        background-color: azure;
    }

    :root {
        --accent: darkolivegreen;
        --border-width:  5px ;
    }

</style>


<link rel="stylesheet" href="https://judahgnewman.netlify.com/css/main.css">





<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu">


 <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"> 


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
 

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/go.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/haskell.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/kotlin.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/swift.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
    
    <script>hljs.initHighlightingOnLoad();</script>






<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<script>$(document).on('click', function() { $('.collapse').collapse('hide'); })</script>
 <meta name="generator" content="Hugo 0.73.0" />
    

    


    


</head>

<body>
    

    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container">
            <div class="navbar-header">
                <a class="navbar-brand visible-xs" href="#">Improving Fairness in Machine Learning Systems: What Do Industry Practitioners Need?</a>
                <button class="navbar-toggle" data-target=".navbar-collapse" data-toggle="collapse">
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
            </div>
            <div class="collapse navbar-collapse">
                
                <ul class="nav navbar-nav">
                    
                    <li><a href="/">Home</a></li>
                    
                    <li><a href="/about/">About</a></li>
                    
                    <li><a href="/writing/">Writing</a></li>
                    
                    <li><a href="/reading/">Reading</a></li>
                    
                    <li><a href="/research/">Research</a></li>
                    
                </ul>
                
                
                <ul class="nav navbar-nav navbar-right">
                    
                    <li class="navbar-icon"><a href="mailto:judah.newman@gmail.com"><i class="fa fa-envelope-o"></i></a></li>
                    
                    <li class="navbar-icon"><a href="https://github.com/jgnewman96/"><i class="fa fa-github"></i></a></li>
                    
                    <li class="navbar-icon"><a href="https://www.linkedin.com/in/jgnewman96/"><i class="fa fa-linkedin"></i></a></li>
                    
                </ul>
                
            </div>
        </div>
    </nav>

<main>

    <div>
        <h2>Improving Fairness in Machine Learning Systems: What Do Industry Practitioners Need?</h2>
        <h5></h5>
        <h7 class="section-heading">
            
<a href="https://judahgnewman.netlify.com/tags/data-science"><kbd class="item-tag">Data Science</kbd></a>


<a href="https://judahgnewman.netlify.com/categories/papers"><kbd class="item-category">Papers</kbd></a>

            
            
            
            <br>
            <i class="far fa-clock"></i> Reading Time: 6 minutes
    </div>




    <div align="start" class="content"><p>By Kenneth Holsten, <a href="http://www.jennwv.com/">Jennifer Wortman Vaughn</a>, <a href="http://users.umiacs.umd.edu/~hal/">Hal Daume III</a>, <a href="https://www.microsoft.com/en-us/research/people/mdudik/">Miroslav Dudik</a> and <a href="http://dirichlet.net/">Hanna Wallach</a></p>
<p><a href="https://arxiv.org/pdf/1812.05239.pdf">Paper Link</a></p>
<h2 id="paper-motivation">Paper Motivation</h2>
<p>There has been a recent surge in research developing tools to mitigate harms of machine learning. For these tools to have an impact they must actually meet industry practitioners needs</p>
<h2 id="paper-contribution">Paper Contribution</h2>
<p>The paper outlines commercial teams‘ challenges and needs when developing fairer ML systems.  The authors outline where there is disconnect between the current research and what people actually need. The papers findings come from interviews of 35 practitioners and survey of 267 different practitioners.</p>
<h2 id="background">Background</h2>
<p>The current tools that are being created for ML practitioners to address challenges with bias are driven by algorithmic methods rather than real world needs. For tools to have an impact they must be driven by a deep understanding of the actual challenges engineers face when developing fairer ML systems. The goal of this work is to identify future opportunities for research and how to make that research align with engineer&rsquo;s actual needs</p>
<h3 id="methods">Methods</h3>
<p>The authors conducted 35 one-on-one interviews with ML engineers across 10 major companies. They then followed up the interview with a survey of 267 engineers to see if their themes generalized</p>
<p>The authors spend a fair chunk of the paper explaining the entire process of how they recruited subjects and how they conducted interviews. I will not replicate all of the information here. The one thing I do want to highlight is how the authors asked questions to understand interviewees ideal state of the world. Once they understood the ideal state of the world they next honed in on the current state of the world. This way of questioning allowed them to find where the most opportunities for improvement are.</p>
<p><strong>Fairness-aware Data Collection</strong></p>
<p>The majority of ML research on fairness has focused on algorithms treating the dataset as fixed. Many teams actually have the ability to change their datasets and that can be a fruitful starting point when thinking about fairness.Lots of the people they interviewed stated that they would try to collect more data to tackle fairness problems.Currently interviewees report not much thought is put into which data is ingested. If the data is there, it is going to be ingested. Interviewees express the desire for tools that better facilitate the collection of data in a fair way. Many engineers also specified the role of having a good test set as being important to capture bias. Tools to ensure that test sets covered all populations equally or showed errors in test sets by subgroups would be helpful.</p>
<p><strong>Challenges due to Blind Spots</strong></p>
<p>Interviewees highlighted not knowing where they should be looking for bias or which sub groups should be looked at. Lots of interviewees said it would be helpful to have additional support identifying relative sub populations.  Often blind spots are shown after a product has already been deployed and an outsider finds the blind spot. It would be helpful to have tools to help detect these blind spots before deployment. A useful tool could be one that facilitates knowledge and sharing across team or company boundaries.</p>
<p><strong>Needs for More Proactive Auditing Processes</strong>
Current auditing processes are often reactive after an issue has been raised rather than before a product is deployed. People are not currently rewarded in their companies for addressing issues related to fairness. Fairness is not a priority for them. Fairness needs to be incorporated as part of the regular workflow of model development</p>
<p>One difficulty that came up over and over again is how fairness is so context dependent. This makes it hard to actually take knowledge from other places often. Getting fairness right takes a lot of time and work and in companies that work is not valued.</p>
<p><strong>Needs for More Holistic Auditing Methods</strong></p>
<p>Fairness is often a system level property rather than just applicable to one model. In complicated systems it can be much harder to find fairness problems.Fairness can be thought of as monitoring systems for real world harm rather than just monitoring one model for unfairness. This task is much more complicated and much more difficult. Simulation based systems for understanding complex interactions would be helpful.</p>
<p><strong>Addressing detected Issues</strong></p>
<p>Even once an issue has been identified it is not always easy to come up with a remedy. What would be the best approach, switching to a different model? or collecting more / different types of data?. For different contexts or different problems the best approach can be quite different. When teams have decided to collect more data, how much more data is sufficient for solving the problem?</p>
<p><strong>Biases in the Humans in the Loop</strong></p>
<p>The humans working on developing these models might have biases inherent in themselves that bleed through to the system</p>
<h3 id="my-closing-thoughts">My closing thoughts</h3>
<p>This paper highlights so many opportunities to improve fairness practices for machine learning in industry. This is the level where fairness interventions will often make a difference. Writing a research paper about fairness might not actually make any systems more equitable. Creating tools that are easy for people to use that help detect bias and give them solutions to correct that bias will have a large impact. This paper does a great job of providing a huge number of different directions for research.</p>
<p>There is a lot of research about how it can be problematic to intervene in a community without actually understanding the context or the people in that community. To some extent, that is what fairness researchers do. They came up with a solution or a direction that does not actually recognize or help the needs of practitioners. While this type of research is not sexy, it is very important. We can build upon this paper in a way that solves real problems.</p>
<p>If the research community does not work on this and make it open source then some company is going to create tools to do this. IBM is already putting a lot of resources into creating tools around fairness. I would not be surprised if there are start ups that are already working on this. I would much prefer researchers to be working on this and making open source tools than having a company develop it.</p>
<p>One other takeaway from this work is that something that might help practitioners a lot is work that does not fall under the umbrella of research or software engineering. It is much more building communities and knowledge sharing. Taking the research that is already out there and making it more digestible. Creating communities for people to ask questions and connect with others. Often the most powerful thing you can do is not developing something new but making existing information more accessible and facilitating people to talk with each other. This type of work is not well incentivized though.</p>
</div>

    <script src="https://utteranc.es/client.js" repo="jgnewman96/website" issue-term="title" theme="github-light"
    crossorigin="anonymous" async>
    </script>


</main>





<footer>
    <p class="copyright text-muted">
        © 2019 - 2020 Judah Newman,  All opinions are my own. Theme adopted from <a href="https://themes.gohugo.io/minimal/">Minimal</a>. Powered by <a href="https://gohugo.io">Hugo</a>
    </p>
</footer>



</body>


</html>