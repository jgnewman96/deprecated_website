<!DOCTYPE html>
<html lang="en-us">

<head>
    
    

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Competing Bandits in Matching Markets</title>
    
    <style>

    html body {
        font-family: 'Ubuntu', sans-serif;
        background-color: azure;
    }

    :root {
        --accent: darkolivegreen;
        --border-width:  5px ;
    }

</style>


<link rel="stylesheet" href="https://judahgnewman.netlify.com/css/main.css">





<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu">


 <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"> 


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
 

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/go.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/haskell.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/kotlin.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/swift.min.js"></script>
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
    
    <script>hljs.initHighlightingOnLoad();</script>






<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<script>$(document).on('click', function() { $('.collapse').collapse('hide'); })</script>
 <meta name="generator" content="Hugo 0.80.0" />
    

    


    


</head>

<body>
    

    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container">
            <div class="navbar-header">
                <a class="navbar-brand visible-xs" href="#">Competing Bandits in Matching Markets</a>
                <button class="navbar-toggle" data-target=".navbar-collapse" data-toggle="collapse">
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
            </div>
            <div class="collapse navbar-collapse">
                
                <ul class="nav navbar-nav">
                    
                    <li><a href="/">Home</a></li>
                    
                    <li><a href="/about/">About</a></li>
                    
                    <li><a href="/writing/">Writing</a></li>
                    
                    <li><a href="/reading/">Reading</a></li>
                    
                    <li><a href="/research/">Research</a></li>
                    
                </ul>
                
                
                <ul class="nav navbar-nav navbar-right">
                    
                    <li class="navbar-icon"><a href="mailto:judah.newman@gmail.com"><i class="fa fa-envelope-o"></i></a></li>
                    
                    <li class="navbar-icon"><a href="https://github.com/jgnewman96/"><i class="fa fa-github"></i></a></li>
                    
                    <li class="navbar-icon"><a href="https://www.linkedin.com/in/jgnewman96/"><i class="fa fa-linkedin"></i></a></li>
                    
                </ul>
                
            </div>
        </div>
    </nav>

<main>

    <div>
        <h2>Competing Bandits in Matching Markets</h2>
        <h5></h5>
        <h7 class="section-heading">
            
<a href="https://judahgnewman.netlify.com/tags/economics"><kbd class="item-tag">Economics</kbd></a>


<a href="https://judahgnewman.netlify.com/categories/papers"><kbd class="item-category">Papers</kbd></a>

            
            
            
            <br>
            <i class="far fa-clock"></i> Reading Time: 8 minutes
    </div>




    <div align="start" class="content"><h2 id="summary">Summary</h2>
<p>This paper explores matching in two sided markets through the multi armed bandit problem. An example of a two sided market we can model using multi armed bandits is matching workers to employees. There is a pool of workers and a pool of employees. We want to find the optimal matching of employees and workers. Employees have preferences over workers, and workers have preferences over employees. There has been a lot of working that examines how to achieve an optimal matching. The most well known example of this is the <a href="http://www.eecs.harvard.edu/cs286r/courses/fall09/papers/galeshapley.pdf">Gale-Shapley algorithm</a>.</p>
<p>The novel contribution of this paper is it examines the setting where we do not know agents preferences. We have to learn them over time. In Gale-Shapley we know everyone&rsquo;s preferences and have to find a matching that is optimal according to those preferences. In this new situation, rather than making one matching using known preferences, there are multiple time steps and at each time step we have to make a matching. As we make matches we see the outcome of a match and learn agents preferences.</p>
<p>The paper sets up the scenario according the the following rules:</p>
<ul>
<li>There are N agents and K arms. (An arm is just a matching. So in the example of employees and workers, employers are agents and picking a worker is selecting an arm)</li>
<li>N &lt; K â†’ every agent gets an arm</li>
<li>At time step t each agent selects an arm</li>
<li>Arms have preferences over agents which are known</li>
<li>When multiple agents select the same arm, the agent which the arm has a higher preference for gets matched</li>
<li>The agent receives a reward from matching to arm that is sampled from a gaussian with a mean based on the specific arm-agent pairing.</li>
<li>An optimal match is defined as when an agent gets it preferred arm. A pessimal match is when an agent gets its least preferred arm.</li>
</ul>
<p>This setting is difficult because there is a trade off between exploration and exploitation. If you know that an arm gives you a high reward you may want to keep pulling it over and over again. If you have find a worker that is good you want to keep hiring them. On the other hand, it is possible there is a worker that is even better that you do not know about. In order to find that worker that is even better you have to do exploring, rather than continuing to higher your known optimal. There is a trade-off between maximizing your pay-off with what you already know, versus being able to get a higher pay off from what you do not know.</p>
<p>The authors present two different platform that could facilitate the matching: decentralized and centralized.</p>
<hr>
<p>In a centralized platform agents send in a ranking of arms and then the platform returns a matching vector. With a centralized platform we never have any conflicts where two agents select the same arm.</p>
<p>For a centralized platform one approach is the <em>explore-then-commit</em> approach. The explore section is devoted to learning agents preferences. The platform would return random matchings sampling from the space of all possible matchings. This would allow the platform to learn about agents preferences. The platform would then transition to the commit phase where it would create the best matching learned from the explore phase.</p>
<p>The authors argue this approach is not optimal because we are not taking advantage of what we learning during the explore phase. We only use what we have learned when we get to the commit phase. This could lead to us exploring matches that we know are not less optimal then when we have already tried. The authors propose another approach where at each time step the platform uses agents preferences to return a stable matching. The platform sees the results of this matching and learns more about preferences before creating the next stable matching at the next time step. The authors call this method Upper Confidence Bound (UCB) matching.</p>
<hr>
<p>There are two types of decentralized platforms. In a decentralized platform with <em>partial information</em> agents observe each other actions and conflicts but cannot coordinate. In a decentralized platform with <em>no information</em> agents cannot even observe others actions.</p>
<p>Using Gale-Shapley matching it has been shown that agents cannot do better by faking their preferences. The authors also explore honesty in this setting. They find that under the assumption everyone else is honestly presenting their preferences, an agent cannot achieve a better outcome by submitting false preferences.</p>
<p>The authors look at the performance of both policies for centralized platforms and show that UCB approach performs better than explore-then-commit. They briefly look at the explore-then-commit strategy for decentralized platforms mostly as a way of motivating the necessity of further work.</p>
<h2 id="thoughts">Thoughts</h2>
<ul>
<li>
<p>I really enjoyed this paper! I have been thinking a lot about tradeoffs of late. In almost all things there is an inherent tradeoff between different approaches. We want to find a solution that takes the benefits of the different approaches into account and is able to act optimally. In this setting the tradeoff between exploitation and exploration is quite clear. This paper does a good job of taking something complicated and simplifying it to a place where we can see the core trade off.</p>
</li>
<li>
<p>The authors claim that this paper is a blend of machine learning and economics. In this paper I do not see many aspects of machine learning but here is why I imagine the authors make that claim. Machine Learning very rarely gives us to tools to handle scarcity. A classic machine learning setting would be learning agents preferences over arms. Machine learning would allow us to learn fairly good estimates of preferences without each agent having to sample every arm. By sampling a subset of the arms we would be able to create preferences for each agent. An example of this is Netflix learning users preferences of videos. Netflix is able to create estimates for a user&rsquo;s preference for a video, without them watching the video based on their preferences over other videos. In the Netflix situation though, multiple agents can watch the same video. We do not have to worry about finding an optimal matching because everyone can be watching the same video. But imagine a situation with scarcity. Only one person can watch each video. Machine Learning does not give us the tools to solve this problem. To solve this we need to turn towards economics. The first thing I learned in economics classes was how to think about an agent with a budget constraint. An agent only has so many resources and then has to allocate them in an optimal way. Economics give us the framework to answer this type of question by building a model of human behavior. We know users preferences and using this model we can derive what they will do. This paper blends machine learning and economics because we can use machine learning to learn agents preferences and then economics to predict behavior once we know preferences.</p>
</li>
<li>
<p>Another way machine learning could be really powerful in this setting is to create individual policies for a decentralized platform. Imagine you are an agent in a decentralized platform and you have to make a decision between exploitation of what you know or exploring more. You could use the data you have already seen to help make this decision. If you assume rewards follow a known distribution, you could use the rewards you have seen to make an inference about what that distribution is. You can then make an educated decision about whether you are at a point where it makes sense to explore or to exploit.</p>
</li>
<li>
<p>Another interesting line of research is examining how much worse a decentralized platform is from a centralized platform. While a centralized platform is nice because we have more information when we make decisions, there are also dangers of a centralized platform. It gives one entity a lot of control over the market. Imagine the centralized platform decided that it did not want to make matchings that were optimal for agents, but rather matchings that it found to be optimal. It will be important to understand how much worse decentralized platforms are at making matchings under different assumptions.</p>
</li>
<li>
<p>This paper only looks at rewards coming from certain matchings. It does not consider that there might be socially optimal matchings that incorporate things outside of the individual rewards. It is possible that there are certain rewards we only receive when a set of matchings is made. For example if a certain ten matches are met, then there is an additional reward to each of the agents in those ten matches. This increases the complexity of the problem because it is not only about the individual match but about groups of matches.</p>
</li>
<li>
<p>Another interesting direction is exploring a platform in between centralized and decentralized. I can imagine a scenario where small groups can coordinate and communicate but the entire set of agents cannot. What would happen if groups of ten agents could communicate?</p>
</li>
</ul>
</div>

    <script src="https://utteranc.es/client.js" repo="jgnewman96/website" issue-term="title" theme="github-light"
    crossorigin="anonymous" async>
    </script>


</main>





<footer>
    <p class="copyright text-muted">
        Â© 2019 - 2020 Judah Newman,  All opinions are my own. Theme adopted from <a href="https://themes.gohugo.io/minimal/">Minimal</a>. Powered by <a href="https://gohugo.io">Hugo</a>
    </p>
</footer>



</body>


</html>