[{"categories":["Papers"],"contents":"By Harini Suresh and John Guttag\nPaper Motivation We need a better understanding of the potential unwanted consequences from machine learning. We often hear about “biased data“ but this term has become a catch all that prevents nuanced understanding.\nPaper Contribution The authors propose a framework that partitions harms from machine learning into six different categories. The paper covers what contributes to each harm and how to remedy them. The authors argue it is critical that the solutions we develop are from application specific understandings rather than abstract notions of fairness. This framework aims to develop a vocabulary which can be used to clearly state what the problems are and make practitioners assumptions immediately understandable.\nBackground Machine Learning models learn from historical data and generalize to unseen data. In recent years we have seen the mindless application of ML lead to problems with predictive policing and facial recognition. Discussions of problems with ML focus on “biased data“ but biased data can mean many different things. Biased data is an imprecise term that does not provide direction about how to mitigate its consequences or prevent harm. There are many other factors besides issues with data that can lead to machine learning causing harm.\nTo provide a richer framework of potential harms from ML, the authors break down a machine learning process into the following steps\n Data Collection Data Preparation Model Development Model Evaluation Model Postprocessing Model Deployment 1  Sources of Bias Historical Bias: When there is misalignment between the world and the values and objectives in the model\n Example: In 2018 5% of Fortune 500 CEOs were Women. Should a search for photos of CEOs represent that?  Representation Bias: When the population used to train the model is not representative of the population we apply the model to. This is often caused by our training sample containing too few people from certain populations. Fixing representation bias could come from changing the data collection phase or the model deployment phase.\n Example: ImageNet is a commonly used dataset to train computer vision systems. The majority of the images in it are from the U.S. This leads to models trained on ImageNet having poor performance on photos taken outside of the U.S. We could either collect more photos from outside the U.S. or not deploy systems trained on ImageNet for non-US use cases.  Measurement Bias: The set of features and labels that we use leave out information or introduce noise\n Example: Arrest rate is a proxy for crime. Not everyone who gets arrested commits a crime and plenty of people who commit a crime do not get arrested. Further, it is possible that the record keeping of arrest rates is noisy or biased. Certain regions might keeper tighter records or more detailed records are kept when the person arrested is a minority. Example: Often we use a measure that is a simplification of our outcome of interest. In lots of places income is used as quantitative measure of success. But success can mean a lot of different things besides income. Example: Minorities are more heavily policed and therefore more likely to be arrested. This means our model would predict minorities are more likely to commit crime. Minorities are predicted as having a higher chance to commit a crime because they are getting policed more to start, not because ethnicity is actually a predictor of how likely one is to commit a crime.  Aggregation bias: When different populations are combined and treated as homogenous. This can be seen when certain variables mean something different for different sub groups. Our model will fit the dominant population and then have a large error on non-dominant populations.\n Example: Diabetes is experienced differently for different ethnicities. HBA1C is a value that is used to measure how likely it is a patient has diabetes. The same level of HBA1C for one ethnicity means something very different for another ethnicity.  Evaluation Bias: When the way we are evaluating does not equally consider different populations. When we use metrics that are not representative of the way the model will be deployed\n Example: Facial recognition performs a lot worse on dark-skinned females. If dark-skinned females make up a small part of our data than wrong predictions for that population will not have a large impact on our accuracy metrics.  Deployment Bias: When a system is used or interpreted in inappropriate way. Models are deployed in a complex system with a lot of different actors using them in different ways. Often the people acting on the model score did not contribute to building the model.\n Example: Judges using risk assessment models to determine length of appropriate sentence.  My Closing Thoughts While I might not agree with all parts of the authors framework, I really appreciate them putting this framework on paper. It gives us all a more nuanced place to start the discussion. Once something is on paper we then have the opportunity to refine it and improve on it.\nThe paper touched on this briefly, but I want to see more discussion about how we have to build for the world we want, not the one we have. This is the fundamental way I think about historical bias. Every dataset we encounter is going to have biases of our society ingrained. I find a helpful starting place is identifying what the world we want looks like. Then we can begin to understand what is preventing us from being in that world.\nIn deployment bias the authors did not talk as much about how different stages in the ML lifecycle will often be completed by different people. It might be a different person in charge of deploying the model then the person who built the model. Having different people in charge of different sections can lead to a lose of information which can introduce misuse.\nThe formalizations and mitigations section of the paper is nice in theory but I felt it was kind of weak. The case studies at the end also seemed like they were rushed. Because space in a paper is limited it can be difficult to do certain sections full justice when they have to be short. I was reflecting on how it is very important to nail down the identity of your paper and then strip away sections that do not contribute to that identity. The paper might have been stronger by deleting those two sections, or just deleting one and fleshing out the other more.\nSimilarly, the related works section took up a lot of space in this paper. I personally really enjoyed the related works section, it gave me a lot of other papers that I want to read. But this was not a survey paper. All of the space taken up by the related work section took away from developing some of the ideas in the paper more fully.\nAll of the recent reading I have done about fair AI and ML makes me think about how important transparency is. By transparency I do not mean model transparency but process transparency. There is an entire system that has nothing to with ML that goes into building a model. There are organizations and actors, all which have flaws and blindspots. We should mandate more transparency into all of the different steps that went into building a model. People will always miss certain assumptions or practices that have the potential to be harmful. If we have transparency though we can collectively identify those dangers and then work to mitigate them.\n  This list makes the process seems like a straightforward and linear one. From my experience the process does not always happen in this order and it is hardly linear. There will often be cycles in the process and things will often happen concurrently. Itemizing these steps is a useful beginning and helps ground the conversation. \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://judahgnewman.netlify.com/writing/papers/2020/ml_harms_framework/","tags":["Machine Learning"],"title":"A Framework for Understanding Unintended Consequences of Machine Learning"},{"categories":["Books"],"contents":"How to Do Nothing By Jenny Odell is a meandering book focused on how to change our relationship with technology and those around us. While it would be easy to classify this book as self-help, it does not feel as sure of itself as other self-help books. Self help books often are written from the perspective of an all knowing author who has everything figured out and wants to share their knowledge with the feeble reader. How to Do Nothing has a couple of key arguments, but it never feels like Odell is over confident or proselytizing about the correct way to life your life. Odell has some values and principles that she believes. She uses stories to convey how these approaches have helped ground her and helped her find meaning.\nOne of the frameworks that Odell talks about which I appreciate is bio-regionalism. Bio-regionalism focuses on connecting with your immediate surroundings. Get to know both the land and local community around you. Having moved every year since I graduated college, I have struggled to connect with my local surroundings. Knowing that I am likely to move again puts some resistance between me and investing in my current surroundings. I appreciated Odell\u0026rsquo;s thoughts on bio-regionalism and it forced me to reflect on the ways I interact with my local community.\nA thread that flows through all the book is the power of resistance in place. Odell describes how it can be attractive to think about resistance as getting away from something. We romanticize the idea of getting outside of society or rebuilding something from scratch. But this type of resistance is less powerful. We should resist without running away. Rather than getting rid of your phone because of your bad relationship with it, try to fix your relationship with your phone. We cannot run away from the things that we have difficulties with or that we want to change. We must find a way to interact with them on our own terms that aligns with our principles and values.\nOne narrative I have seen many times is that individual action does not matter. That recycling, or composting and changing my own consumptions habits in the grand scheme of things does not matter. That the scale of impact of an individuals\u0026rsquo; actions is so small, it is inconsequential. Another way you might see this narrative manifest, is that the most important thing you can do is vote. The argument is that voting has a much larger impact than any other action. While I understand this line of argument, it is one struggle with immensely.\nIt is true that my decision to bike rather than drive does not have a large impact. But, if everyone did choose to bike, that be quite impactful. I want to live in a world where people feel that there individual contributions make a large difference. The narrative that individual action does not matter also misses the effect of social pressure. If you live in a community were most people are composting then you are more likely to compost. When I decide to ride my bike, there is small impact of that individual trip switching from a car to a bike. But there is also the impact of other people seeing me bike and our society switching slightly more towards a one where people bike then one where people drive.\nQuotes  The point of doing nothing, as I define it, isn’t to return to work refreshed and ready to be more productive, but rather to question what we currently perceive as productive.   In a situation that would have us answer yes or no (on its terms), it takes work, and will, to keep answering something else.   Thoreau’s ultimate hope was that if enough individual people decided at once to exercise their moral judgment instead of continuing to play the game, then the game might actually change for once   It’s also about control, since if we recognize that what we experience as the self is completely bound to others, determined not by essential qualities but by relationships, then we must further relinquish the ideas of a controllable identity and of a neutral, apolitical existence \n","permalink":"https://judahgnewman.netlify.com/writing/books/2020/do_nothing/","tags":["Non-fiction","Mindfulness"],"title":"How to Do Nothing"},{"categories":["Papers"],"contents":"By Facebook Research\nPapers Motivation Large distributed software systems with many users interacting are difficult to fully test. There are large numbers of edge cases which are difficult to identify ahead of time. Lots of bugs arise from complex interactions between users and the software that only occur at scale.\nPapers Contribution This paper outlines the general principles of web enabled simulations (WES). The development of Facebook\u0026rsquo;s WW simulation is used to guide a discussion of WES in general. WES takes a different approach to software testing by focusing on user behavior rather than system behavior.\nBackground A web enabled simulation (WES) simulates the behavior of users on a software problem. WES simulations build on recent developments in reinforcement learning. Each user is modeled as an agent playing a game on the software platform. A multi-agent approach is taken where each bot simulates a user. The bot\u0026rsquo;s behavior can be encapsulated in rules or via machine learning\nThe goal of a WES is similar to other software testing systems, to find and fix issues with the software. This environment is isolated from the production environment. This type of software testing can test bugs and features in a way that is not possible of other approaches. Facebook\u0026rsquo;s WW tests what happens when users try to break the community standards. It allows Facebook to test their response to bad actors.\nTesting new Software Changes Scenario: We want to understand the end user behavioral response to new privacy restrictions before they go into place. We use a WES to run simulations with the new policy and compare it to the old policy or slightly different policies.\nWES facilities Automated Mechanism Design. If the new policy has a parameter we are trying to optimize, we can run many simulations with different values for that parameter.\nNormal testing is focused on one user and their individual actions. This limits the ability to test features or bugs resulting from a communities actions. These social bugs require a new form of testing: Social Testing. WES systems are well suited to social testing. Social Testing allows developers to test levels of abstraction higher than previously possible\nFacebook\u0026rsquo;s WW WW is an environment built at Facebook to test the reliability, integrity and privacy of their platforms. Bots in WW only interact with FB\u0026rsquo;s backend code and do not interact with the GUI at all. WW bots are trained using reinforcement learning. A bot takes an action in the environment and then the state, with an accompanying reward is returned to them. The bot then decides on another action aiming to maximize the received reward.\nAn example of how WW is used is to simulate scamming. For this simulation multiple bots are needed, The scammer and the target. The scamming bot is trying to find a target and scam them. Target bots follow a rule based system hopefully modeling what targets look like in the real world. The platform is measured in its ability to impede scammers from finding suitable targets\nApplications of WW Currently WW is being used to test FB\u0026rsquo;s ability to deal with bad actors. If bots find ways to post bad content or break other parts of FB\u0026rsquo;s platforms then developers can stop real bad actors from taking a similar path in the future. WW does an automated search for mechanisms that impede bad actors.\nWW can also be used to test certain metrics at scale. WW provides a final full ecosystem which allows testing of key platform metrics before a change is rolled out to users.\nOpen Problems and Challenges  How to better train bots to act like real users or bad actors?  We need realistic social interaction. Is a fitness function able to express this?   How do we search the mechanism space to find optimal design?  My Closing Thoughts   Reading a paper every week with Tushar has opened my eyes to all the different stages of research. Different papers take different approaches and convey different depths of information. A paper which is at the beginning of a research agenda, has a very different style than a paper building on a known field. This paper is a very early paper in its space. It proposes the idea of a WES with some further questions. I often can be critical of papers like this because it feels like there is so much more for them to contribute. But that can come in later papers. It is a losing battle to expect everything from one paper.\n  One part of this paper I struggled with is how tied it is to a specific company. It seemed to go against the open source nature of research. Developing a WES seems very difficult and Facebook is only able to do so because they have an immense amount of resources. It would be difficult for anyone other than people at a company like Facebook to extend this research further. They also did not open source any of the code that went into developing WW. If WW was an open source platform that external parties could interact with, so much more could be learned about Facebook. How do certain design decisions that Facebook makes drive outcomes and what are the other possibilities?\n  It is interesting that this paper focused so much on integrity. In my head there are so many questions that WW could answer that seem more exciting than integrity. I imagine that the team developing WW had to find a first use case to prove their direction is worthwhile. They were successful with integrity and will now build it for other use cases.\n  ","permalink":"https://judahgnewman.netlify.com/writing/papers/2020/wes/","tags":["Design"],"title":"WES: Agent-based User Interaction Simulation on Real Infrastructure"},{"categories":["Papers"],"contents":"Paper Motivation Model builders may want to ensure their model is fair with regard to a protected attribute. (E.g. Wanting to make sure our model is not discriminating against people because of their race). Previous research has shown that quantifying a model\u0026rsquo;s fairness is not straight forward. Even if the protected attribute is not an input to the model it is still possible that the model is discriminatory.\nPaper Contribution The authors present a methodology of adjusting a learned predictor to remove discrimination according to their definition of discrimination. The authors also investigate their measure of discrimination and highlight what it includes and what it is missing. They find that their notion of fairness both gives more actionable information and facilitates training learners that provide more utility. The authors methodological fix is a post-processing step rather than a wholesale change to the entire learning pipeline which can be quite costly\nBackground There is currently a lot of interest in algorithmically measuring and ensuring fairness of machine learning algorithms. There is still however (as of 2016, and 2020 as far as I know!) no vetted methodology for avoiding discrimination in machine learning. The most naive and simple is training the algorithm without the protected attribute. This approach is actually insufficient though because of redundant encodings. redundant encodings is when the model effectively learns the protected attribute through other features in the dataset.\nAnother definition of fairness is demographic parity. demographic parity means the decision of the classifier is independent of the protected attribute. Training classifiers that satisfy demographic parity is flawed in two ways according to the authors.\n It does not ensure fairness. It also may cripple the utility we hope to achieve  The paper assumes the following problem setting. We have a target Y that we are trying to predict using available features X and a protected attribute A\nFairness Definition equalized odds: A predictor satisfies equalized odds with respect to a protected attribute if the predictor and the attribute are independent conditional on the outcome\n Our predictor can depend on the protected attribute but only through the actual outcomes. We must have equal true positive rates and false positive rates across classes in our protected attribute This definition punishes classifiers which only perform well for certain groups of the protected attribute  equal opportunity is a relaxed notion of equalized odds using the idea of a preferred outcome. If there is a preferred outcome, (getting a loan), then we only require parity for the preferred outcome.\n This is a weaker definition then equalized odds but still an interesting one because it allows for a more accurate model  oblivious: A measure is oblivious if it only depends on the joint distribution of the true outcome Y the protected attribute A and the classifier - An oblivious measure narrows the scope of different things to focus on when dealing with fairness. 1\nHow do we achieve equalized odds Equalized odds can be achieved through a post training correction that does not require changing the entire training pipeline. Using equalized odds we cant set up our problem via constrained optimization. We want to find the predictor that minimizes l2 loss while satisfying our fairness constraint. We want to minimize our loss subject to the constraint that the true positive and the false positive rates are equal.\nThe authors proof that this problem can be combined into one linear program the depends solely on the measures accessible to an oblivious measure. The authors also prove a method for achieve the same fairness guarantee when using a classifier that outputs a score rather than a prediction. The authors use the ROC curve, which depends on the false positive and the true positive rate to do so.The prove shows that the ROC curve must lie in a specific plane where every point on the plane meets our constraint.If we do so then any cutoff we choose to use as our final classification will satisfy our fairness constraint. While their solution is no longer a linear program it can still be solved numerically using ternary search\nThe authors also prove that a non-discriminating bayes optimal classifier can be derived from the Bayes optimal regressor. We can find a closed solution for how much worse the optimal solution that satisfies the constraint is than original optimal regressor.\nDo oblivious measures actually get rid of discrimination? The authors introduce two scenarios that could be very different in terms of fairness but both met their definition of equalized odds.\nScenario 1: We have a feature X1 (great great grandfather\u0026rsquo;s profession) that is highly correlated with the protected attribute. It is independent from the target but the protected attribute is correlated with the target. There is a second value X2 (criminal history in justice situations) which is only correlated to the protected attribute through the target variable.\n A classifier that would be fair here would only depend on X2 and not X1 A more accurate classifier would take into account X1 but this would not meet our measure of fairness  Scenario 2: We have a variable X3 (wealth) which is correlated with our protected attribute and predictive of our target.\n Using X3 in our model may or may not be seen fair discriminatory but by our measure it would be considered as unfair. We can correct our model to be one that uses X3 but does satisfy equalized odds  The two scenarios above seem fairly different. The difference in these scenarios is not picked up by equalized odds. Both models would satisfy equalized odds. These two scenarios are actually indistinguishable using any oblivious test. The authors prove this by creating a proof that follows the two scenarios they outlined above.\nCase study: FICO Scores The authors and the paper with a case study examining different fairness measures in the context of FICO scores with regard to race. FICO scores are used to predict credit worthiness in the US. The five different classifiers looked at are.\n Max profit: there is no fairness constraint Race blind: The threshold for determining a loan is the same for each group Demographic parity: A threshold is determined for each group so that the same percentage in each group is above the cutoff Equal opportunity: The fraction of non-defaulting group members that qualify for the loans is the same Equalized odds: The fraction of non-defaulters and the fraction of defaulters that qualify for loans is the same across each group  Using the authors proposed metrics (equal opportunity and equalized odds) we get a threshold in between max profit / race blind and demographic parity. Under max-profit and race blind situations minorities would have a much harder time qualifying for loans. Under demographic parity the opposite would be true\nConclusion The authors argue that a better path forward is to measure unfairness rather than proving fairness. Proving fairness is very difficult and there will always be certain things we miss.\nRequiring certain fairness constraints can force the model builder to collect better data rather than relying on protected attributes\nMy Closing Thoughts After finishing the paper realized I was still a little unsure about how to implement the post-processing adjustment. The paper could have benefitted from clearly identifying what the adjustment is and how people could make that adjustment. After going back through the paper I understand how to set up the optimization problem after we have trained a classifier. But more explicitly taking the reader through these steps would make it easier for practitioners to adopt this approach.\nI find that papers are stronger when proofs are in the appendix. It makes the main body of the paper readable and allows more detail in the proofs. This switch might have also helped clear up some of the confusion I faced on initial read.\nIt seems that since this paper was published the community has agreed we need stronger definitions of fairness and that a post-processing step is not enough. The authors of this paper call out as much in the body of the paper. But we also do not want to kill the good for the sake of the perfect. Would it be great if most practitioners adopted this approach? 100%. That would require building this approach into packages and making it easier for people to do.\nI really appreciate framing fairness as a constrained optimization problem. We want to find the best classifier while still maintaining fairness. Once we have identified it as a constrained optimization problem it is easier to identify what tools to use and which mental frameworks are helpful. This problem framework is what I found to be the most compelling part of the paper.\nI really appreciated the part in the conclusion about measuring unfairness rather than proving fairness. Fairness is not some binary. A model is not either fair or unfair. It will have some amount of unfairness. Measuring that unfairness and where it is coming from seems to be the most helpful path moving forward. So that leaves the question. What are the sources of unfairness and how do we measure them?\n  Narrowing of scope in such a way is both instrumental and limiting. It makes the problem more tractable and easy to wrap our head around one part of it. But it also means any solution we have is incomplete. The authors mention as much later in the paper. \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://judahgnewman.netlify.com/writing/papers/2020/equalized_odds_supervised_learning/","tags":["Machine Learning","Policy"],"title":"Equality of Opportunity in Supervised Learning"},{"categories":["Papers"],"contents":"By Duri Long and Brian Magerko\nPaper Motivation Artificial Intelligence is increasingly integrated into user-facing technology but public understanding is limited. We need more research investigating what users need to effectively interact with AI and how to design learner centered AI technologies that increase user understanding.\nPaper Contribution This paper formally defines AI literacy based on other existing research. The authors synthesize interdisciplinary research into a set of core competencies. They also make design suggestions for creating learner centered AI\nBackground End-users often lack the most fundamental understanding that they are interacting with AI. Researchers are beginning to address public misconceptions through inspecting how people make sense of AI.This paper takes a slightly different direction by identifying skills necessary in a future where AI transforms every aspect of our lives.\nThere has been lots of recent work on how to foster a better understanding of AI in groups with non-technical backgrounds and for people at a younger age.The “AI for K-12“ working has identified five big ideas to help teach each grade band what it needs to know about AI\n Computers perceive the world through sensors Agents maintain models/representations of the world and use them for reasoning Computers can learn from data 1 Making agents interact with humans is a substantial challenge for AI developers Applications can impact society in both positive and negative ways  While AI has existed as a field for over sixty years, most of the research on AI education for non-technical learners has been published in the last two years. The authors wanted to draw on more than just this recent work. They conducted an exploratory review of literature across multiple fields.\nWhat is AI Literacy? The term literacy originally refers to the ability to express ourselves using written language. Literacy has the historically context of broadening access to knowledge and providing people with the ability to share and communicate.Literacy has come to be a term used to mean competency in a certain topic to effectively communicate: digital literacy, computational literacy, data literacy etc\u0026hellip;\n We define AI literacy as a set of competencies that enables individuals to critically evaluate AI technologies; communicate and collaborate effectively with AI; and use AI as a tool online, at home, and in the workplace.  Methodology The authors conducted an exploratory review of interdisciplinary literature in order to define AI literacy competencies and design considerations for developers.\nThere search was guided by two questions\n What do AI experts think people need to know about AI? What existing misconceptions do non-technical learners have?  In the paper the authors describe in more detail how they searched for literature. 2\nConceptual Framework The authors divide their competencies and design consideration into five overarching themes. For each theme the authors listed some competencies and some design considerations.\n What is AI?   Competency 1: The ability to recognize AI Competency 2: The ability to reason about what it means to be intelligent Competency 3: Understanding that AI is interdisciplinary Competency 4: Understanding the difference between narrow and general AI  What can AI do?   Competency 5: Understanding AI\u0026rsquo;s current strengths and weaknesses. What domains does it excel in and what domains does it struggle in? Competency 6: Individuals should plan and think about what AI will look like in the future.  How does AI work?   Competency 7: Understanding representations and how information is represented to a computer Competency 8: Understand how systems make a decision Design Consideration 1: Information about the AI system, its state and what it is perceiving should be communicated to users Competency 9: Understand the different steps in training a Machine Learning system (e.g. model selection, training, testing and prediction) Competency 10: Understand humans role in creating AI Design Consideration 2: Simulate the agents algorithm with the individual. Put the individual in the place of the algorithm Competency 11: Basic Data Literacy Competency 12: Understand that learning happens from data Competency 13: Be critical interpreting data Design Consideration 3: Contextualize Data → How was data created and what is it used for? Competency 14: Robotic Agents can take action in the world Competency 15: Understanding how sensors communicate information to agents  How should AI be used?   Competency 16: Identify and describe key ethical issues surrounding AI  How do people perceive AI?   Design Consideration 4: Promote transparency in all parts of AI design Design Consideration 5: Unveil the inner mechanisms of the system to a user gradually Competency 17: Understand that agents are programmability Design Consideration 6: Provide opportunities for users to design and program Design Consideration 7: Consider individuals‘ competencies when designing an AI system Design Consideration 8: Encourage learners to be critical consumers of AI Design Consideration 9: Consider that end users might have different values and backgrounds Design Consideration 10: Provide support for parents teaching kids about AI Design Consideration 11: Design AI learning that fosters social interaction and collaboration Design Consideration 12: Consider leveraging learners interests, e.g. music or art to teach them about AI in a specific context Design Consideration 13: Acknowledge users preconceptions or biases that might come from media Design Consideration 14: Introduce perspective about AI that are not as popular in the media Design Consideration 15: Lower barriers to entry  My Closing Thoughts I am quite conflicted about this paper. Or not conflicted but disappointed. This paper had so much promise and aligns with a lot of my personal values about AI. It asks a really important question and surfaces some good ideas. But the paper was quite tough to read and the finally product felt more like a list then an actual framework. While a list is valuable and a good reference, a framework is often a lot more powerful.\nA lot of AI research has focused on how can the AI community make AI better. Those are important questions but this paper takes a slightly different approach. It focuses on how to make the public better consumers of AI. I know personally being AI literate is quite powerful and love the attempt to define what it actually means to be AI literate.\nI was hoping the paper would provide a framework rather than just a list of items. To me a framework gives a more hierarchical way of thinking about AI Literacy and maybe defines different levels of AI literacy. Here would be a quick stab from me at a framework. In order to understand AI you must understand there are four different parts that compromise AI.\n A task Data related to the task An Optimization problem An Algorithm related to all three of the above  Within each of these parts there are different depths to your understanding. As part of the task it is important to know what tasks AI is good at or bad at. Why can certain tasks be easily automated and others cannot. We can also assign some of their competencies to the data part. This kind of hierarchical framework makes it easier to focus on just one small section rather than a long laundry list.\nHaving this framework rather than the laundry list is important because it makes gaining AI Literacy more approachable. If you tell someone here is a list of 16 competencies you need, that seems like they will never be able to understand AI. But breaking it down into a couple of different parts makes it a lot more approachable.\nI would LOVE LOVE LOVE to see future work where the authors of the paper used this research to design curriculum our courses and then got to iterate on their list and make it tighter.\nWhen I was talking through this paper with Tushar  one thing we discussed was how not all research has to do anything. While I might have wished for this paper to have done something slightly different, it provides an exciting jumping over point for future research. Taking this list of ideas and putting it into a teaching framework is a job for another paper potentially.\n  Why is this only computers and not all agents? Humans also learn from data! \u0026#x21a9;\u0026#xfe0e;\n While seeing the authors put down all of their search terms seemed kind of trite it was interesting to see them describe their search in granularity. It would be interesting to do a replication of this paper and see if you found similar sources or different ones. \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://judahgnewman.netlify.com/writing/papers/2020/ai_literacy/","tags":["Design","Software Engineering"],"title":"What is Ai Literacy? Competencies and Design Considerations"},{"categories":["Books"],"contents":"The Color Purple has been my list since I went to a meditation retreat earlier this year led by Rhonda Magee. She highly recommended reading this novel and remarked a number of times about how powerful it is.\nThe Color Purple touches on a couple of deep and complex subjects in a short space. It conveys emotions on these subjects in a different way because it is a piece of fiction. Fiction can be read at a distance, where as non-fiction forces direct reference on the world we live. This distance provides the reader with more room for reflection. The novel is tied together by letters written by two sisters, Celie and Nettie.\nThe story shows how surroundings, particularly one\u0026rsquo;s immediate community impacts development. Every character is immensely influenced by their immediate surroundings. We see the juxtaposition of Nettie and Cellie who are sisters but move through the world in very different ways. Because of Cellie\u0026rsquo;s relationship to their father she is provided with a different set of opportunities than Nettie. This leads to Cellie and Nettie being on different continents and developing in different environments. We also see the differences in Cellie before and after Shug Avery has come into her life.\nThe Characters development overtime and their ability to change is quite powerful. We first see this with Cellie and how she takes more control over her life and is more in touch with her wants through her interactions with Shug. We also see a drastic change in Albert after Cellie and Shug leave him. For Cellie the change was brought through someone new entering her life, but for Albert it was from people leaving their life.\nThe passage where Cellie and Shug Avery talk about their relationship to God and their relationship to the Church is a particularly powerful one. Shug helps Cellie start to question truths she has accepted her entire life. It is powerful to see Cellie come to a new understanding and for her concept of God to be re-oriented.\nI have not fully processed how to think about the Nettie story line and everything that happens to her in Africa. The story touches heavily on aspects of colonialism and working with communities that are vastly different from your own. It also puts a magnifying class to the difference of being Black in the United States to being Black in Africa.\nI highly recommend reading The Color Purple. Different readers will find different characters or story lines that they connect to. The novel forced me to reflect on how perception of events rather than events themselves can often play a big role in our experience. I am excited to read more of Alice Walker\u0026rsquo;s work.\nQuotes  There is so much we don’t understand. And so much unhappiness comes because of that   There’s something in all of us that wants a medal for what we have done. That wants to be appreciated. And Africans certainly don’t deal in medals. They hardly seem to care whether missionaries exist   Just cause I love her don’t take away none of her rights.   And that in wondering bout the big things and asking bout the big things, you learn about the little ones, almost by accident \n","permalink":"https://judahgnewman.netlify.com/writing/books/2020/color_purple/","tags":["Fiction"],"title":"The Color Purple"},{"categories":["Books"],"contents":"I have read Atul Gawande\u0026rsquo;s writing in the New Yorker over the past year was quite impressed. He has a very measured tone and is very persuasive. Being Mortal was the only one of his books available at the library.\nThe best non-fiction books are those that combine stories with an analytical view. If it is only stories then often times the book is not making a larger point. Just an analytical argument can often miss the human side of a topic and can be a dry read. Gawande does a good job mixing a critic of current eldercare practices with stories about individual people. The most powerful part of the book is when Gawande talks about his own experience of his father passing.\nThe book does an incredible job of building up an argument. It begins by just looking at eldercare. It then takes that one step further to look at people with terminal illnesses. By the end we are thinking through larger questions about the purpose of life and how we want to structure our own.\nThis book hit me particularly hard because I was staying with my parents when I was reading it. As I get older, and my parents also get older, are relationship has continued to evolve. We have reached the tipping point where I am taking care of them more than they take care of me. When I go home I am often the one to do a lot of the cooking and cleaning. I am checking to make sure they are taking care of themselves. While my parents are still quite self-sufficient this book made me reflect on the process of them getting older and what that is going to look like. It made me think about how I can do a better job making sure that the rest of their years are really meaningful ones.\nQuotes  Our reluctance to honestly examine the experience of aging and dying has increased the harm we inflict on people and denied them the basic comforts they most need   People with serious illness have priorities besides simply prolonging their lives. Surveys find that their top concerns include avoiding suffering, strengthening relationships with family and friends, being mentally aware, not being a burden on others, and achieving a sense that their life is complete   In other words, our decision making in medicine has failed so spectacularly that we have reached the point of actively inflicting harm on patients rather than confronting the subject of mortality   The pressure remains all in one direction, toward doing more, because the only mistake clinicians seem to fear is doing too little. Most have no appreciation that equally terrible mistakes are possible in the other direction—that doing too much could be no less devastating to a person’s life  ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/being_mortal/","tags":["Non-fiction"],"title":"Being Mortal"},{"categories":["Papers"],"contents":"By Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals\nPapers Motivation The authors note that successful deep learning models often exhibit small differences between training and test performance. This is generally attributed to modeling techniques that are supposed to achieve generalization, such as regularization.\nPapers Contribution Using many experiments, the authors show that these traditional approaches do not provide a satisfactory answers for why deep learning generalizes well. Regularization is not what leads to good generalization, and state of the art deep learning can successfully fit random noise.\nBackground Neural networks often have more parameters than the number of data examples they are trained on. Yet these models do not exhibit overfitting. We observe a “small\u0026rdquo; generalization error (the difference between training and test error)\nSo why do some neural networks generalize well and others that don\u0026rsquo;t?. The traditional view of generalization is unable to distinguish between neural networks that generalize well and those that do not.\nRandomization Tests The authors ran an experiment where the true labels for a data set are replaced with random ones. The goal of this test is to understand the model capacity of a neural network. Even with random labels the neural networks achieve 0 training error. This means test error is equivalent to the results of a random guess because there is no correlation between the training data and the test data. As a result of changing our data, generalization error has gone up a lot without changing anything about the model. This may be a surprising result because if there is no signal in the data we would expect the network would struggle to learn anything.\nThe authors ran experiments on the continuum from no randomization to full randomization. Here are the six different experiments they ran\n Train with the True Labels Train with Partially corrupted labels, The label is corrupted with probability p Train with Random labels Train with Shuffled Pixels: A random permutation of pixels is chosen and then that permutation is applied to all images Train with Random Pixels: A different random permutation is applied to each image Train with data from a Gaussian: A Gaussian distribution is used to generate random pixels for each image  For every single one of these experiments the neural network is still able to converge\nImplications of randomization tests This result proposes challenges for traditional methods of thinking about generalization\n A neural network has the ability to memorize the entire data set Optimization on random labels is still an easy problem This experiment rules out VC-dimension, Rademacher complexity and uniform stability as possible explanations for generalization  Explicit regularization In the above experiments explicit regularization was not used. Regularizes are the standard tool to mitigate overfitting. Explicit regularization is supposed to confine the solution space to one that should generalize well.\nTo better understand the role of regularization the authors compare what happens when neural networks are trained with and without regularization. The authors look at three regularizes:\n Data augmentation: For example for image data, changing the brightness or saturation Weight Decay: Equivalent to an l2 regularizer on the weights. Essentially push the weights closer to zero Dropout: Mask out each element of a layer output randomly with a given dropout probability  The authors find the regularized neural networks do tend to generalize better, but even with all the regularizers turned off, the models generalize well. Data augmentation seems the be the most powerful regularizer and the other two do not make as much of a difference.\n It is difficult to say that the regularizers count as a fundamental phase change in the generalization capability of  Finite sample expressivity Previous work on expressivity has looked at the “population level“: What functions of the entire solution space, can or cannot be represented?. The authors argue that a more relevant question is what is the expressive power of a neural network on data of size n. When the number of parameters p of a network is greater than n, even just a simple two-layer neural network can represent any function of the input sample.\nImplicit regularization Early stopping could help with generalization but it does not have overwhelming benefits. Batch normalization has been found to improve generalization performance. To test the impact of batch normalization they train the inception model without batch normalization. The impact on generalization is only about 3 to 4 percent.\nCan Linear Models help us Understand NN If the number of dimensions is greater than the number of data points, we can fit any labeling. But can we generalize well with a rich model and no regularization? Do all solutions generalize equally well? Will one solution generalize better than others?\nSGD finds a very specific solution to this problem based on how it makes updates. SGD find the minimum l2-norm solution to the problem. Said another way “out of all models that exactly fit the data, SGD will often converge to the solution with the minimum norm“. This minimum norm helps provide some guidance but it does not have full explanatory power\nMy Closing Thoughts This paper is written quite well and is approachable. It covers some dense topics but is the main body of the paper is readable. I have noticed that using the appendix well is an important part of making a paper approachable.\nThis paper also really drives home the importance of mixing theory with experimentation. They show that theory we have traditionally used, might not apply anymore. It is important to be constantly testing theory and aligning our theory with practice.\nRecently, I have enjoyed reading papers less for their findings and more for lessons on experimental design. One of the nicest parts of this paper is how they set up their experiments.\nTwo thoughts on the actual contents of the paper.\n  In the one quote I have above, the authors use the term \u0026ldquo;phase change\u0026rdquo;. I imagine what they mean using that term is that regularization is not something that brings you from non-generalization to generalization. But what determines generalization or non-generalization. I would have liked clearer definitions of those terms. Does a 5 percent difference in accuracy represent a phase change? what about 10%?\n  It is interesting that the authors think about generalization solely in terms of model properties. When I think about generalization I am often thinking about the properties of my data. To me generalization is not just fitting unseen data, but fitting unseen data that might have some distribution shift or a be generated in a different way. If the unseen data is exactly the same as the seen data, then a model which fits the data well will of course generalize.\n  ","permalink":"https://judahgnewman.netlify.com/writing/papers/2020/rethinking_generalization/","tags":["Deep Learning"],"title":"Understanding Deep Learning Requires Re-thinking Generalization"},{"categories":["Books"],"contents":"Wow, this book absolutely wrecked me. It is easily one of the saddest books I have read recently. It is a different kind of sad than other sad books I have read. Non-fiction often can be quite sad. But this book is just heartbreaking in how relatable it is. The stories are not wrapped up with nice clean resolutions. Characters do not always get redemption. The stories just really hit home.\nEach of the stories in Jhumpa Lahiri\u0026rsquo;s book center on one relationship. It might be the relationship between father and daughter, brother and sister or even two lovers. The book covers such a vast array of relationships with depth. There is insight into the relationship between parents and children, siblings and people on both sides of a relationship. It is a helpful reminder that similar types of complexity arise in different types of relationships.\nEven though each story was quite different there were some common themes. All of the stories centered on people from the Indian Sub-continent living in the United States. The characters struggled with their relationships to their parents and their heritage. While none of the stories felt like they were directly about that struggle or tension, that struggle is deeply investigated over the entirety of the book.\nThe stories show both the diversity and similarities in experience of someone from the Indian Sub-continent living in the United states. This simultaneous diversity and similarity is quite powerful. It shows that we have to understand each of these people as individuals with their own struggles. But it also forces us to recognize that some problems and tensions are more universal then we might often think.\nQuotes  Life grew and grew until a certain point. The point he had reached now.   that solitude was what one relished most, the only thing that, even in fleeting, diminished doses, kept one sane?  ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/unaccustomed_earth/","tags":["Fiction"],"title":"Unaccustomed Earth"},{"categories":["Books"],"contents":"This book was such a good read, I loved learning the fundamentals of cooking. Most other recipe books give you a bunch of recipes but never explain why something tastes good or why flavors go together. I have been drifting towards this approach to cooking for a while now. I was trying not to follow recipes, but instead use recipes as inspiration. It never felt like following a recipe exactly by the book was sustainable.\nThis book contains so much quality information that I know i will need to return to it multiple times. I actually did not read any of the recipes because I was reading on my kindle and hag gotten the book from the library. Owning cookbooks during this point in my life seems cumbersome but I am excited to own this book in the future.\nIt is interesting reading this book being a vegetarian because so much of it does not relate to my cooking. A lot of the book focuses on how to prepare meat. One of my favorite parts of being a vegetarian is that it forces me to cook a lot more. When you are a vegetarian going out to restaurants often means you are a second class citizen. Most restaurants mains dishes are aimed toward meat eaters. When I go to a restaurant I often feel like I am missing out. When I cook for myself though, I never feel like I am missing anything.\nThis book could also be summed up in the feeling way: USE YOUR SENSES. The philosophy put forward in the book is to be constantly be tasting and looking at your food. Do not trust some directions someone else wrote. Observe what is going on right in front of you and respond to it. Have a picture in your mind of what your final product looks like and then work from there.\nI really liked her approach to cooking. Start by creating a vision of your final product. Then take one step back, what needs to happen one step before that final product. Keep taking steps backward until you get to the start. Now you have a recipe! This approach could be applied to many other processes besides cooking.\nQuotes  Like a jazz musician’s ear, with use it will grow more sensitive, more refined, and more skilled at improvisation.   the best cooks looked at the food, not the heat source   She must be a great cook. Indeed, the best cooks I’ve ever met—whether in home or professional kitchens—are careful observers  ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/salt_fat_acid_heat/","tags":["Non-fiction"],"title":"Salt Fat Acid Heat"},{"categories":["Papers"],"contents":"By Jessica Su, Krishna Kamath, Aneesh Sharma, Johan Ugander, and Sharad Goel\nThis paper from Twitter and Stanford has the following motivation:\n The theory of structural diversity predicts that individuals derive increased utility from interacting with people from different parts of their social network. Previous evidence for this theory comes from observational studies which makes it difficult to make any casual statement.  So what is structural diversity? People consider the actions of their social contacts when making decisions. A basic framework is the following: the more people you know who do something, the more likely they are to do it.\n e.g. The more of your friends who ski, the more likely you are to ski  Structural diversity extends this framework. It is not simply the number of contacts, but also the network structure of those individuals. You are more likely to do something if your contact who due an activity contain structural diversity. Containing structural diversity means they come from different parts of your social network.\n e.g. A individual is more likely to ski if two work colleagues and two friends do then if four friends do.  If we accept the the theory of structural diversity we should find that people are more likely to take up an activity if the part of their network that does that activity is structurally diverse.\nThe experiment In this paper the authors conduct a randomized controlled experiment on Twitter to evaluate the casual impacts of structural diversity on actions. he authors first show that observational data is consistent with previous findings that structural diversity impacts retention on Twitter. However, in the randomized experiment, there is no difference in retention between the treatment groups with low, medium and high structural diversity. This experiment implies in our context the correlation between structural diversity and retention does not seem to be casual\nThe experiment was done on a randomly selected group of the 4.2 million users on Twitter during 2016. The outcome of interest in the paper is 3 month retention. The authors define 3 month retention as going on twitter once in the 30 day period 90-120 days since joining. 1\nThe quantitative definition they use for structural diversity is a variant of average pairwise cosine similarity. The lower the level of similarity the higher the diversity.\n sim(v, w) is the similarity for two users v and w It is the cosine similarity of the binary incidence vectors for users who follow them. Two users are considered similar if similar users follow them  Due to twitter\u0026rsquo;s production system can only compute similarity for 40 most similar users and set everyone else to zero. 2    The key part of the experimental design for this paper is how they go about randomizing structural diversity. During sign up users are prompted to select topics they are interested in and then shown accounts related to those topics. Next users are asked to import contacts from their phone or email. After that users are shown recommended accounts to follow based on their social circle.\nTwitter uses an ML algorithm to provide a ranked list of individuals. This ranked list is bast on how likely they believe the new users are to follow them. New users are shown the top 20 people of this ranked list. The authors manipulate this list of 20 people to divide users into groups of low, medium and high structural diversity.\nFor each new user, a similarity graph was created for their contacts. An edge is drawn between two users if they are in each others top 40 most similar users. One section in a users social network is a part of the graph that does not connect to other parts. Users were than eligible to be assigned into one of the random groups if they have one component of size 20, two components of size 10 and three components of size 7. This eligibility requirement is necessary because if they do not have this structure then they cannot be assigned to any of the three groups. 3\n A user is then assigned to one of the groups  high similarity -\u0026gt; all 20 people are from their largest component medium similarity -\u0026gt; 10 people from their largest and 10 people from their second largest low similarity -\u0026gt; 7 from the two largest and 6 from the next largest control: Twitter\u0026rsquo;s ML algorithm 4    They authors look at the subset of users in their study that decided to select all of the recommended people to follow. 5\nResults The authors first show that if you just look at observational data, it does look like there is a correlation between structural diversity and retention. But this difference goes away when you look at the results of their experiment. But, it also turns out that this relationship goes away when you look only at eligible users.\nClosing Thoughts   While the findings of this paper are kind of interesting, the most interesting part of this paper is running a huge experiment on a social network. The authors do a good job enumerating the difficulties of this and it is really cool to see this kind of work happening.\n  While the authors do call out generalizability concerns in the conclusions of the paper, I think the beginning of the paper is overselling the results a little bit. The generalizability of their results is truly quite a narrow group.\n  I wish the authors had focused more on this difference between data for eligible users and non-eligible users. The correlation between diversity and retention disappears when we look at only eligible users. This means that their finding of no effect makes a ton of sense, it is not in disagreement with the observational data. So why did this difference happen between eligible and non-eligible. It seems like eligible users had the a structural diverse network already on twitter whereas non-eligible did not. I really would have loved more discussion of this.\n  I also would have loved to see more discussion of the normative impacts of their findings. How do their findings impact how we think about design in general. One of the reasons experiments is so powerful, is that often we can take the findings in apply it to another use case. There are so many tech companies that use Machine Learning to optimize the design of a system. What I want to see is research about why these design choices are working and therefore these design principles can be applied to other places.\n    It does not make sense to me why the used such a narrow definition of retention / engagement. It seems like they could have used other metrics that were not a binary but rather continuous. Using a continuous variable would make me less concerned about forking paths \u0026#x21a9;\u0026#xfe0e;\n I would have liked to see some evaluation of how this limitation impacts their results. If this was 20 vs. 60. vs. 100 what would that have looked like. \u0026#x21a9;\u0026#xfe0e;\n We will return to this later, but this eligibility requirement is going to be a big deal. only 500,000 out of 4.2 million were eligible. \u0026#x21a9;\u0026#xfe0e;\n I would be interested to have seen more discussion of twitter\u0026rsquo;s ML algorithm. How does it work and how does the retention rates for control compare to the other groups. \u0026#x21a9;\u0026#xfe0e;\n This diminishes the generalizability of findings to an even smaller sub population. \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://judahgnewman.netlify.com/writing/papers/2020/structural_diversity_experiment/","tags":["Design"],"title":"An Experimental Study of Structural Diversity in Social Networks"},{"categories":["Papers"],"contents":"Paper by Augenstein et al.\nI have stared a weekly reading group with my friend and former co-worker Tushar Chandra. Our format is that one person proposes a set of papers, and then the other picks a paper for the week.\nSummary The authors motivate their paper with the following logical steps.\n Manual data inspection is a key part of the machine learning workflow There are occasions when manual data inspection is either undesirable or infeasible (privacy requirements, or federated learning where data is stored at the edge). When data inspection is infeasible, generative models trained with federated methods can be a substitute for the role of manual data inspection.  The authors point out a trade-off between good data stewardship and the needs of a modeler. Good data stewardship can make modelers lives more difficulty. This work is driven by maintaining good data stewardship and not make a modelers life worse off.\nThe paper outlines six tasks where a modeler generally needs access to raw data\n Sanity Checking Data Debugging Mistakes Debugging unknown labels/classes Debugging poor performance on specific slice of data Human labeling of additional data Detecting bias in the training data  From my experience 2 through 4 are essentially the same thing. When you are debugging you do not know what the issue is until you identify it. This means the requirements for 2-4 are the same, because we are taking the same approach to debugging.\nThe authors argue that practitioners can use generative models instead of data inspection.\n Generative models will have a fundamental role to play in enabling the widespread us of privacy-preserving ML workflows   Differentially Private Federated Generative Models The types of models proposed in the paper are a combination of three different technologies: generative models, federated learning and differential privacy\nGenerative Models are a class of models that can synthesize new examples.\n An example of a generative model is a Variational Auto Encoder or a GAN. A generative model takes data in a different latent space and maps it to the space our data resides in. For example, a VAE works in the following way.  There is an Encoder network and a Decoder Network. The Encoder network takes data examples, (images, text etc..) and maps them to a latent space. The Decoder network takes that latent space as input and maps it back to our original space. To train a model, we minimize the difference between the original data example and the output of passing that example through the Encoder and then out the Decoder. Once we have trained the Decoder, we can use the Decoder to create new data examples by sampling from the latent space that the Encoder maps too.    Federated Learning is the processing of training and evaluating against distributed data\n The most common cases of federated learning are when working with mobile or IOT devices. Raw user data never leaves the edge device We train a model by randomly choosing a subset of devices to download the current model. Each device then locally computes a model update based on its data. The device then sends updates back to the coordinating server where they are aggregated to make a global update  Differential Privacy is a privacy guarantee for data that gets used to train the algorithm. The goal is that a model and its parameters can be released and yet information about any individual in the training data can not be reverse engineered. One way I have seen differential privacy formulated is the following: Adding or subtract an individual data point from the training data cannot have a meaningful impact on the model.\nDifferential privacy and federated learning together afford user privacy protections. While generative models often memorize certain examples leaking privacy, differential privacy will not allow a model to be published that displays this behavior.\n The paper next discusses two case studies for how to use their proposed approach.\nDifferentially Private Federated RNNS for text\n  An RNN works by taking a sequence of tokens as input and then outputting the probability of the next token. Tokens could be characters, words or sets of words depending on how you set up your task. We can frame an RNN as a generative model by sampling from that output distribution to generate many different sentences.\n  Experiment: We have a keyboard app that uses a word level language model to offer next-word prediction (eg. gmail). Our goal is to detect a bug in the training model pipeline\n  We have introduced a bug by incorrectly concatenating the first two tokens of every sentence. All of these incorrectly concatenated tokens will show up as out side of our vocabulary\n  Our model is a word level model but we can also use a character level model for debugging. We can use these models to spot the bug. We observe that our OOV happens at the beginning of the sentence and contains a space.\n  Differential Private Federated GANs for images\n  A GAN (generative adversarial network) works by training two models. One is a generator which maps a random input in a low dimensional space to a high dimensional space like an image. The other is a discriminator which judges whether an input is real or created by the generator. The two models are trained by trying to defeat the other one.\n  Experiment: on-device handwriting classification → A bank app that scans a check and needs to process text. We introduce a new bug that flips pixel intensities. We see that more people start correcting the model output but we do not know why. We cannot inspect the data itself, but still need to find the source of the bug.\n  We can do this by training two new models. One on data where we are performing the best and one where we performing the worst. We can contrast the images synthesized by the two GANs to identify the bug.\n   The authors conclude the paper with some current limitations. 1. federated generative models should require little parameter tuning. Currently training a model requires a lot of parameter tuning and this can make using these types of models quite difficult. 2. The fewer examples a bug shows up in, the harder it is to spot. It will be particularly tricky to spot bugs using generative models if they do not show up very often.\nEnding Thoughts   I really enjoyed reading this paper. When I first started reading the paper I thought they were going to be introducing a new algorithm. But really what they are introducing is a framework or approach. You cannot blindly adopt this approach and get good performance and security. The practitioner needs to still be quite creative about debugging. But that is actually why I like this paper so much! The case studies show ways to be creative about debugging. These approaches could be really helpful even in a setting where you did not need to use federated learning.\n  The paper does a really good job of using an appendix. There is so much content here that most of the results and the details of the experiments have to go in the appendix. This allows the main paper to be an easy read and do a very good explanation job. If you want to dig into the details, you can refer to the appendix.\n  I have a particular affinity for papers that lay our future areas of research. They directly call out that while this work is very promising, it is still in its beginning and there are so many further avenues for research.\n  Both of the examples in the paper assumed we had a trained model that worked and then a bug was introduced. I would bet that a much harder domain is when we are building a model for the very first time. Maybe it is unrealistic to say we will need to build a model without ever seeing any data. The more realistic approach is that once we put the model into action we will not see new users data. The task of trying to build a model without seeing data would be an interesting project though.\n  It is interesting to me that the authors assert there are situations where we will not be able to see data. My experience with data stewardship has been that often raw data cannot be seen, but some version of de-anonymized data and with certain attributes hidden can be seen. I am having trouble imagining a scenario where no version of the data can be seen by the model builder. But it might just be something to aim for. I can understand how if you can guarantee users that data will never leave the device that is a pretty great privacy guarantee.\n  ","permalink":"https://judahgnewman.netlify.com/writing/papers/2020/generative_models_privacy/","tags":["Deep Learning"],"title":"Generative Models for Effective Machine Learning On Private, Decentralized Datasets"},{"categories":["Textbooks"],"contents":"I had waited to read this classic paper By Kenneth Arrow for a long time now. Since I started working in the political sphere I have been interested in how to best represents people\u0026rsquo;s opinions and how to aggregate them. This paper is so impressive because it works directly from first principles and makes all of its assumptions quite clear.\nArrow begins by asserting there are two methods by which social choices can be made in a democracy. Voting and a Market Mechanism. These are methods of aggregating individuals wills to make a collective social decision. Outside of a democracy there are some other ways social choices can be made: dictatorship, convention. In these other methods there is no conflict between different individuals wills. This paper is concerned with voting\nHere is an example of voting to ground our thinking. We have a community of three voters and they must choose among three alternative Each person ranks the three choices by their preference.\n  At a community level, we prefer option A to B if a majority of the community prefers option A over option B\n Individual 1 Ranking: A \u0026gt; B \u0026gt; C Individual 2 Ranking: B \u0026gt; C \u0026gt; A Individual 3 Ranking: C \u0026gt; A \u0026gt; B    A majority of the community prefers A over B, prefers B over C and prefers C over A. The focus of the will be how to effectively aggregate individual preferences to maintain certain desired behavior\n  One example of a desired behavior → Transativity, e.g. If A is preferred to B and B preferred to C, then A should be preferred to C. We can ensure this at the individual level, but it might fall apart in aggregate. The above example shows this.\n  If just simple aggregation as in the above example does not satisfy our desired conditions, Are there better ways of aggregating individual preferences?\n Economics is generally concerned with how to achieve a social maximum outcome derived from individual desires. But the search for a clear definition of social optimum is difficult. Economics has used a weaker definition in the post: no individual can be made better off without making someone worse off.\nBut this definition does not suffice for social policy. There are many policies that will satisfy the above condition and we must find a way to differentiate between them\n  For any method of deriving social choices by aggregating individual preference patterns which satisfies certain natural conditions, it is possible to find individual preference patterns which give rise to a social choice pattern which is not a linear ordering  The above quote is effectively the thesis of this paper. There is no way to aggregate individual preferences to maintain certain desired behavior. This desired behavior will be outlined shortly.\nArrow is going to prove the above argument from the ground up. Lets define the problem:\nProblem Setting We have a set of possible choices S. We need to pick one choice from S which.\n  Within two values in the set we have three options X \u0026gt; Y, X = Y or X \u0026lt; Y.\n  In our final ranking we want the following things to be true\n For all X, X \u0026gt;= X If X \u0026gt; Y, then X \u0026gt;= Y if X \u0026gt; Y and Y \u0026gt; Z then X \u0026gt; Z - transitive property if X = Y and Y = Z then X = Z Either X \u0026gt;= Y or Y \u0026gt; X if X \u0026gt; Y and Y \u0026gt;= Z then X \u0026gt; Z    If for all pairs of preferences we know which one is preferred, then we know the entire preference scale\n  let \\(R_I\\) be the ordering relation of social states for individual let \\(R\\) be the ordering relation for the society\n \\(R\\) depends on the ranking by all individuals \\(R_1 \u0026hellip;.. R_n\\) where n is the number of individuals in the community    We assume that individuals are rationale meaning there conditions follow the above constraints\n We want to take the preferences from individuals and make an ordering among the states that is also rationale for society    The Social Welfare Function maps \\(R_1 \u0026hellip;.. R_n\\) to \\(R\\). There are five conditions we want our social welfare function to meet\n The social welfare function must be defined and deterministic for all possible individual orderings If X \u0026gt; Y and then X either rises or does not fall in the ordering of each individual X \u0026gt; Y - Positive association of social and individual values  This condition does not hold for rank ordering:  Example: Three voters with four candidates (x, y, z, w) first choice = 4 points, second choice = 3 points etc Individual 1 and 2: X \u0026gt; Y \u0026gt; Z \u0026gt; W Individual 3: Z \u0026gt; W \u0026gt; X \u0026gt; Y Final Tally: X: 10, Z: 8 Y: 7, W: 5 Now imagine candidate Y dies, we should by the above property maintain the same results. X either rises or does not fall. But we do not. Instead we get a tie Between Z \u0026amp; X X: 7, Z: 7, W: 4     if the relationship between X and Y is the same in two different orderings, then the social decision between X and Y must be the same - The Independence of irrelevant alternatives Our social welfare function cannot be imposed. Imposes means that X \u0026gt;= Y regardless of individuals preferences - The condition of citizens sovereignty Our social welfare function cannot be dictatorial. Dictatorial means there exists an individual i such that the ordering of the group is always the same as individual i\u0026rsquo;s ordering. - The condition of non-dictatorship  The Possibility Theorem: It will be shown that there is no social welfare function that satisfies conditions 1-5. These conditions lead to a contradiction.\n Prove We will proof the possibility theorem by example. We have two individuals with three options, x, y and z. Individuals are 1 and 2 with orderings \\(R_1\\) and \\(R_2\\). Our proof will go by laying out a list of consequences of our conditions until we reach a contradiction\nConsequence 1: If both rank x \u0026gt; y, then society as a whole prefers x \u0026gt; y\n By condition 4: there must be an ordering where X \u0026gt; Y for the society By Condition 2: If we raise X to the top of the ordering without changing anything then X will still be \u0026gt; Y. By Condition 3: Since the choice only depends on X relative to Y if X \u0026gt; Y for all individuals then X \u0026gt; Y  Consequence 2: if P1 has X \u0026gt; Y and P2 has Y \u0026gt; X, while society has X \u0026gt; Y then whenever P1 has X \u0026gt; Y , society must have X \u0026gt; Y\ne.g. If the will of person 1 prevails over 2, that will be the case still if 2 is indifferent or agrees with 1\n  Take X for P2 and put it at the bottom of their list. If in this situation society prefers X \u0026gt; Y then by condition 2 if X moves any higher in P2‘s list society will still prefer X \u0026gt; Y.\n  But what if we have a situation like the following. P1 has X \u0026gt; Y \u0026gt; Z and P2 has Z \u0026gt; Y \u0026gt; X. Society has X \u0026gt; Y. If P1 moves X down to Z \u0026gt; X \u0026gt; Y why does Society still need X \u0026gt; Y?\n By condition 3, if the ordering is the same then the societal outcome must still be the same    Consequence 3: If P1 has X \u0026gt; Y and P2 has Y \u0026gt; X then X = Y\ne.g. If the two individuals have exactly opposing interests on the choice between two alternatives society will be indifferent\nProving this one is going to be a doozy, and will show where a contradiction comes in.\nWe will prove this by contradiction. Suppose the above is not true. This mean either X \u0026gt;= Y or Y \u0026gt;= X. This would lead to a contradiction.\nAssume we have P1: X \u0026gt;= Y and P2: Y \u0026gt;= X and society X \u0026gt;= Y\n  Consequence 2 Will hold. This means if P1: X \u0026gt;= Y and P2: Y \u0026gt;= X society will X \u0026gt;= Y. Lets now walk through some scenarios.\n  P1: X \u0026gt; Y \u0026gt; Z P2: Y \u0026gt; Z \u0026gt; X -\u0026gt; Society: X \u0026gt; Y \u0026gt; Z\n New Consequence -\u0026gt; p1: X \u0026gt;= Z and P2: X \u0026lt;= Z and Society must X \u0026gt;= Z. By consequence 2 the above has to hold.    P1: Y \u0026gt; X \u0026gt; Z, P2 : Z \u0026gt; Y \u0026gt; X, Society: Y \u0026gt; X \u0026gt; Z\n Everyone prefers Y \u0026gt; X and the above consequence for X \u0026gt; Z . Consequence -\u0026gt; P1: Y \u0026gt;= Z and P2: Z \u0026lt;= Y then Society Y\u0026gt;= Z    P1: Y \u0026gt; Z \u0026gt; X P2 Z \u0026gt; X \u0026gt; Y -\u0026gt; society: Y\u0026gt; Z \u0026gt; X\n Everyone prefers Z \u0026gt; X and the above consequence for Y \u0026gt; Z New Consequence: If P1: Y \u0026gt;= X and P2: Y \u0026lt;= X -\u0026gt; Society Y \u0026gt;= X    P1: Z \u0026gt; Y \u0026gt; X, P2: X \u0026gt; Z \u0026gt; Y -\u0026gt; Society: Z \u0026gt; Y \u0026gt; X\n Everyone prefers Z \u0026gt; Y and the above consequence Y \u0026gt;= X New consequence : If P1: Z \u0026gt;= X and P2 X\u0026lt;= Z -\u0026gt; Society Z \u0026gt;= X    P1: Z \u0026gt; X \u0026gt; Y: P2: X \u0026gt; Y \u0026gt; Z -\u0026gt; Society: Z \u0026gt; X \u0026gt; Y\n Everyone prefers X \u0026gt; Y and the above consequence for Z \u0026gt; X New consequence: If P1: Z \u0026gt;= Y and P2: Z \u0026lt;= Y \u0026ndash;\u0026gt; Society Z \u0026gt; Y    By all of these consequences individual 1 is a dictator. Therefore the consequence must be false, therefore consequence 3 is proved\n  Last Scenario\n P1: X \u0026gt; Y \u0026gt; Z, P2: Z \u0026gt; X \u0026gt; Y  X \u0026gt; Y in all rankings Y = Z by Consequence 3 X = Z By Consequence 3 X cannot be both indifferent and greater than consequence 3      There is no social welfare function that satisfies conditions 1 - 5\n   Implications of the possibility theorem  If consumers values can be represented by a wide range of individual orderings, the doctrine of voters sovereignty is incompatible with that of collective rationality   The failure of purely individualistic assumptions to lead to a well-defined social welfare function means, in effect, that there must be a divergence between social and private benefits if we are to be able to discuss a social optimum  Thoughts I was interested in reading this paper partly for the ideas expressed in it, but also because of how influential of a paper it is. I wanted to understand what made it such an influential paper. I was also interested in what reading a paper from the 1950s would be like.\nOne of the most striking aspects of the paper is how little reference there is to related work. When I read more modern papers it is often essential to understanding the other surrounding literature. Each paper is in dialogue with the papers that came before it. While this is still true for this paper, it is very much a response to other work happening at the time, it does not take previous research as given.\nArrow build his argument from pure fundamentals. He also does it in an approachable way. His prove can be understand because it is an example that is easy for us to wrap our heads around.\nWhen I first read the paper, I was like this is kind of silly. Conditions 2 and 3 seems to be what makes the paper hold together. While I can understand why those conditions might be desirable, they do not seem necessary to me. But when you start thinking about what it would mean for those conditions not to hold, the paper makes more sense. If we took away conditions 2 and 3 then the social welfare function could start doing some really funky things.\nIt is in some ways surprising that the way we present research in 2020 is the same as it was in 1950. I would have imagined that 70 years of technological progress would have lead to a better way to present research findings.\n","permalink":"https://judahgnewman.netlify.com/writing/papers/2020/difficulty_welfare/","tags":["Economics"],"title":"A Difficulty in the Concept of Social Welfare"},{"categories":["Textbooks"],"contents":"","permalink":"https://judahgnewman.netlify.com/writing/textbooks/harmless_econ/chapter3-1/","tags":["Economics","Statistics"],"title":"Mostly Harmless Econometrics Chapter 3.1"},{"categories":["Research"],"contents":"For the past two weeks, I have been working through Me \u0026amp; White Supremacy, Layla Saed\u0026rsquo;s month long interrogation of white supremacy. I am reading the book, journaling each day and then meeting with a group of co-workers weekly to discuss our experience. While journaling, I noticed my tendency to gravitate toward simple solutions. The chapters about tone policing and cultural appropriation were quite challenged. I was looking for specific guidelines about what my behavior should look like. A checklist to tell me when something was or was not cultural appropriation. Explicit guidance that I could always follow.\nThis is both an unfair and doomed proposition. It is unfair because my actions and behavior are my own responsibility. Asking someone else to inform me of the correct way to act in all situations is unsustainable. Each situation comes with its own intricacies and complications. Just following a checklist will not be sufficient. There will be situations where the checklist leads me astray. Instead it is important for me to process the context in realtime and evaluate the situation in front me.\nThe real problem with relying on check lists is that it offloads the decision making process. When I use a checklist I am no longer grappling with difficult questions. The checklist is doing all the work.\nAsserting myself, versus taking a back seat in a conversation is heavily dependent on who else is in their room. Making an appropriate decision is dependent on understanding other individuals‘ histories and our group dynamic as a whole. If I make that decision using a checklist I am processing information in a different way. Using the checklist means I am more concerned with my actions being perceived as the right ones, rather than having an understanding of the situation. I can say \u0026ldquo;look I went through the checklist therefore my actions are defensible\u0026rdquo;.\nThese reflections reminded me of Rhonda V Magee\u0026rsquo;s messages about radical complexity. At a meditation retreat I attended earlier this year, she talked about how the world is radically complex. But we have not been taught to hold that radical complexity and examine it. We have been taught to simplify it. To simplify situations so we can run them through a checklist and then make a decision. But this simplification will always lead mean missing part of the full picture. It deludes us that what matters is making the correct decision and not also what goes into that decision\n After reading this article by Pratyusha Kalluri I am reflecting on how the AI community might be following into a similar trap with “Fair AI“.\nFair AI research is focused on evaluating algorithms for systematic bias. Is the error rate different for different populations? Does the algorithm use certain protected traits such as race or gender to make a decision? Research about fairness in AI systems has exploded in the past few years. I do not want to downplay the importance of this work. Lots of researchers are asking very important questions that need to be asked. But I do wonder if this work is deluding both people inside and outside of the AI community.\nIs it important that we use algorithms which are fair by these definitions? Yes! But does having a supposedly fair algorithm mean it is safe to use? No. Algorithms passing these specific tests are only a small part of the picture. We should refrain from and be skeptical of any checklist that people can use to determine if an algorithm is useful.\nThis view of fairness comes from us wanting to move past radical complexity. We tend to gravitate towards simple solutions for difficult problems. While it might be a necessary condition that our algorithm passes certain checks, it is not sufficient. When AI is implemented it will always be part of a larger system. We must interrogate AI in that larger framework.\n We can use a case study to help ground our thinking. Here is a NY Times article about an instance where AI usage went terribly wrong. It led to a man being arrested for a crime that he did not commit. While it is easy to read this article and call for “Fair AI“, a fair algorithm would not have solved all of the problems here.\nThe article mentions that research has shown algorithms are less accurate for black individuals than for white ones because of bias in training data. But even if that were not the case, and the algorithm had passed the fairness check, AI still would have been misused in this context. Similarly there is a quote in the article suggesting that the problems would be solved if law enforcement bought the good facial recognition algorithms rather than the bad ones.\nThe real problems is that the people using the technology have minimal understanding of it. The company that contracts with the police department has no transparency and did not develop the algorithms. The police officers who were using the output of the algorithm knew nothing about it. They did not have insight into how often it is wrong. When people using the output of an algorithm know nothing about it, the algorithm is more likely to be misused. The people who develop technology are responsible for overseeing how it is used.\n Even if an algorithm is fair and the people using it know a lot about it, it might still be a bad idea to deploy the algorithm. It is possible that it will be used to surveil or damage certain populations. Successful development and deployment of AI depends on asking questions not specific to algorithms. It is essential to be asking questions about the system that the algorithm is operating in as well as the algorithm itself.\n Who is developing this technology?  What is their background? Where other voices involved?   What is driving the development of this technology?  What funded this work? What principles were leading the development?   Where will this technology be implemented?  Were the use cases clearly specified by the developer? Are the developers part of the implementation of the algorithm or only the development of it?   How will this technology be used and who will be in charge of maintaining it?  Is the output of the system feed into another system? Who acts on the output of the system? How much knowledge do the people that are using the system have about the development of the systems?      When we ask these questions about the larger system rather than just the algorithm, we have a better chance of uncovering problems with AI adoption. The current approach to fairness focuses too much on the internals of the algorithm. The AI community must proactively ask questions outside the scope of the algorithm.  If we look at some characteristics of the larger AI community we can notice dangerous trends.\n Who is developing Artificial Intelligence?  The Elite! Wealthy people either at large organizations or academic institutions. The field is not diverse and is mostly white and male.   What is driving the development of this technology?  Profit! People are developing AI for the benefit of large organizations. Rarely is AI being developed because of the ways it could help people. The focus is on automation. Not how the automation could unlock opportunities for people.   How will this technology be used and who will be in charge of maintaining it?  The technology is often being implemented by people who have very little knowledge of it. They often do not know how it works and the potential dangers.    If AI researchers truly want to see AI be a force for good they need to start working with communities and pushing themselves to do more difficult things. They need to be more aware of larger societal structures and recognize that you have a responsibility for how your technology is going to be used. While research on algorithmic fairness is important, it will never be enough. Adoption of AI in a harmonious and beneficial way is dependent on large contextual questions about complex systems.\n","permalink":"https://judahgnewman.netlify.com/writing/research/fair_ai_is_not_enough/","tags":["Reflections","Machine Learning"],"title":"A Fair Algorithm is not sufficient"},{"categories":["Papers"],"contents":"By Robin DiAngelo\nThe paper explores why white people are silent when talking about race. It argues that this silence, \u0026ldquo;White Silence\u0026rdquo; functions to maintain white power and privilege. The goal of the paper is to unsettle the complacency that often surrounds White Silence. TO motivate silent whites to stop being silent.\nWhiteness is dynamic, relational and operating at all times and on myriad levels. It is not something simple and not easy to wrap our heads around. We often think we know what racism is and can easily point it out. Racism also operates on many level and can be difficult to identify.\nDiAngelo argues that not contributing one\u0026rsquo;s perspectives serves to ensure that those perspectives cannot be expanded.Silence is bad because it holds up the status quo. White silence has to be considered in context to understand how it is detrimental. Due to the historical role of whiteness silence continues to enforce racism. The same action is different towards different people. Talking over a Black person is different than talking over your brother.\nThere is no one way for whites to engage that will be effective in all contexts. You must be continually grappling with how to best interrupt white privilege. Anti-racism work specifically challenges us to respond different then we normally wouldIt is critical to support those who have voiced a perspective we support.\n Seeing one’s patterns of engagement as merely a function of a unique personality rather than as sociopolitical and coproduced in relation with social others is a privilege only afforded to white people\n To paraphrase the above quote, whites have the privilege to focus on our personality rather than how society has conditioned them.\nWhite Silence comes from racism being seen as individual acts of meanness and people not wanting to be seen as racist. A big part of white silence is protecting oneself from being challenged. In the context of race discussions silence is inarticulate and offensive. We must have the courage to put our integrity to do the right thing above the possibility of repercussions.\nThoughts This quote below highlights the importance of doing active anti-racism work and doing it continually. Having an understanding of a concept is different than being able to act on it. I see this all the time when trying to learn a new concept. I might read about the concept and be able to understand it in theory. But then when I go to apply it, I struggle mightily. This is why it is so important to actually put the things that we read about into action. Only through trial and error can we see deeper than just a high level intellectual grasp.\n We may have an intellectual grasp of the dynamics, but awareness of racial inequity alone is not enough to trump our participation\n The way this paper handled argumentation was very intriguing. DiAngelo proposed possible counter arguments then explained why the counter argument was flawed. It is always nice to read papers from different fields and see how they prove their points in different ways.\n","permalink":"https://judahgnewman.netlify.com/writing/papers/2020/white_silence/","tags":["Race"],"title":"Nothing to Add: A Challenge to White Silence in Racial Discussions"},{"categories":["TextBooks"],"contents":"Four Frequently Asked Questions The authors begin the book by discussing how to formulate research questions. The explain the path to a research questions if by asking four questions.\n  What is the casual relationship of interest?\n This is not to say you cannot have purely descriptive research but purely descriptive research is a step towards causal research Causal research lets us answer questions about counterfactual worlds. A World in which something different has happened Casual questions can be asked about any unit of observations. Individuals, schools, firms or even countries.    What experiment could ideally be used to capture the causal effect of interest?\n Ideal experiments are almost always hypothetical. While we will not be able to carry them out they help us pick research topics A research question that cannot be answered by an experiment is FUQ\u0026rsquo;d, fundamentally unidentified questions. Here is an example of a FUQ\u0026rsquo;d question: Would kids be better off starting school later?  We could randomly select some kids to start at age 6 and some to start at age 5. We then would test the kids when they are in second grade.  But the 6 year olds will be older in second grade then the 5 year olds. But if we tested them at the same age then the kids who started school earlier will have been in school for longer   There is no way to disentangle the start-age effect and the time-in-school effect       If you cannot devise an experiment that answers your question in a world where anything goes, then the odds of generating useful results with a modest budget and non-experimental survey data seem pretty slim\n  What is your identification strategy?\n an identification strategy is how a researcher uses observational data to approximate a real experiment    What is your mode of statistical inference?\n The answer to the above questions contains the following information  What is the population to be studied? What sample are we using? What assumptions are made when constructing standard errors?      The Experiment Ideal The most credible and influential research designs use random assignment\nImagine trying to answer the following question : Do hospitals make people healthier?\n  We are studying a poor elderly population. Some percentage of people in the hospital waiting room are admitted and some are not. We can compare these two groups.\n  The authors collected data for this example and show that people who spent time in the hospital where sicker than those who did not spend time at the hospital.\n  On face value this data could be used to suggest that hospitals make people sicker.\n  But this comparison should not be taken at face value: people who go to hospitals are less healthy to begin with\n  Lets formalize this with some math. hospital treatment is a binary variable {0, 1}. We are interested if health status is impacted by this variable.\n  For each person there are two potential health values. One if they went to hospital and one if they do not. Ideally we would measure the difference in health values between these two values. But we cannot observe both of these outcomes, we can only see one of them.\n  Because we cannot see both for one individual we must learn by comparing the average health of those who were and those were not hospitalized\n  The difference between those who were hospitalized and those who were not contains selection bias. Since the sick are more likely to seek treatment those who were hospitalized will have worse starting health.\n  Random assignment solves this selection problem The group a person is assigned to is not connected to their starting condition.\nRandomized trials are not problem free, but they do avoid this problem of selection. Here is one question we will still want to ask about randomized trails: - Does the randomization successfully balance the subject\u0026rsquo;s characteristics across different treatment groups? We can compare covariates across groups to check this.\nRandomized experiments can be quite costly though so we will often try to answer questions without them. We try to find natural experiments that mimic a randomized trail.\n Regression is a useful tool for the study of causal questions.\n We will often need to control for other factors impacts in our regression For example: We often used conditional random assignment rather than pure random assignment  We might do random assignment within a school but then we will need to control for differences between schools    Thoughts   I love the way this book starts and the focus on how to get a research question. So often we are too focused on niche techniques and algorithms that we forget the core questions we need to ask at the beginning.\n  The chapter does a really good job of using examples to highlight certain points.\n  ","permalink":"https://judahgnewman.netlify.com/writing/textbooks/harmless_econ/chapter1/","tags":["Economics"],"title":"Mostly Harmless Econometrics: Chapters 1 \u0026 2 "},{"categories":["Books"],"contents":"Reading Uninhabitable Earth by David Wallace-Wells really lit a fire under me with regard to climate change. Climate Change is something that does not impact my day to day and so it can be easy to not be thinking about it. It is not top of my mind when I am wasting energy or thinking about flying to visit a friend.\nThe beginning of the book is extremely powerful. It has a number of different chapters all devoted to the different ways climate change is harming our planet. One chapter looks at sea level rise, another at wildfires and another at pollution. Reading through all these chapters in a row paints a powerful picture of how dramatic the impact of climate change will be on the world.\nWhile I the beginning of the book was quite good, the second half felt less strong to me. Wallace-Wells gets into some pretty existential conversations and seems to have a less cohesive narrative. While I love a good existential conversation, it was tough to follow all of the different references he was making. I would have enjoyed a sharper a focus on how we have everything we need to combat climate change. We created this problem and we can also solve it. The problem is lack of political will and individual motivation.\n After reading the book I really wanted some more concrete actions I could be taking. Lucky me I had a tab sitting open since almost the beginning of the year that had just that information. It is a long read but Erika Reinhardt has collected so much information about personal climate action. She highlights lots of different actions we can take to help the environment.\nHere are a couple of things I did after reading through her resources\n I signed up for recurring donations to Wren to offset my carbon footprint. While I know offsetting my carbon footprint is very different from reducing it, doing so is still a worthwhile endeavor. This is no way absolves me of my carbon footprint and I should put more work into reducing my carbon footprint. I know that flying is particularly bad for the environment. I am going to make donations to Compensaid or the Gold Standard whenever I fly. I connected my utility bill to Arcadia I took all of my investments and switched them into green energy. Here are the index funds I am invested in.  ICLN TAN SMOG PBW   Erika also mentions a lot of different organizations in her article. Here are a couple of the ones I found to be the most intriguing  Sierra Club Citizens Climate Lobby Clean Air Task Force Environmental Voter Project The Years Project    Quotes  But time is perhaps the most mind-bending feature, the worst outcomes arriving so long from now that we reflexively discount their reality.   That we know global warming is our doing should be a comfort, not a cause for despair   It took New York City forty-five years to build three new stops on a single subway line; the threat of catastrophic climate change means we need to entirely rebuild the world’s infrastructure in considerably less time.  ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/unhabit_earth/","tags":["Nature"],"title":"The Uninhabitable Earth"},{"categories":["Books"],"contents":"On Earth We\u0026rsquo;re Briefly Gorgeous By Ocean Vuong is a novel from the perspective of a narrator writing to his mother. The narrator recounts many different stories from his childhood and his relationship with different influences in his life. The letter jumps all over the place recounting stories about his Grandma\u0026rsquo;s life, to segments from the narrators point of view now that he is an adult.\nThis book reminded me in a lot of ways of some passages in Severance. The narrator is trying to imagine what it might have been like for his mother or grandmother being an immigrant and raising him. He tries to connect with his mother in different ways but also thoroughly resents his mom. The existence that his mom knows is so different from his own and he struggles to connect with her.\nWhile the book was written as a letter to his mom, and the narrator\u0026rsquo;s mom underlies the entire story, the vocal point of the story to me was Trevor. Trevor was the narrator\u0026rsquo;s first lover and the only character we learn about who is not in the narrator\u0026rsquo;s family. Trevor clearly had an immense amount of influence on the narrator. It felt as though the narrator was potentially writing this letter for Trevor rather than for his Mom.\nIt was interesting to intuit how the narrators view of himself changed over the time. The format of the narrative led to discussion of other characters rather than the narrator. The narration was very light on the narrator\u0026rsquo;s thoughts and feelings. We are left to understand them from what the narrator says about others. It was fascinating to see how the narrator begin to take more active roles in the stories he told as he got older.\nThe part of the book that was the most powerful to me was how well it captured the feel of certain places. The book could have been a letter to Hartford as much as it was a letter to his mom. The narrator did such a good job conveying the feeling of different places during different time periods.\nQuotes  the past never a fixed and dormant landscape but one that is re-seen.\n  “Let bygones be bygones” is not a legitimate approach if we wish to call ourselves a constitutional democracy.\n  very impossibility of your reading this is all that makes my telling it possible.\n  Remember: The rules, like streets, can only take you to known places.\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/briefly_gorg/","tags":["Fiction"],"title":"On Earth We're Briefly Gorgeous"},{"categories":["Books"],"contents":"The Color of Law By Richard Rothstein provides an extensive history of discrimination in housing policy in the United States. Rothstein chronicles discrimination at all levels of government and in all facets of housing. Whether it is discrimination through zoning, mortgages or schools, different levels of government continued to find new ways to enforce segregation. Rothstein refutes that housing segregation is a result of people\u0026rsquo;s preferences. He shows that housing segregation is a direct result of governmental policies that still impact our country today.\nRothstein argues that the government is constitutionally obligated to provide amends for the ways it has hurt African Americans through housing policy. Since the government has broken the constitution, it is required to repay victims for the damages it has done. Rothstein makes a case for reparations purely from a legal standpoint of what the United States constitution requires.\nReading this book highlights how we need to understand our history to understand the present moment. Most of the legislation and actions he talks about happened before I was born and a lot of it took place before even my parents were born. Yet these actions are still having serious consequences on our society today. If you only viewed housing segregation in the present you might reach the conclusion that Rothstein refutes; housing segregation is a result of individuals preferences.\nWhile it is possible to dispute some of the individual points that Rothstein makes, the entire book is difficult to refute. You might be able to proof that certain policies do not have the impact he lays out. But as a whole when presented with this evidence, it is impossible to argue that the government did not support and implement housing segregation.\nI appreciated Rothstein\u0026rsquo;s sentiments at the end of the book about how to think through next steps. Often we want a quick solution, or a quick calculation of damages. We want a solution that is a win for all people. Rothstein is direct and honest. Fixing this problem is not going to be a win for all. Some people have thoroughly benefitted from housing segregation and addressing the damages that were caused will potentially hurt some people. Rothstein highlights that since government policy is what caused housing segregation it should also be able to solve it. He makes the point that there will not be a perfect policy that solves everything but we have to start somewhere and the government must start trying to fix the wrongs they have committed.\nQuotes  There is generally no judicial remedy for a policy that the Supreme Court wrongheadedly approved. But this does not mean that there is no constitutionally required remedy for such violations\n  “Let bygones be bygones” is not a legitimate approach if we wish to call ourselves a constitutional democracy.\n  Ending segregation in housing, however, is much more complicated. Prohibiting discrimination in voting and restaurants mostly requires modifying future behavior. But ending de jure segregation of housing requires undoing past actions that may seem irreversible\n  REMEDIES THAT can undo nearly a century of de jure residential segregation will have to be both complex and imprecise. After so much time, we can no longer provide adequate justice to the descendants of those whose constitutional rights were violated. Our focus can be only to develop policies that promote an integrated society, understanding that it will be impossible to fully untangle the web of inequality that we’ve woven.\n  With very rare exceptions, textbook after textbook adopts the same mythology. If middle and high school students are being taught a false history, is it any wonder that they come to believe that African Americans are segregated only because they don’t want to marry or because they prefer to live only among themselves?\n  But we delude ourselves if we think that desegregation can only be a win-win experience for all. There are costs involved, and some may be substantial\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/color_of_law/","tags":["History","Race"],"title":"The Color of Law: A Forgotten History of How Our Government Segregated America"},{"categories":["Books"],"contents":"Girl, Woman, Other by Bernardine Evaristo is a mix of a novel with a set of short stories. Each chapter is told from the perspective of a different women in England. Some of the stories are set in the present and some of them are set a hundred years ago. The stories cover a vast range of different experiences from people who grew up very poor to people who grew up with some wealth. It has stories about people in the country side and people in the city. Stories told from young perspectives and older perspectives.\nWhile each story is its own entity, by putting all of them in one book and loosely tieing them together through a single event, the author has created a thread that ties all of them them. The reader can no loner read each story as it\u0026rsquo;s own but is connecting it to the stories that come before it and come after it. The context of each story is all the other stories in the book.\nOften a character will show up both in their own story but also in a story from someone elses perspective. By having characters switch between being narrators or being narrated about, we get different pictures of them. We see both how they think of themselves as well as how others think of them. This forces us to recognize that each character\u0026rsquo;s narration is not completely reliable. We are confronted with this most dramatically with Carole and her teacher Mrs. King. Just as I never saw when I was going through school, Carole does not recognize that Mrs. King is also a person with hopes, dreams and fears. It is so easy for students to forget that teachers are just people too.\n Reading all of the stories in a row forced two opposite types of reflections in me. On one end it allowed me to see the similarities between all these different stories. All of the characters are struggling with acceptance, they struggle to find partners that empower them and struggle with loneliness. On the other end of the spectrum I reflected on how each of these stories was uniquely individual. While they might face similar fears, their situations are quite different and how they cope with them are quiet different.\nIt is easy to think of these reflections as opposites or incongruent with each other. Well what is it? Are they all different and unique or all they all the same? Well it is both! Everyone is struggling with loneliness and finding their place. But they all have a very unique context and the forms that loneliness take are very different for each of them. For some of the stories loneliness means having a partner that passed away. For some loneliness is not being understood by your parents and for others it is not being understood by your kids. What these means is that while loneliness might be something we all struggle with, it is going to manifest in a unique way for each individual.\nTreating all loneliness as unique or all loneliness as the same is a losing battle. We can recognize that it takes unique forms but that it also has commonalties. Recognizing the commonalities allows us to learn from others and connect with that similarity in others. Recognizing the uniqueness allows us to not be overly prescriptive based on others experience and value that what everyone experiences is different.\n One commonality I noticed across the stories that forced a personal reflection was the damaging impact of having expectations for your partner. Wanting your partner to be your idealized version of a partner rather than accepting who your partner is. This aspect is most readily seen in the story of Dom and Nzinga, but it shows up in the other stories as well. We should not try and push our partners towards some idealized version of them. An idealized version of a person is a falsehood. Being a good partner is about listening, being open, honest and supportive.\nIf you want your partner to be some idealized version of a person they will always fail to meet your expectations.\nQuotes  Diab in Yazz’s room Yazz tells Waris it’s important to counterbalance the state of being cerebral with the state of being corporeal Waris asks her if she means they need to do physical activity because they spend too much time thinking? yes, that’s it, Yazz says, making elaborate movements with her arms as she dances why didn’t you just say that then?\n  Courtney replied that she herself is a big fan of the non sequitur which really only means that a conversation is free-flowing and intuitive, as opposed to following a predictable trajectory, so to speak\n  (typical English, all this sharing preamble instead of just speaking directly about the matter at hand)\n  it was a debate, the other day, about whether a poem was good because they related to it, or whether it was good in and of itself\n  it’s easy to forget that England is made up of many Englands\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/girl_woman_other/","tags":["Fiction","Reflections"],"title":"Girl, Woman, Other"},{"categories":["Papers"],"contents":"By Djallel Bouneffouf and Irina Rish\nSummary The authors begin the paper by highlighting that Multi-Armed Bandit (MAB) algorithms have gotten a lot of attention recently for their ability to learn with less feedback. Multi-Armed Bandit approaches thrive in situations where we can not use a supervised approach and have to make decisions with minimal feedback. This paper is a survey of the top recent developments in the field. They authors introduce a taxonomy to classify different types of MAB problems and explain the state of the art in each category.\nA MAB problem is a sequential decision making problem. At each point in time, an agent must choose the best action out of several alternatives. After the agent picks an action the agent observes the reward from that action. Observing the reward from the action gives us more information to help make a better decision in the next time period. At the core of MAB problems is finding a trade off between exploration and exploitation. Exploration is making a different decision and learning information about the reward from that decision, and exploitation is making a decision we have already made because we know it\u0026rsquo;s reward. In a MAB problem agent must learn how to make decisions to maximize the cumulative reward over all time periods.\nA specific type of MAB problem is a Contextual Multi Armed Bandit (CMAB). In CMAB there is a context, or feature set associated with each action. For example, if you want to pick which drug a person should take, then the context is information about the drug. The context might consist of who the manufacturer is, the efficacy of the drug in trials etc\u0026hellip; For this problem, exploration is choosing a new drug, and exploitation is taking the same drug.\n After laying out the high level formation of a CMAB problem the authors explain different applications\n Healthcare: Use a MAB approach for conducting clinical trials. Rather than one trial with one result we can think of trials as sequential decision making. Finance: Sequential portfolio decision making. We get rewards from making decisions and then have to make another decision. Dynamic Pricing: A retail company has to determine what price to set their goods for each time period. Recommender Systems: A system has to balance recommending similar things versus new ones. The system has lots of information about content but very little information about each user. Influence Maximization: A product wants to maximize the number of users that become aware of a product by selecting a seed groups of users to expose. Information Retrieval: What document should be returned to a specific user based on a search Dialogue Systems: Use Multi Armed Bandits to pick the best response to a piece of dialogue. Use MAB to make proactive chatbots. Having a multi-domain dialogue system where we want to choose which chatbot to use. Anomaly Detection: Maximize the number of true anomalies that is shown to the human. Having both exploration and exploitation is imperative to adapt to changes.   Next the authors discuss how bandits can be used in machine learning.\n Algorithm Selection: We can thinking of picking which algorithm to use in a problem as a MAB setting. Hyperparameter optimization: This can be turned into a bandit problem. Feature Selection: we want to only use a small subset of features, we need to pick features Active Learning: Labeling all examples is impossible so we need to pick which examples to label.  Thoughts In the beginning of the paper the authors assert that they create a taxonomy for MAB problems, but they never explain their taxonomy in the paper. They show the taxonomy in a couple of charts but never explain the different groups. The authors also do not look at how different algorithms work better for different groups in their taxonomy. They provide very little guidance about which version of MAB to use or the tradeoffs between different versions. I was hoping to see some discussion about how different versions of MAB work better for different problems.\nHere is the authors taxonomy explained. They break MAB problems into four categories. A MAB problem is either contextual or non-contextual and it is either stationary or non-stationery. The authors explain what it means for it to be contextual, we observe some context about each arm that we can learn from. Pulling an arm gives us information about not just itself but other arms with a similar context. A non-stationery problem is one where the reward from pulling an arm actually changes over time. If you have to pick which stock to invest in, the reward from each stock changes over time.\n","permalink":"https://judahgnewman.netlify.com/writing/papers/2020/survey_mab/","tags":["Reinforcement Learning"],"title":"A Survey on Practical Applications of Multi-Armed and Contextual Bandits"},{"categories":["Books"],"contents":"Twitter and Tear Gas By Zeynep Tufekci is a pointedly insightful read. I had been planning to read it ever since she was interviewed on Stratechery. It felt like a really good time to read this book with the BLM protests going on across the world. Twitter and Tear Gas is part a history of modern protests, part a personal reflection and part a deep dive on the societal impacts of technology.\nThe biggest theme I took away is how something or someone\u0026rsquo;s biggest strength is also it\u0026rsquo;s biggest weakness. Tufekci shows how some of the most powerful parts about technology also are it\u0026rsquo;s weakest. Technology allows people to organize protests very easily. But that ease of organization can make it harder for protests to have a lasting impact because they do not develop the necessary organizational skills. Just as the internet makes it harder for information to be censored, it makes it easier for people to be hit with information overload and not know who to trust. Tufekci does not make bold claims about technology being a panacea, but rather deftly highlights both helpful and harmful impacts it has had on protest movements.\nI was particularly fascinated by the last chapter about how authoritarian governments are responding in the technology era. Tufekci\u0026rsquo;s framing as censorship not just as blocking information but also drowning out information or making information hard to trust, is very helpful. It showed me that Trump is practicing censorship with all his calls of fake news. The focus on fake news has made it so people have no idea who to trust.\nThis chapter made me reflect on how building trust in the internet age is really difficult. It is something that happens gradually over time. We need organizations that work hard to build trust and that begins with not having ad driven business model. With an ad driven business model, a companies incentive is not aligned to develop trust. With ads as the primary source of revenue, you want to maximize eyeballs on your content, not viewers trust in your content.\nA media organization paid solely by subscribers could provide a lot of value by being a source that people can trust. The organization can take time to publish on things until they see the full picture. There is no incentive to be the first one to a story. They can say, hey nothing really new has happened this week, we do not have new content for you. We need media organizations or just organizations in general to be focused on building trust.\nThe other incredible thing to me about this book was how much I learned about technology broadly by studying technology in such a narrow way. Tufekci was specifically interested in how technology has impacted modern protests. She was not trying to make sweeping conclusions about technology in general. By studying such a narrow space, Tufekci was able to ask very specific questions. Asking specific questions allows us to actually answer them and get a better understanding. Asking and answering these narrow questions can give us insight into understanding larger ones.\nQuotes  These signals of underlying capacities often derived their power from being threats or promises of what else their participants could do—if you could hold a large march, you could also change the narrative, threaten disruption, or bring about electoral or institutional change.\n  In the twenty-first century and in the networked public sphere, it is more useful to think of attention as a resource allocated and acquired on local, national, and transnational scales, and censorship as a broad term for denial of attention through multiple means, including, but not limited to, the traditional definition of censorship as an effort to actively block information from getting out.\n  Platforms that have nonpolitical functions can become more politically powerful because it is harder to censor their large numbers of users\n  What gets lost in popular accounts of the civil rights movement is the meticulous and lengthy organizing work sustained over a long period that was essential for every protest action\n  Network internalities are the benefits and collective capabilities attained during the process of forming durable networks which occur regardless of what the task is, or how trivial it may seem, as long as it poses challenges that must be overcome collectively and require decision making, building of trust, and delegation among a semidurable network of people who interact over time.\n  Cars certainly promote certain behaviors and nudge people’s activities in new directions, but they do not do this in a simple, uniform manner\n  That the boundaries of race are socially constructed does not make their effects any less real or patternless.\n  Social movements regularly face police pressure and even severer types of repression, and their ability to shift their protest tactics is often a key determinant of whether they can survive in the long term\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/twitter_and_tear_gas/","tags":["Non-Fiction","Policy","History"],"title":"Twitter and Tear Gas: The Power and Fragility of Networked Protest "},{"categories":["Books"],"contents":"Hacking the Electorate By Eitan Hersh is an depth look at what information is accessible to campaigns and how they use that information to interact with voters. Hersh contextualizes his work by highlighting that he is focusing on the perspective of a campaign. A lot of previous research is through the lense perspective of voters. Hersh\u0026rsquo;s main argument is that by looking through the perspective of campaigns we can understand how campaigns perceive voters. This view gives allows us to form hypothesize about what actions campaigns are going to take.\nHacking the Electorate is clearly written by an academic. It reads almost like an academic paper, albeit an extremely long one. This writing style made the book slow at certain times but it really drives Hersh\u0026rsquo;s points home. There is a lot of repetition of the essential points.\n To oversimplify the book into one paragraph, here is Hersh\u0026rsquo;s argument. Campaigns only have limited information about individuals which comes from public voting records. These records contain information about every individual that has voted. In some states they contain information about the ethnicity of voters or which party voters are affiliated with. The contents of this information impacts how campaigns perceive voters. Campaigns perceptions influence their interactions with voters which in turn affects who participates in elections. One example of this is that some states collect party registration information. In these states campaigns know which voters are the same party as them. Campaigns who know potential voters\u0026rsquo; parties will spend more time trying to increase turnout of than anything else. This leads to campaigns interacting with people who are the same party as them rather than the general electorate.\nWhile the book is a bit of a dry read, Hersh does a good job making compelling arguments. For each argument he highlights some reasons why it might be wrong and then attempts to rebuke them. In some instances he is very successful in this and others less so. I found the chapter about party identification and how that drives campaigns to be very convincing. The chapter about how racial identification drives actions was less convincing. There are some common traits about the states that keep ethnicity information about voters which could be driving differences. All of the states that record race are southern states. Hersh does not thoroughly address why these states keep race information and how keeping race information might be correlated with other factors.\nAt the end of the book, Hersh discuses the consequences of his research. He points out some undesirable characteristics of the current paradigm and changes that could be made. I really appreciate his idea about allowing voters to see what information is stored about them and to edit it if necessary.\n It is easy to read this book and trend towards an extreme. Politicians are using data collection to help them target certain voters. This information allows politicians to target different voters with different messages. There is no longer a cohesive vision of who a politician is when they are different to different voters. When voters have different perceptions of politicians due to imbalances of information, that can led to fracturing and polarization. A possible conclusion from this line of thinking might be to ban microtargeting and certain types of data collection. banning I believe that trying to stop these technologies, while coming from a good place, is a misguided direction. These technologies are already ingrained in our society. While there are negative consequences of them there are also positive ones.\nRather than a discussion about whether we want these technologies, we should be think about how we want these technologies to be used. I love Hersh\u0026rsquo;s push for more transparency about data collection. Let\u0026rsquo;s both increase transparency but also increase individual control. Voters should know what information is being stored about them, who is accessing it and how it is being used. Voters should have to give consent before their information is used. When their information is used they should know what was accessed and what was done with it.\nWe should also push for this technology to make better governing rather than more targeted campaigning. We should use voter information to give citizens more resources about public services. We can create better targeted social services. Rather than outlawing the technology as bad because of how it is currently being used, we need to imagine a world where it is being used in a way that upholds the values we cherish.\nQuotes  State simplifications such as maps, censuses, cadastral lists, and standard units of measurement represent techniques for grasping a large and complex reality\n  As with other aspects of the law, seemingly small policy differences can have consequential effects on the conduct of campaigns and the outcomes of elections\n  Voters are complex, and campaigns use data to perceive a simplified version of that complex reality\n  their efforts will only take them so far; as long as the root population data come from public records, campaigns will only be able to stretch these data so far in forming predictions of attributes like the propensity to be uninformed about political issues\n  Ordinary voters ought to be able to at least access and perhaps comment on the profiles that political campaigns, parties, and associated companies build about them. In the end, opening up databases to voters might be in the interest of political campaigns and parties as well, not just of voters.\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/hacking_electorate/","tags":["Non Fiction","Policy"],"title":"Hacking the Electorate: How Campaigns Percieve Voters"},{"categories":["Books"],"contents":"Exhalation by Ted Chiang is a set of short stories that really showcases some of what I love about sci-fi. Chiang uses each story to explore our contemporary culture in a way that is only possible through sci-fi. Good science fiction is not writing about a different. It is writing about our own society and using other places to provide a context that would not be possible otherwise. By looking at and asking questions about what the world would look like in other scenarios we can better understand our own world.\nAlmost every story in this set is actually incredible. In each story Chaing showcases a different writing style and a different angle. Some of the stories are long and really suck you into the world and some are short and leave you with an uneasy feeling. Each one forces you to reflect more on your own existence and put it in context of other types of existence.\n Often really good sci-fi is driven by a couple of key concepts. There is one idea that the writer wants to explore and that exploration drives the entire novel. In the first story in this set Chiang explores what time travel would look like if we could not actually change the past or the future. In another story Chiang explores the idea of cultivating artificial intelligence over a very long time period. Sci-fi stories are often driven by one key idea. This does not only show up in Chiang\u0026rsquo;s writing but other famous stories as well. In Snow Crash by Neal Stephenson there are a couple of key ideas that drive the majority of the plot. The Three-Body Problem is another novel that is really driven by a couple of key concepts.\nWhile I loved the stories in Exhalation there were ways it was limited by being short stories. I know that not every story needs to do everything. Especially with short stories, there is only so much you can do in a story. But Chiang\u0026rsquo;s story were missing some depth in their characters. The ideas that Chiang was exploring all seemed to be driven by a voice of god narrator. The exploration was not through characters changing and interacting with their surroundings as much.\nI wonder if having a story driven by one or a couple of key ideas leads to authors neglecting other parts of a good story. My favorite sci-fi writing is that of Ursula Le Guin because she does not give up character depth. She is able to maintain characters that change and are complex while still having key ideas that drive the novel. If you have not read Left Hand of Darkness I highly recommend checking it out.\nQuotes  But in truth the source of life is a difference in air pressure, the flow of air from spaces where it is thick to those where it is thin\n  Pretend that you have free will. It’s essential that you behave as if your decisions matter, even though you know they don’t. The reality isn’t important; what’s important is your belief, and believing the lie is the only way to avoid a waking coma. Civilization now depends on self-deception. Perhaps it always has.\n  They’re blind to a simple truth: complex minds can’t develop on their own \u0026hellip; For a mind to even approach its full potential, it needs cultivation by other minds.\n  if you want to create the common sense that comes from twenty years of being in the world, you need to devote twenty years to the task.\n  In most cases we have to forget a little bit before we can forgive; when we no longer experience the pain as fresh, the insult is easier to forgive, which in turn makes it less memorable, and so on. It’s this psychological feedback loop that makes initially infuriating offenses seem pardonable in the mirror of hindsight.\n  People are made of stories. Our memories are not the impartial accumulation of every second we’ve lived; they’re the narrative that we assembled out of selected moments.\n  Instead I will say that it’s easier for me to appreciate the benefits of literacy and harder to recognize everything it has cost us.\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/exhalation/","tags":["Science Fiction"],"title":"Exhalation"},{"categories":["Papers"],"contents":"By Lucas Bernardi, Tehmis Mavridis, Pablo Estevez\nSummary This paper from Booking is not focused on algorithmic advances but on how the company has derived value from using Machine Learning. The authors argue that more papers similar to this one need to be published.\nThe authors highlight some parts of the Machine Learning process that are specific to booking. They do this to explain why there situation is different then other technology firms. - At Booking a bad recommendation is quite costly. A bad trip is much worse than recommending a bad song or television show - They have minimal support for a lot of tasks. Guests travel quite infrequently and past history is not always relevant\nPart of the paper is exploring how we quantify the impact models are having on the business. Does high model accuracy automatically mean high business impact?\nThe authors provide a different lesson for each stage of the machine learning process: Inception, Modeling, Deployment, Monitoring, and Evaluation.\nInception\nThe authors broadly define machine learning models as tools to help you learn from your user base. This tool could be very specific such as optimizing the size of a layout. It could also be very broad and abstract such as understanding how flexible a user is about their trip.\nThe authors divide models into the following categories:\n traveler preference models: how flexible are you about aspects of your travel traveller context models; what is the theme of the trip, who are they going with, why are they going, what is the purpose of the trip item space navigation models; take in data about user interactions, clicks, scrolling, etc and then try to show most relevant items user interface optimization models: what the user interface should look like content curation: what format should content come in: free text, images content augmentation: augment that content with context, example: how good of a value is this, what is the price trend on this  Modeling\nThe authors propose estimating model business value using RCTs. We can compare key metrics such as conversion, customer service tickers or cancellations. The authors find that there is not necessarily a correlation between offline performance and increased business value. This means a more accurate model does not always mean a better business.\nThe authors propose the following explanations:\n value performance saturation: gains in accuracy do not actually mean business gains after a certain point uncanny valley effect: if a model is too accurate and predicts what a user can do it turns a user off proxy over-optimization: optimizes a proxy and not the real thing. driving clicks instead of conversions.  A key part of successful machine learning is being thought about how we set up the problem. There are multiple different set ups for the same problem. Whenever the team is starting to think about building a model they look at the following things\n learning difficulty: How well does a very simple baseline approach do. Prefer problems where baseline approach does a lot better than random data to concept match: how well does out data match what we are modeling selection bias: is our data biased because of the way we are getting it  Deployment\nThe authors note that throughout Booking latency increase conversion. Machine learning models take time and therefore decrease latency. The authors want to minimize how much they are increasing latency so they suggest using models that are sparse with fewer parameters. They also suggest minimizing feature transformations.\nMonitoring\nWhen models are in production it is critical to monitor the performance to assess if model drift is happening. Monitoring of models is hard for the following reasons\n incomplete feedback: we do not know always know the true labels, we just see the predictions delayed feedback: we will sometimes not see the true label until much later  The authors suggest using response distribution analysis for monitoring. response distribution analysis involves inspecting a histogram of model predictions. We can use the following heuristics.\n A good discriminatory model has peak at 1 and peak at 0 A bad model is unimodel with a center in the middle predicting one value a ton more than everything else might be bad. very noisy and non smooth distributions are a problem. differences in distribution between the training data and the predictions in product might be an indicator of drift  response distribution analysis is connivent because it can be applied to any classifier. It is robust to class distribution and addresses both incomplete and delayed feedback.\nEvaluation\nAs mentioned earlier the authors use RCTs to evaluate their models. They use use three groups rather than the traditional treated and not treated There is control group C that experiences no change, There are two treatment groups. Both invoke the model but only in T1 do users see the change.\nThis approach has two benefits. It allows us to isolate the impact of the decrease in latency and the impact of the model. It also gets rid of problems with eligibility criteria because some set of users will never even be eligible for the model.\nThoughts Overall this paper did not provide anything ground breaking but it did provide good context and insight into how a company is using Machine Learning. It is nice to see a paper from Booking. Most of the papers that come out of Industry are coming from very large organizations that do a lot of research like Google, Facebook or Amazon. This paper gives more insight into how a smaller company that is not as cutting edge is using ML.\nAcademic papers can often be so divorced from actual reality that in can be difficult to see the real impact that they have. It is important for academia to do research that does not have an immediate tangible impact, but it is also important to have research on how people are using these tools. Academia would undoubtedly benefit in some places with a tighter feedback loop with actual implementation. Is the research that academia is doing mattering and how are people using it? One of the highlights from this paper is that latency matters a lot. This might suggest more research should be focused on decreasing latency and making smaller models rather than continuing to make larger and larger models.\nIndustry on the flip side can also learn a lot from academia. Not every project needs to have a tangible business impact than can be seen immediately after the research is done. It is important to work on some projects that do not have an immediate use case. It is important to pursue things because they interest you sometimes rather than because it is good for the business.\n","permalink":"https://judahgnewman.netlify.com/writing/papers/2020/ml_at_booking/","tags":["Machine Learning","Software Engineering"],"title":"150 Succesful Machine Learning Models: 6 Lessons Learned at Booking.com"},{"categories":["Books"],"contents":"Hyperion by Dan Simmons is likely to join the group of sci-fi series where I read the first novel in the series and then do not read the others. This in no way means that I did not enjoy the book, but I find the first book in sci-fi series to be the most compelling. The world building and the introduction of characters is so gripping. There are so many books out there that I am compelled to read and so I often do not continue with the series.\nWhile Hyperion was structured as one novel, it was really a set of short stories that maintained some continity and took place in the same world. Each story had its own style and its own narrator. Structuing a novel in this way allowed for many different narrators to express opinions about the same topic. Each story had a relation to the Shrike. The stories were so in depth and stand alone that it wasy easy to forgot they existed inside a larger story.\nQuotes  the fact that we spend most of our mental lives in brain mansions built of words means that we lack the objectivity necessary to see the terrible distortion of reality which language brings\n  She had always felt that the essence of human experience lay not primarily in the peak experiences, the wedding days and triumphs which stood out in the memory like dates circled in red on old calendars, but, rather, in the unself-conscious flow of little things—the weekend afternoon with each member of the family engaged in his or her own pursuit, their crossings and connections casual, dialogues imminently forgettable, but the sum of such hours creating a synergy which was important and eternal.\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/hyperion/","tags":["Science Fiction"],"title":"Hyperion"},{"categories":["TextBooks"],"contents":"I have decided to combine my notes from chapters four, five, and six into one post. These chapters are light on new information and heavier on examples and applications. The power of this book is the number of examples it provides. One way to learn is to look at examples and then adopt them for your own use case. This book is perfect for that type of learning.\nChapter 4 Chapter four is centered on the law of large numbers. The author assumes that many people have not heard about law of large numbers but I learned about the law of large numbers many times during undergrad. We had to derive the law in three different classes I was in.\nI believe the author is highlighting the law of large numbers because he believes the application of it is not always intuitive. People understand the law of large numbers really easily when working with simple data sets. But as our data sets become more complicated we forget to think about the law of large numbers.\nHere is an example where I think the author really wants us to think about the law of large numbers. We have a data set that tracks student performance across different schools. We might want to include in our model some impact unique to each school. We have to be careful because it is possible that we do not have a lot of data for some schools. We build one model for all students but we lose track of the fact that our model will be more accurate and less noisy for schools where we have more data.\nTHe problem is that we treat all inference the same. Imagine we built a logistic regression model. While logistic regression does have confidence intervals for the learned coefficients, practitioners do not always use them. This is one reason baking uncertainty into all of our models is critical. If we include uncertainty then we will know that our estimate for kids in schools where we have less data is more uncertain than in schools where we have more data.\nChapter 5 Chapter 5 introduces the idea of loss functions. When we are training traditional Machine Learning techniques the loss functions are often directly baked into training. When we train a neural net we have to specify what our loss function is. The author shows how we can use loss functions with models we have trained with pymc3 to make our models flexible to different contexts.\nIncorporating different loss functions is important because it allows us to treat different types of errors differently. Rarely do all types of errors have the same consequence. For example, over predicting might be more dangerous than under predicting. Or under predicting by 2% might be a lot worse right around 0 then it is in other places. Loss functions allow us to tie our models closely to the business context we are using them in.\nThe author uses an example of stock market returns. We care about the difference in our prediction from the truth but we also really care about the sign difference. It is much worse for us to get the sign wrong in our error then an equivalent error of the same margin where we do not get the sign wrong.\nChapter 6 Chapter 6 is all about how we pick our priors. The author breaks priors down into two separate categories. One type of prior is an objective prior. Objective priors are ones where we are not including any additional information. Every point in our solution space is equally likely. Subjective priors on the other hand are where we include some information and therefore make certain places in our solution space more likely and others less likely.\nIn this chapter the authors explore how to get information from domain experts and to include that in our model through priors. Often domain experts are not statistics experts and so we do not want to ask them to give us a distribution. Instead we want to solicit feedback in a way more native to them and then we can turn their input into a distribution. Earlier this year I read a paper that was a survey of different ways to do this.\nTwo useful resources that were linked in this chapter. The chapter mentioned how it can be helpful it useConjuge priors for mathematical simplicity. The chapter also introduced me to the idea of prior distributions over matrices. This is really powerful when we want to predict multiple things that are correlated with each other. The Wishart Distribution and the [LKJ distribution] (http://bois.caltech.edu/distribution_explorer/multivariate_continuous/lkj.html) are distributions over matrices.\nAt the end of the chapter the author shows my favorite derivation I learned in school. The derivation shows that penalized linear regression is the same as bayesian regression with certain priors. We show that using l1 or l2 norm corresponds to different priors. Understanding regularization as a prior was such a helpful step for me.\n","permalink":"https://judahgnewman.netlify.com/writing/textbooks/bayes_hackers/bayes_hackers_4_5_6/","tags":["Data Science","Statistics"],"title":"Bayesian Methods for Hackers: Chapters 4, 5 and 6"},{"categories":["TextBooks"],"contents":"Notes Chapter 3 in Bayesian Method for Hackers is center on Markov Chain Monte Carlo (MCMC). It is possible to use pymc3 and bayesian models without understanding the underlying algorithms. Understanding this algorithms plays a pivotal role though if we want to be serious practitioners. When our model works out perfectly, not knowing the underlying algorithms is fine. But if we are trying to do anything complicated, our first attempt will almost certainly not work. When we have to debug and iterate, understanding how the algorithm works and how to diagnose problems becomes critical.\nThe chapter begins be having us think about the parameter space for our model. If our model has N unknowns then the solution space for our model is N dimensional. Using bayesian models, we identify what spaces in that N dimensional space are more likely and which are less likely. If we use non-bayesian methods then we identify just one point in that N dimensional space.\nPriors in our bayesian model provide likelihoods for different places in our solution space. Often we will use a uniform prior which means everywhere in our solution space is equally like. After specifying our prior we then include in our data. When we include data, the solution space stays the same, but likelihood of different areas changes. Areas that line up with our data become more likely and areas that do not align with our data becomes less likely.\nMarkov Chain Monte Carlo (MCMC) So how do we find the areas in our solution space that are most likely? MCMC is an algorithm that does an intelligent search of the solution space. If the solution space is very large it could be difficult to search the entire space. MCMC works by exploring the nearby areas and moving towards areas of higher probability. If MCMC works it will move to the area of the highest probability and then sample many times in that area. Once we have gotten lots of samples in the highest probability area we can create a histogram of our models parameters.\nHere is an outline for MCMC algorithms:\n Start at a position (our prior) Propose a new position Accept/Reject position based on adherence to data \u0026amp; prior If Accept move to new position, return to 1, if reject return to 1 After large number of iterations, return all positions  MCMC algorithms are memoryless. The algorithm only cares about the current position. The next step is only dependent on the step before it and not a long list of steps. The algorithm is memoryless. It does not care about how it got to where it is.\nDiagnostics Once we have run MCMC, how do we know if our result is a good one. The best way to diagnose MCMC is too look at a couple of different plots. Pymc3 comes built in with a lot of nice plotting tools. There are three different plots that are very helpful is assessing your fit.\nA posterior plot allows us to view histograms for the posterior of parameters in our model. We can inspect what is the most likely value for a parameter and what is the uncertainty about that parameter.\nA trace plot allows us to view a scatter plot of where MCMC was sampling. Ideally we will see a fuzzy caterpillar. We want MCMC to be walking around in the highest probability space.\nAn autocorrelation plot allows us to view how related each sample is with the previous sample. When we do MCMC we want our chain to have low autocorrelation. This is not a sufficient condition but a necessary condition. If we have high autocorrelation then our chain is not doing a random walk in the highest probability space.\nSome extra MCMC Thoughts / Notes   When we are sampling our parameters we should not mix different samples together. If the value of two parameters is related this will lead to bad inference.\n  It can help with convergence to start at good values. This does not mean setting our prior to be at good values but rather setting our testval to be something that makes sense. It is especially good to start near the peak rather than far away from it. pymc3 comes built with a capability to do this. We can specify that starting point should be the maximum a posterior estimate or the map.\n  When using MCMC, we often have to specify a burn-in period. The burn-in period is when we are still navigating to the region of highest probability. We do not want to include this time period in our final posterior histograms. We discard the burn-in period.\n  Thoughts   I really appreciated the approach of this chapter. It was really helpful for me to think about the solution space for our model. It can be really helpful to return to fundamentals when learning a new algorithm or working on a problem. For anytime of model there will also be a solution space and we can always use that to ground our thinking.\n  To help the reader understand how MCMC was moving the author kept using the metaphor of traversing a mountain. We can think about optimization in terms of finding valleys. I find that metaphors are one of the most powerful way to explain things to people. It can be really difficult to grasp concepts in the abstract. Using a metaphor to ground thinking is so helpful.\n  ","permalink":"https://judahgnewman.netlify.com/writing/textbooks/bayes_hackers/bayes_hackers_3/","tags":["Data Science","Statistics"],"title":"Bayesian Methods for Hackers: Chapter 3"},{"categories":["Books"],"contents":"Trick Mirror by Jia Tolentino is a series of essays exploring identity and the self in our current era. Tolentino blends personal stories and self reflections with larger dissections of structural systems in our society. I found Tolentino\u0026rsquo;s writing incredibly compelling. She does not prescribe absolutes and allows nuance to shine through. She does not prescribe exact solutions but rather examines structures in our society and how she navigates them.\nTolentino\u0026rsquo;s essays focused on how large structures pressure and impacts individuals in different situations. Whether it is the internet, reality tv, or weddings, Tolentino was continually struggling with how to be an individual in these large powerful systems. In each essay Tolentino explores how much impact can an individual have when they are part of these societal wide structures. Is an individual able to make a difference or are some issues a result of larger structures? How can we keep moving forward when it feels like our individual efforts do not make a difference?\nOne example of this difficulty between individual action and larger structural incentives is the internet. Tolentino argues that the internet is inherently structured to manipulate and takes advantage of us. But is opting out of the internet really an option. Is it even possible that we could turn the internet into something that we want it to be? If the the base incentives of life on the internet are misaligned with our visions and hopes, can we even make a difference as an individual? How do we think about collective action?\n The other theme that stuck out to me across the essays were Tolentino\u0026rsquo;s thoughts on the continuity of identity. She covers growing up in Texas, going to college, being in the peace corp, going to graduate school in Michigan and living in New York as a writer. Even though Tolentino\u0026rsquo;s life goes in many different ways, she highlights a continuity of self. This continuity is most apparent in her essay about reality TV. She reflects on her time as a contest on a reality tv show and how she sees a lot of her current self in that character.\nWhen I think about different periods of my life, the majority of my focus is on differences. I reflect on differences in my social interactions and what tasks were occupying my time. It was interesting to see Tolentino focus on how she has stayed the same in a lot of ways. She has clearly changed a lot as well, but in creating a cohesive narrative her focus seemed to be on those similarities. I would be curious to read more from her about how she has changed. It might also be instructive for me to reflect on ways past me is similar to current me.\nI thought Tolentino shined the most as a writer when she let her personality come through. Some of the essays were more focused on who she was and some were more historical or analytical pieces. The best essays where when she was able to combine both of these together.\nQuotes  The last few years have taught me to suspend my desire for a conclusion, to assume that nothing is static and that renegotiation will be perpetual, to hope primarily that little truths will keep emerging in time\n  It’s as if our culture has mustered an immune-system response to continue breaking the fever of gender equality—as if some deep patriarchal logic has made it that women need to achieve ever-higher levels of beauty to make up for the fact that we are no longer economically and legally dependent on men.\n  A more expansive idea of beauty is a good thing—I have appreciated it personally—and yet it depends on the precept, formalized by a culture where ordinary faces are routinely photographed for quantified approval, that beauty is still of paramount importance.\n  the idea that personal advancement is a subversive form of political progress has been accepted as gospel. The trickiest thing about this idea is that it is incomplete and insufficient without being entirely wrong.\n  the values of celebrity—visibility, performance, aspiration, extreme physical beauty—promote an approach to womanhood that relies on individual exceptionalism in an inherently conservative way\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/trick_mirror/","tags":["Non-Fiction","Reflections"],"title":"Trick Mirror: Reflections on Self Delusion"},{"categories":["Papers"],"contents":"By Linda Flower and John R. Hayes\nSummary This is an older paper from 1981. The authors provide a model to answer the following question \u0026ldquo;What guides the decisions writers make when they write?\u0026rdquo;\nThe paper is organized in four different sections that build on top of each other. The authors collected data using a protocol analysis where participants were asked to write an essay for Seventeen Magazine about their job. As the participants were writing they were told to think aloud. There was a tape recorder placed next to participants. The observations in the paper come from analyzing the transcripts of all the participants.\nObservation 1: Writing is completed through a set of distinctive thinking processes. Previous models of writing have been stage models. Stage models reflect the growth of the written product. A classical stage model would divide writing into three stages. Pre-writing, Writing, Re-Writng. The authors instead propose a process model. In a process model the units of analysis are elementary mental processes. Rather than focusing on the final product, we focus on what the writer is doing.\nThe authors emphasis that a writers surroundings and context impact them during the writing process. Here are two observations from the paper centered on how the environment impacts writers.\n If a writer\u0026rsquo;s representation of her rhetorical problem is inaccurate or simply underdeveloped, then she is unlikely to \u0026ldquo;solve\u0026rdquo; or attend to the missing aspects of the problem.\n  each word in the growing text determines and limits the choices of what can come next.\n The authors highlight that studying children can help us learn more about the writing process. Studying children is an easy way to observe participants improving their writing skills. By observing children we learn how certain skills develop.\n2. These thinking processes are hierarchical and embedded within each other In previous writing models the process was a linear one. The writer would first accomplish one task in full and then move on to the next task in full. The authors of this paper posit that writing does not follow a linear structure. There are actually cycles and some processes happen inside of other processes. For example, writing one sentence might be a mini version of the entire writing process.\n3. Writing is guided by goals The authors draw a distinction between content oriented goals and process oriented goals. A content oriented goal is about an end product. A process goal is about what actions the writer is taking. An example of a process goal is \u0026ldquo;I just want to get all my ideas on paper\u0026rdquo;. One observation from the paper is that good writers use more process oriented goals. Using process oriented goals gives the writer conscious control over their work flow.\n experienced writers \u0026hellip; can often draw on elaborate networks of goals which are so well learned as to be automatic\n The authors also argue that poor writers do a bad job of forming high level goals sometimes only relying on lower level goals.\n4. There are high level goals and then smaller sub level goals The authors observe that there are different goals we interact with as we write. There is the high level goal of the entire writing piece. Then there are smaller goals for a section or a paragraph or even just a sentence.\n The distinctive thing about good writers is their tendency to return to that higher-level goal and to re-view and consolidate what has just been learned through exploring.\n  Yet this process of setting and developing sub-goals, and at times regenerating those goals is a powerful creative process.\n Thoughts While I was reading this paper I found it very helpful to apply this model or the lessons from this model to a different field. I spend the majority of my time programming rather than writing. It was helpful to think about some of the lessons from this paper and applying them to coding. Coding makes it really easy to have lower level goals. What do I want this function to do? What do I want this class to do? I sometimes struggle while I code though, to remember the higher level goal. I should try to keep the higher level goal in mind more and return to it as I worker on lower level goals.\nIf some of the ideas from this paper can apply to software engineering, then some ideas from software engineering can be applied to writing. One thing that is really powerful about software engineering is the use of tests. We use tests to verify that functions are meeting our lower level goals. Could we bring the idea of tests to writing? One idea is that getting other people to read your writing is similar to testing it. If I would not publish code that does not have tests then I should not publish writing which has not been read by some testers first.\nThere are two ideas that the authors return to many times which I found to have some harmony with each other. The first idea is that the text is a constraint. Once you have written a sentence it limits the possible outcomes of your next sentence. Once you have written a couple paragraphs the scope of your writing piece is very well defined. The other idea is that we learn more as we write and form new goals in response to that. It is impossible for us to have all of our goals figured out ahead of time. As we write, we learn more about what our lower level goals should be and how to achieve them. If we accept both of these ideas, then I would argue it sometimes makes a lot of sense to completely scrap our work and start from scratch. Scraping our work does not mean that the work was invaluable. The work was actually quite valuable, it helped us learn a lot. But, because the text we have written is constraining us, our possibilities are limited. They are limited by work we did when we knew less. Scraping that work gives us a blank canvas, working from a place where we know a lot more.\nThis paper attacks a fundamental question from first principles. Rather than extending prior work they use prior working as a way to contextualize their thinking and then start from scratch. While there approach is necessary because they are building from scratch, it is a helpful framework for all research.\n Identify the major elements of the question Explain how the major elements relate Use the model to help answer critical questions  I really appreciate that they use their model as a starting point for further research. Rather than creating a model to explain everything they recognize that a model is helpful because it allows us to craft further research agendas.\n Nevertheless, it is for us a working hypothesis and springboard for further research, and we hope that insofar as it suggests testable hypotheses it will be the same for others.\n  A model such as the one presented here is first and foremost a tool for researchers to think with. By giving a testable shape and definition to our observations, we have tried to pose new questions to be answered.\n Earlier this year I read the Art of Learning by Josh Waitzkin. The following quote reminded me of something Waitzkin talks about.\n If the writer must devote conscious attention to demands such as spelling and grammar, the task of translating can interfere with the more global process of planning what one wants to say.\n Waitzkin talks about the importance of certain tasks moving from the conscious to the unconscious. When you are learning an instrument you learn the scales not to play the scales well, but so that certain techniques become unconscious. If you had to literally think about everything that you were doing, it would be impossible to do anything. This provides some evidence for why knowing how to spell things is still important. In the internet age you can look up anything. But having to think about how to spell takes up mental energy. When you do not have to think about spelling your mental energy can be focused on higher level processes.\n","permalink":"https://judahgnewman.netlify.com/writing/papers/2020/theory_of_writng/","tags":["Reflections","Design"],"title":"A Cognitive Process Theory of Writing"},{"categories":["Books"],"contents":"Conversations with Friends is the second book in the past month that I have read by Sally Rooney. Similarly to Normal People, this novel explores communication between people in romantic relationships. It highlights the dangers and destructive nature of expecting others to understand us. When we expect others to understand rather than communicating there will be failures. We make assumptions about how other people are processing events and get caught in the world inside our heads.\nConversations with Friends is particularly concerned with internal representation versus external understanding. Francis\u0026rsquo; actions are often perceived very differently then how she believes they will. Almost everyone believes that Francis is cold but she does not see herself that way. Characters constantly misinterpret each others actions and thoughts. This mismatch between internal view and external view blocks us from seeing how our actions impact others.\nIn both Normal People and Conversations with Friends Sally Rooney captures what college felt like for me. Life felt hectic and exciting. Situations change a lot from week to week. A lot feels out of your control. During college I struggled to be in touch with my own feelings. If I am not in touch with my own feelings it becomes that much more difficult to be aware of others.\nI found reading both of Sally Rooney\u0026rsquo;s books to be quite painful. It reminded me of college and how difficult that times was in a lot of ways. It reminded me of the times that I hurt other people and where things seemed to be spiraling out of control. It reminded me of being quite conflicted and not sure how to resolve anything. These books were painful because I am still so close to that period in my life. During my day to day now, that period can feel far away, but it is actually still quite close.\nIn some ways I am surprised how much people enjoy these two books by Sally Rooney. While they were meaningful reads, I am not sure how much I enjoyed them. I wonder if it was more difficult for me because I am still so close in age to the characters in the book. The further removed you are from their age the less you might identify with them and the easier it is to read? Maybe people enjoyed the book because of how difficult it was to read and how much it captured that difficulty.\nLast thought: The letter from Melissa to Frances in the middle of the book was an absolute masterpiece. It was incredible. This book is worth the reading just for those few pages.\nQuotes  Real writers, and also painters, had to keep on looking at the ugly things they had done for good. I hated that everything I did was so ugly, but also that I lacked the courage to confront how ugly it was.\n  I loved when he was available to me like this, when our relationship was like a Word document that we were writing and editing together, or a long private joke that nobody else could understand.\n  I closed my eyes and felt all the furniture in my room begin to disappear, like a backward game of Tetris, lifting up toward the top of the screen and then vanishing, and the next thing that would vanish would be me.\n  My relationship with you is also produced by your relationship with Melissa, and with Nick, and with your childhood self, etc., etc.\n  You underestimate your own power so you don’t have to blame yourself for treating other people badly.\n  You live through certain things before you understand them. You can’t always take the analytical position.\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/conv_friends/","tags":["Fiction"],"title":"Conversations with Friends"},{"categories":["TextBooks"],"contents":"Notes The chapter explains the syntax of pymc3 and how to turn different concepts into pymc3 code. It outlines the difference between a stochastic variable and determinstic variable\n A stochastic variable is random. This means even if all the information about the variable is known, we do not know what its value is. Drawing a value from a distribution is a stochastic variable. A deterministic variable can be determined from its parameters. Some examples of deterministic variables below  4 4 + 4    There is one tricky piece here. 4 + poisson(4) would be a stochastic variable but the following is a stochastic variable followed by a deterministic one.\n x = poisson(4) : stochastic 4 + x : deterministic   The author outlines a process for thinking about modeling when using Bayesian Inference. We should think about our model in the following process.\nHow is my data generated?\n What is the best random variable to describe my data? What do we need to know for that distribution? How can we calculate what we need to know for that distribution?  This process helps us tell a narrative about our data. There is a story about where our data is coming from and how it is generated. This narrative is desirable because humans understand stories intuitively. It provides a way to explain our data to other people.\nThis process also makes it trivial to sample / generate data. Once we have created our data generating process we can plug in different parameters and then generate data.\nThe author then runs through three different examples using this modeling approach and pymc3. Some of my thoughts about these examples can be seen below.\nThoughts   Examples are such a powerful way to learn. Referencing an examples makes it easy to extend an approach to more complicated problems. I really appreciate this book\u0026rsquo;s approach of using examples.\n  The author of the book repeatedly bashes frequentist approaches. Before the beginning of the A / B testing example he bashes significance testing. I kind of hate this rhetoric. It does not explore why people like frequentist approaches and what the benefits of a bayesian approach are. In this particular example the frequentist approach is actually very similar to the bayesian approach. The frequentist approach just assumes normality and does not make the assumptions it is using clear. Rather than bashing frequentist approaches the author should highlight how the bayesian approach is more flexible and makes our assumptions more evident.\n  In the last example, the author argues that plots should be used to assess goodness of fit for a model. I agree that plots can often add more information then just a metric, but the example that the author uses is poor. Separation plots are difficult to understand and do not add much information. For a logistic problem I would have provided a different plot. We could create a plot that has for each class the predicted probability as a function of the variable. We can then overlay on top of this the true results. This gives us a better sense of when we are predicting the wrong class and where the model has higher confidence.\n  ","permalink":"https://judahgnewman.netlify.com/writing/textbooks/bayes_hackers/bayes_hackers_2/","tags":["Data Science","Statistics"],"title":"Bayesian Methods for Hackers: Chapter 2"},{"categories":["Books"],"contents":"Go Ahead in the Rain: Notes to a Tribe Called Quest is a beautiful book. It blends a memoir, with a history of a rap group, with a history of a time period. While the book is ostensibly about the rap group, it actualls talks about so much more than one rap group. Abdurraqib writes a poetic style of prose that conveys so much emotion.\nAbdurraqib writes through the lense of his personal experience with the group. He talks about how the groups music affected him and how he understood it in context of other music that was being made at a similar time. By displaying the emotional significance of the music, Abdurraqib is able to convey more about the group then just recounting facts. While Abdurraqib\u0026rsquo;s account is inherently personal, it gives us insight into how the different group memebers were percieved and how they interacted with each other.\nA Tribe Called Quest holds a particular place in my own heart. I had not heard of a Tribe Called Quest before I started listening to them in High School. There music straddeled two worlds where I was not a part of either. A lot of my friends where in the jazz band. I did not play an instrument and so when they would play together all I could do was listen. Another section of my friends was very into the new younger rappers that were putting out music at the time. While I would try to stay abreast of this music I would always hear the most recent music later than everyone else.\nFinding and listening to A Tribe\u0026rsquo;s music was an important part of forming an identity for me. It was something that I had found that no one else was listening. I did not find it because of my older brother or my parents. I did not find it because of my friends. It was something that I enjoyed because I personally had found it.\nSimilar to Abdurraqib, I also thought Phife Dawg was the star of the show and not Q-tip. I knew that Q-tip had a more illustrious solo career but I never understood why. I thought Phife\u0026rsquo;s raps were better and enjoyed the sound of his voice more. When I look back at the celebrities / people I am attracted to, there seems to be a pattern. I always loved Russell Westbrook a lot more than I loved Kevin Durant. I am attracted to the little brother. The one who is trying to get out of the older brother\u0026rsquo;s shadow. It may or may not have something to do with my own relation to my older brother :).\nIn highschool I loved to put on an album and listen to it all the way through on the way to or from school. Now, I almost never listen to albums. When I listen to music, I find a spotify playlist and listen to that. I rarely listen to exclusively one artist, let alone one album. While spotify and streaming music have made listening to music very convienent, I wonder if they not only change how we listen to music, but how music is created. Artists know that nobody listens to an album all the way through. Rather than trying to create a cohesive album, artists are focused on making good singles.\nHere are some albums I have been listening to as I read this book.\n Monie Love: Down to Earth Queen Latifah: All Hail the Queen De La Soul Ice Cube: AmeriKKKa\u0026rsquo;s Most Wanted Dr. Dre: The Chronic Outkast: Aquemini  Quotes  I was a shy and nervous kid, wracked with anxiety before I understood what anxiety was. I was desperate for a way to wear silence in a way that looked and felt cool to anyone in my presence.\n  When there is nothing to point to as an excuse for distance, the distance looms larger, more difficult to push.\n  The collective is, more that anything, a support system. Musically, yes. But beyond that, it is a grouping of friends telling each other that their ideas are valuable, that someone will believe in them, even if the people who believe in them are confined to the same studio, tinkering in all of the same weird ways.\n  Sampling created a dialogue between past and present and helped bridge a gap between the music a rapper was first introduced to and the music they desperately wanted to share with the new world\n  I’m talking about what it is to be from a place that promises to love you while holding a gun to your neck. I’m talking about what it feels like to have the gun lowered, briefly, by the hands of some unseen grace. Sometimes, it is a protest that stretches long into a night, or sometimes it is a reading where a room hears familiar words and cries along with you as you read them out loud. But sometimes, it is a perfect album that arrives just in time to build a small community around you. To briefly hold a hand over your eyes and make a new and welcoming darkness of the world outside, even when it is on fire.\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/go_in_rain/","tags":["Non-Fiction","History"],"title":"Go Ahead in the Rain: Notes to A Tribe Called Quest"},{"categories":["Personal"],"contents":"I have been think about my workflow a lot of late decided to write my friend Tushar a letter. Here is that letter in full.\nHey Tush,\nI hope you are getting some good relaxation in this weekend. It is a rainy day here in Baton Rouge so we are trapped inside. I have been thinking a lot about my work flow since reading that Gelman piece post I forwarded you. I always find bouncing ideas off you to be really helpful. I have learned a lot from seeing and talking with you about your workflow. Here are some assorted thoughts, respond to whatever interests you.\nWhen I am working on a more strictly engineering task my workflow is a lot better and I am in a much better headspace than when modeling. This relates a lot to the debugging post you made last week. My thinking is much more systematic and defined when I am working on an engineering task. With engineering, I am able to build it up piece by piece, isolate different parts and have a good understanding of each part. It also feels like I am making tangible progress as I go.\nMy head space and process when I am modeling is much more chaotic. I will often just try something out and see if it improves my optimization metric. Very similar in a lot of ways to what you describe in your post. I do not reason about why making this change is important or what the significance of a parameter is. I just change the parameter hoping it will help.\nI have been thinking about this bad tendency for a while now. I wrote a post earlier this year about how incorporating the scientific method might help. Here is a snippet from that piece which I think is pertinent.\n We see the promise and potential of automating a task and forget to be systematic thinkers. We forget the larger goals and focus on one metric. There is a metric we can measure, so we optimize it. \u0026hellip;\u0026hellip; It is harder to measure how much of a system we understand than accuracy on a defined task. Our goal should be understanding of a system rather than optimizing a certain metric. Model building should be a process of using the scientific method to better understand the driving forces in the world around us.\n My suggestion of using the scientific method was derived from two factors. 1. The scientific method forces us to focus on understanding rather than specific metrics 2. The scientific method provides a desirable workflow structure\nJust knowing that the scientific method will help me has proven not enough to fix some of my bad habits. I will sometimes get into a good flow of defining hypothesis and then testing them out. I find that this is often when I will make the most progress on model iteration. As a bonus it also usually helps me find bugs in my code. But often I still abandon this process and focus on the one or two metrics I have defined.\n Here are two thoughts about more changes I can make to my workflow\n Building Model Understanding Functionality Early  Model understanding and interpretation should be a more integral part of my workflow. When I am evaluating a model it should focus less on the optimization metric. It should focus more on what the model is learning. I should build functionality that lets me see which features are important and how different features interact. I should then compare this to my a priori hypothesis and trends in the raw data. If I build this functionality it makes the scientific method structure a lot easier. I will have an understanding of how changes impact what my model is learning rather than the impact on the optimization metric.\nUsing State Machines  This idea is less concrete but has the potential to be more powerful. My thinking is more systematic if there is more structure in what I am working on. When my task is more narrowly defined it becomes a lot easier to wrap my head around. A couple of week ago Pointer contained an article about state machines. I believe a modeling workflow can be defined using a state machine. This state machine would break the process into smaller parts a give the process more structure.\nA state machine is a graph that defines the possible states a process can be in. The article explains \u0026ldquo;State machines not only encode which transitions are valid, in doing so they also encode which transitions are invalid.\u0026rdquo;\nWhy would using a state machine to model my workflow be so helpful? That state machine could enforce certain good practices and restrict bad habits. I would be forced to document before moving on to the next thing. Here is a very basic diagram of what a modeling workflow state machine could look like.\nThis diagram is quite simplistic but a useful starting point. There are probably more states and more cycles within the graph. Each red circle represent a state and then the blue squares represent an action that must be taken before transitioning states. It might even be possible to define sub state machines for each state, so model evaluation could have its own state machine.\nThis diagram is illustrative because it highlights some of of the underlying structure in our workflows. While each practitioner will have their own tendencies there is some structure that workflows will follow. By making restrictions on this workflow we can help enforce good practices and remove bad practices.\nI have some more thoughts related to all of this centering on documentation and information retrieval, but we can get to those later.\nRespond to whatever interests you most but here are a couple questions to get the blood flowing:\n What type of structures or frameworks do you use when modeling to help guide your thinking? What tools / information do you find to be the most helpful in model understanding? What am I missing with my State Machine representation? Not in terms of which states or connections, but the overall structure?  Thinking of you Tush! Hope you are doing well.\nJudah\n","permalink":"https://judahgnewman.netlify.com/writing/research/workflow_statemachine/","tags":["Data Science","Reflections"],"title":"Workflow State Machine: Letter to Tushar"},{"categories":["Textbooks"],"contents":"Discrete Distributions A discrete distribution on has specified values, such as integers. It has a probability mass function.\nPoisson The poisson distribution expresses the probability that a given number of events occur in a time interval. The larger the lambda the more times the event will occur.\n$$P(Z=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$$\n$$ E[Z| \\lambda] = \\lambda $$\nThe poisson distribution provides a probability for any positive integer.\nA larger lambda gives more to higher values. A smaller lambda gives more probability to smaller values.\nBernoulli The bernoulli distribution is for a value that can be either 0 or 1. The parameter p describes the probability the value will be 1 and (p-1) is the probability the value will be 0.\n$$P(Z=1) = p $$ $$E[Z | p] = p $$\nBinomial The binomial distribution parametrizes the number of times an event will happen in n tries. It is the extension of the bernoulli distribution from one attempt to n attempts. Each event is independent from every other event in the sequence.\n$$ P(Z=k){N \\choose k}p^{k}(1-p)^{N-k} $$\n$$ E[Z | p, N] = Np $$\nContinuous Distributions Can take on arbitrarily many values. It has a probability density function.\nNormal Distribution The normal distribution is one of the most common distributions.\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2} $$\n$$ E[x | \\mu] = \\mu $$\nExponential Distribution $$f_{z}(z | \\lambda) = \\lambda e^{- \\lambda z} $$\n$$ E[z | \\lambda] = \\frac{1}{\\lambda} $$\nA smaller lambda gives more density towards higher values.\nLogistic Function This is a common S shaped curve / function. Its values go from 0 to 1 or from 1 to 0 depending on the sign of beta.\n$$ p(t | \\beta) = \\frac{1}{1 + e^{\\beta t}} $$\n","permalink":"https://judahgnewman.netlify.com/writing/textbooks/prob_distr/","tags":["Statistics"],"title":"Probability Distributions"},{"categories":["Textbooks"],"contents":"Bayesian Methods for Hackers is an online textbook created by Cam Davidson Pilon. The book focuses on bridging the gap from theory to practice. The author argues that textbooks often focus on theory to the point where they become inaccessible. By focusing on accessability these methods become avaliable to anyone.\nNotes The first chapter begins with an outline of how to think in a bayesian frameworks. Bayesian methods are our process of updating beliefs after seeing new evidence. Rather than making a definitive decision these methods express their uncertainty. In the frequentist world, a probability is the long term probability of an outcome. In the bayesian world, a probability measures our confidence in a belief. As N goes to infinity then bayesian inference and frequentist inference become the same thing. Bayesian inference excels when we are dealing with small N.\nThe chapter contains a quote from Andrew Gelman which I found very interesting. He argues that we actually never have large N size. If we have a large N we should look at segments of our data to answer more interesting questions. If we are looking at a national data, as our data gets larger we are then able to look at trends by state or city.\nWhat is the probability of A given X can be notated as P(A | X). This can be a combination of our prior and our evidence. Bayes rule can be expressed as the following.\n$$P(A | X) = \\frac{P(X | A) P(A)}{P(X)}$$\nIn the rest of the first chapter the author goes through some examples of how bayesian thinking works and talks through a couple of different probability distributions. I have found that having a key grasp on the different probability distributions is really important. I have dedicated this page to probability distributions. I will continue to add to it whenever I encounter new distributions.\nProbability distributions can be used to model values that we are interested in. In the textbook the author looks at a couple of different examples. The classic example is using a Bernoulli distribution to model a coin flip or a binomial distribution to model a series of coin flips. The parameters of these probability distributions are hidden from us. We want to use the data to help infer what those parameters are. If we learn the parameters of the probability distribution we will then be able to make inferences.\nAt the end of the chapter the author provides an example of using Pymc3 to solve a problem. The author has data about his number of text messages every day. He wants to answer whether or not there was a change in his texting behavior during this period. He specifies a model for his texting behavior and then fits it using pymc3. This model shows that there likely was some change in his texting behavior because the parameter for the probability distribution likely changes.\nThoughts The example that the author used towards the end of the chapter is very interesting but left me wanting a lot. The author made the whole process look easy \u0026amp; simple. We specify a model and boom here is an answer. There are a lot of key parts of the modeling process that the author leaves out. We have to compare this model to other possible models. Is this model the one that best explains your behavior? How would we compare this model to one where we believed there were two changes to your texting behavior?\nOur results and model are directly a result of the questions we ask. The best model is determined by how much it helps us answer the question swe are interested in. The question the author asked was did my texting behavior change? I guess to answer this question, the model he proposes works well. A much more interesting question would be, how can we understand my texting behavior. Rather than assuming there was only one change we could compare different models with different numbers of changes.\nAnother question I like to ask myself is the following \u0026ldquo;If I had to solve this problem in a different way, how would I do it?\u0026rdquo;. Two compelling approaches came forward to me. With all of these approaches the number of clusters is a parameter.\n We could try clustering the data forcing clusters to be continuos over time. We would want to maximize within cluster similarity and minimize between cluster similarity. This would allow us to cluster our days into different segments of behavior. Another option would be to compare averages. Look at the average before a day cut-off and after a day cut-off. Find the day cut-off which maximizes that difference. Then keep repeating this process until we get the desired number of clusters.  Two more existential thoughts:\n  Is anything actually distributed by a probability function? Maybe as N reaches infinity, but for sample sizes we care about the probability distribution is always an approximation. This means that there is no true value for a parameter only our best guess given what we know.\n  I could see the following counter argument to a bayesian line of thinking. \u0026ldquo;Real life consists of decisions. While it might be nice to express your uncertainty, we have to make a decision. Why not incorporate that decision into the statistics, rather than off loading it to someone else.\u0026rdquo; This line of thinking is problematic for a couple of reasons. Uncertainty is a big part in assessing the trade-offs in a decision. Expressing uncertainty and being transparent allows individuals to make their own decisions and assessments rather than forcing it upon them. We should always lean towards being more transparent and giving people as much information as possible. This gives them the power to make an educated decision rather than being opaque and showing them a decision based on arbitrary guidelines. The idea that 5% significance should apply to all situations is completely ridiculous.\n  ","permalink":"https://judahgnewman.netlify.com/writing/textbooks/bayes_hackers/bayes_hackers_1/","tags":["Data Science","Statistics"],"title":"Bayesian Methods for Hackers: Prologue and Chapter 1"},{"categories":["Books"],"contents":"Severance by Ling Ma is easily one of my favorite books of the year. Severance elegantly ties together many different genres. It is an immigrant story, a coming of age story, a story about heritage and an apocalypse story. Ma blends these different stories into one cohesive narrative. Often during apocalypse novels the majority of the focus is on surviving and how the world has changed. Tieing the apocalypse to an individual\u0026rsquo;s history allowed us to explore how Candace\u0026rsquo;s past impacted her actions during the apocalypse.\nThe present moment is a very relevant time to be reading this novel. Many of the facts about COVID-19 are similar to the fever in Ma\u0026rsquo;s novel. It was actually quite eerie how similar they were. Ma did a good job capturing how people would respond to a similar virus. The similarity made me reflect on our current situation and the magnitude of it. While things are not as bleak as in Ma\u0026rsquo;s novel, our current state is more similar to an apocalypse novel then I realized.\nSeverance has so many vivid scenes. I really felt the pain or anguish or excitement that the characters were feeling. I love reading about New York and Chicago. I find it exciting and comforting to read about someone moving through spaces that I also moved through.\nCandance, the main character of the novel, reminded me of characters from Otessa Moshfegh\u0026rsquo;s writing. Candance\u0026rsquo;s description of her feelings felt similar to some of Moshfegh\u0026rsquo;s characters. Candace\u0026rsquo;s descriptions made it feel like certain events or experiences just happened to her. She did not have agency in controlling a lot of things. This reminded me of depictions from Moshfegh\u0026rsquo;s novels.\nAfter completing the book, I am still unclear about some of the messages from the book. I know that a book does not need to have one coherent message. But there seemed to be one passage toward the end of the book where Ma was really trying to drive a point home. It is the last quote at the bottom of the page. Ma seems to be critiquing living in cities. But what is she comparing living in cities to? Is she saying that living outside of cities is better? Could here point not actually be about cities but rather being a part of the system?\nTo complicate things even further, the character in the novel who is most anti-city or anti-system, Johnathon, is not portrayed in a positive light. Johnathon, Candace\u0026rsquo;s ex-boyfriend, wants to leave New York. Johnathon does not have a regular job and is trying very hard to exist outside of the system. But it does not appear that he is any happier than Candance. If anything, he is depicted as distant and too aloof.\nWe can also connect these pieces with the actual fever in the novel. The fever causes people to be stuck in a repetitious cycle. They repeat the same pattern and actions over and over again. This idea of agency and being part of a system can be connected to how Candance and her mom are depicted. Rather than having agency things are happening to them. That the system is forcing them to go in certain directions.\nSo where does all of that leave us? Is Ma critiquing the systems we have in place that cause us to do the same thing day in and day out? Yes. Is Ma advocating for us to try and break out of that system? I am unsure. Maybe Ma is suggesting that it really does not work for one individual to break out of that system on his own. For there to be a real change it has to be at a societal level?\nQuotes  The End begins before you are ever aware of it. It passes as ordinary\n  So, I said, searching. Tell me about what you do. I regretted it as soon as I asked. It was the question everyone asked everyone else in New York, so careerist, so boring.\n  When other people are happy, I don’t have to worry about them. There is room for my happiness.\n  Because being online is equivalent to living in the past. And, while we can agree that the internet has many uses, one of its significant side effects is that we all live too much in the past.\n  We just wanted to feel flush with time to do things of no quantifiable value, our hopeful side pursuits like writing or drawing or something, something other than what we did for money\n  To despise someone is intimate by default\n  To live in a city is to take part in and to propagate its impossible systems. To wake up. To go to work in the morning. It is also to take pleasure in those systems because, otherwise, who could repeat the same routines, year in, year out?\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/severance/","tags":["Fiction","Science Fiction"],"title":"Severance"},{"categories":["Books"],"contents":"Reading Catch and Kill by Ronan Farrow was a real eye opener for me. I knew who Harvey Weinstein was but I did not know any of the details. A lot of the events covered in the book happened while I was in college when I did not really follow the news.\nMy largest takeaway was that while the book might be centered on Harvey Weinstein, it is actually about so much more than any individual man. It is about our society as a whole, the power structures that we have in place and what kind of behavior we tolerate / support. These problems are not about a few individual bad actors. It is about the systematic structures that we have built.\nIt is easy to look down on perpetrators of sexual assault and treat them as outliers. It is easy to say these are bad people and they were going to be bad people no matter what. It is a lot more difficult to ask why are they perpetrators of sexual assault? What is it about our society that leads to people behaving in this manner? Catch and Kill showed that it was not just Harvey Weinstein. Almost every man in a position of power in the media industry had some encounter with sexual assault.\nOver the past decade we have become more aware of how common sexual assault is. Sexual assault is still much more common then we understand. Farrow documents how hard it is for victims of sexual assault to talk about their experience. Many of these allegations get swept under the rug with non-disclosure agreements and large sums of money. We need to ask larger questions about our society if we want things to change. It is enough to put a persecute a small portion of the men who are accused of rape.\nWe need to understand why men in society feel entitled to women\u0026rsquo;s bodies. We should better explore how men express themselves and why it often comes out in violent and angry ways. We should be asking questions about money and power. How do people internalize power dynamics and let that guide their actions? Money is just another form of power. Inequality in wealth is helping some while hurting others. All of these questions are tied together.\nQuotes  In the end, the employees said, Jonathan’s routine had been so boring the subcontractor surveilling him had given up. “I’m interesting!” Jonathan said, when I told him. “I am a very interesting person! I went to an escape room!\n  In the end, the courage of women can’t be stamped out. And stories—the big ones, the true ones—can be caught but never killed.\n  “You know, the press is as much part of our democracy as Congress or the executive branch or the judicial branch. It has to keep things in check. And when the powerful control the press, or make the press useless, if the people can’t trust the press, the people lose. And the powerful can do what they want.”\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/catch_kill/","tags":["Non-Fiction"],"title":"Catch and Kill"},{"categories":["Papers"],"contents":"By Matthew R. Williams and Terrance D. Savitsky\nSummary This paper provides a framework for creating survey weights using constrained optimization. The problem we are solving can be laid out in the following way:\nWe have a sample of size n taken from a population of size N. Each person in the population has a probability of being included in the survey. The probability of being in the sample is correlated with our response of interest. To account for this probability every person in our sample gets a weight of 1/p_i, where p_i is the probability of being selected. After applying these weights we can now infer about the population.\nHere is an example to ground our thinking. Our population is doctors in the United States. We send out an email to all doctors asking them to participate in our survey. We are interested in do doctors support using more technology in hospitals. A sample of the doctors we email respond. We want to answer questions about our population of doctors, not just our sample of doctors. Doctors with a higher probability of responding to our survey might be more likely to support technology. We need to downweight the doctors with a higher probability of being in our survey otherwise we will overrepresent the number of doctors who support using more technology. If we knew p_i for every doctor, we could apply a weight of 1/p_i to each doctor in our sample.\nWe do not actually know p_i though. It is unobservable. We can observe other variables that might be correlated with p_i though. For example, gender or age or ethnicity might relate to p_i. Males might be more likely to be in our survey than females. We can weight our survey so that these variables line up with the population. If we know the population is 50% male, but our sample is 75% male, we then want to downweight those responses so that our sample lines up with the population.\nWe call these variables, weighting variables. We do not know ahead of time which weighting variables will lead to the best sample. If we have a matrix X of possible weighting variables, we need to somehow pick which weighting variables to use. As the size of X increases, the number of potential weighting variables grows, there no longer becomes a closed form solution to our problem. In order to solve this problem we can use constrained optimization with a loss function.\n \u0026ldquo;The focus shifts from finding one optimal solution to finding one or more reasonable solutions which satisfy most of the constraints and restrictions\u0026rdquo;\n The constrained optimization allows us to use both soft and hard constraints. A hard constraint means that our solution most meet that constraint. For example our weights must make it so 50% of the sample is male. A soft constraint means that we want our solution to come as close as possible to the constraint. We penalize a solution the further away it is from the constraint.\nWhen we use hard constraints it is possible that no solution will exist if we add a lot of constraints. Using soft constraints we will always be able to find a solution. When we use soft constraints we can weight how much we care about a given constraint. We might care about gender lining up with the population more than we care about age.\nThe rest of the paper discusses different options for deviances (their word for loss metrics). The authors then apply their solution to a data set and show the results.\nThe authors evaluate their model by looking at how many constraints it can hit. They argue that a weighting that meets more constraints is a better weighting.\nThoughts   The authors are not able to evaluate the accuracy of their weighting because they do not know the truth values for the variable of interest in the population. A next step in research would be to examine a setting where we knew the true value in a population. We could then answer questions about if different loss metrics led more accurate weights. It is also possible that each sample is unique and there will not be a set of weighting variables or a loss metric that is consistently the best.\n  I enjoyed the reframing of weighting as a constrained optimization. This approach makes it clear what assumptions we are making and makes it easy to compare different weighting approaches. We can directly tell the algorithm we care about these constraints more than these other constraints. When we report our weighting we should be clear about what assumptions we made and why we made them. Another possible approach would be do to many runs of weighting and see how results change as we change our assumptions. This would provide insight into the importance of certain assumptions and how the result could be different if our assumption is wrong.\n  ","permalink":"https://judahgnewman.netlify.com/writing/papers/2020/opt_surv_weights/","tags":["Statistics"],"title":"Optimization of Survey Weights under a Large Number of Conflicting Constraints"},{"categories":["Books"],"contents":"Normal People by Sally Rooney is one of my most anticipated books in the past couple of years. I had heard from many different sources about how good this book is. Family, friends and numerous co-workers all recommended it highly. It is relatively rare that I hear praise about a book from so many different sources.\nI can see why so many different people enjoyed Rooney\u0026rsquo;s writing. She does an incredible job portraying emotion and what it feels like to be in your late teens / early twenties. The book captures how certain moments take on such intense significance. Her writing portrays social anxieties that I could not have put into words.\nPersonally, I found the novel to be intensely painful. It was difficult to see Connell and Marianne repeatedly hurt each other. Numerous times they had large misunderstandings. They would either make an assumptions about what the other person was thinking or they would assume that the other person knew what they were thinking.\nOne particularly powerful aspect of the book was how Marianne and Connell\u0026rsquo;s relationship changed over time. The reader is able to experience the characters growing and changing together. We even see their internal monologues develop and taken on more nuance as they get older. They improve their communication and make more of an effort to understand the other person. Rooney\u0026rsquo;s portrayal of learning more about relationships and how to communicate with another person as we get older felt spot on.\nRooney is able to capture small moments or feelings through metaphor. Quite a few of my favorite quotes from the book are her capturing a feeling through a metaphor.\n This book continued my trend of thinking about trauma and how it stays with us. For both Connell and Marianne as they grow trauma from earlier stays with them. They each have deeply engrained feelings that do not go away. These long formed deep feelings formed impact their ability to be good partners for each other.\nIt is also furthered my thinking on the idea of pivotal moments. This book reminded me that earlier in my life it did feel like there were these intense significant moments. That what happened in the next hour or day was going to have a huge impact on my life. Is that how life actually happens at a certain age? Or just how it is perceived? Even though months would often pass in between each chapter, we could essentially predict where the next chapter would begin. Does each pivotal moment set our trajectory until the next pivotal moment changes it? As we get older are there less of these pivotal moments or does our interpretation of them change?\nQuotes  It’s a face like a piece of technology, and her two eyes are cursors blinking.\n  With only a little subterfuge he can live two entirely separate existences, never confronting the ultimate question of what to do with himself or what kind of person he is.\n  It suggests to Connell that the same imagination he uses as a reader is necessary to understand real people also, and to be intimate with them.\n  The heat beats down on the back of Connell’s neck like the feeling of human eyes staring.\n  Life is the thing you bring with you inside your own head.\n  Their feelings were suppressed so carefully in everyday life, forced into smaller and smaller spaces, until seemingly minor events took on insane and frightening significance.\n  I’m just nervous, he says. I feel like it’s pretty obvious I don’t want you to leave. In a tiny voice she says: I don’t find it obvious what you want.\n  From a young age her life has been abnormal, she knows that. But so much is covered over in time now, the way leaves fall and cover a piece of earth, and eventually mingle with the soil. Things that happened to her then are buried in the earth of her body. She tries to be a good person. But deep down she knows she is a bad person, corrupted, wrong, and all her efforts to be right, to have the right opinions, to say the right things, these efforts only disguise what is buried inside her, the evil part of herself.\n  All these years they’ve been like two little plants sharing the same plot of soil, growing around one another, contorting to make room, taking certain unlikely positions.\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/normal_people/","tags":["Fiction"],"title":"Normal People"},{"categories":["Books"],"contents":"I was introduced to this book through Stratechery and Exponent. One of the co-hosts of Exponent, James Allworth, is a co-author of this book. While I have been reading Ben Thompson\u0026rsquo;s (the author of Stratechery) writing for a couple years now, I knew a lot less about Allworth and his background. While James is a co-author of this book, the book is written from the perspective of Clayton Christensen. Christensen a professor at Harvard Business School, is best known for his work The Innovator\u0026rsquo;s Dillema. This book focuses on taking theories from management consulting and applying them to your own life.\nI welcome the approach of taking logic from the business world and using it to inspect your own life. Taking knowledge from one discipline and applying to another is often quite powerful. It makes clear the similarities between realms that we treat as separate. This cross pollination often provides a new perspective. I tried to take a similar approach here using interviews as a framing for thinking about picking a political candidate.\nWhile there was so much that spoke to me in this book, I actually really struggled with it. I really enjoyed the focus on reflection and identifying what is important. But Christensen talks with such certainty about everything. He expresses certainty about his theories of business and that applying these theories to your life will turn out well. Living a happy and fulfilling live is immensely difficult. While Christensen\u0026rsquo;s theories and frameworks might be helpful, they will not work for everyone.\nThe book also seems to neglect the aspect privilege plays in our ability to live fulfilling lives. It is a lot easier to focus on your purpose if you are not worrying about feeding your children. While the theories Christensen provides might be useful to people in all scenarios, it is easier for people who go to Harvard Business School to apply them, then people who never had the opportunity to attend college.\nI personally struggled with Christensen\u0026rsquo;s focus on religion and the role it played in his life. While religion will play a powerful role in many lives it is not for everyone. I wish the book had not just been written from his point of view but all three of the authors. This would have given insight into how different people with different perspectives apply the same theory. By only being from Christensen\u0026rsquo;s point of view, it diminished how accessible some of the theories felt.\nQuotes  One of the best ways to probe whether you can trust the advice that a theory is offering you is to look for anomalies—something that the theory cannot explain.\n  What are the most important assumptions that have to prove right for these projections to work—and how will we track them?\n  With every moment of your time, every decision about how you spend your energy and your money, you are making a statement about what really matters to you.\n  That is why capital that seeks growth before profits is bad capital.\n  Culture in any organization is formed through repetition.\n  You can see the immediate costs of investing, but it’s really hard to accurately see the costs of not investing.\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/cc_ja_how_measure/","tags":["Non-Fiction","Reflections"],"title":"How Will You Measure Your Life?"},{"categories":["Personal"],"contents":"We have been taught to support political candidates whose ideology most nearly matches our own. While being ideological aligned with a candidate is important, not enough weight is given to candidates other traits. As an organization grows larger, the success of the organization is determined less by the vision or execution of the leader and more by the leaders ability to listen and empower others. The United States political system is so large and unwieldy that the impact of a president is not felt through their ideology but their ability to successfully run a large organization.\n During primary season I have had so many discussions about which candidate to support. These conversations always center on candidate\u0026rsquo;s view points about policy. During the Democratic Primary there was a popular Washington Post quiz that tells you which candidate you should support. The premise of the article was that through answering a series of questions about different polices, we can identify which candidate to support. This article represents a broader trend in political discourse. All of our discussions are focused on policy.\nDuring the Trump presidency there has been a lot written about polarization. Many scholars have highlighted how the U.S. and other Western Countries are more polarized then ever before. This entire discussion focuses on where we stand on a scale going from liberal to conservative. Candidates are boiled down to a point on a spectrum. This person is a socialist, this person is ultra conservative, this person is more centrist.\nPolitical ideology is closely tied with personal values. Choosing a candidate that has similar policy ideas to you often means choosing a candidate that has similar values to you. While picking a candidate whose ideology aligns with your own is important, we focus to much on each candidate\u0026rsquo;s ideology. We determine who would be a good leader based on their idealogy. In a smaller organization the ideology and vision of the leader matters more. For leaders in government there are other factors besides ideology we should be considering more.\n We are going to step away from government for a second to better understand why an intense focus on policy hides who will actually be a good leader.\nImagine we are picking the leader of an organization. The type of organization does not matter, it could be a start-up, a non-profit, a school science project, etc\u0026hellip; We want to pick a leader who is going to make our organization as successful as possible at accomplishing its mission. To start, lets imagine our organization is quite small, it is only one person. What are we looking for in a leader of our organization? If the organization is just one person, we want someone who has a good vision and is going to be able to execute that vision. I want someone who is a hard worker and who has good ideas.\nBut now, let us change the task slightly. We are still picking the leader of an organization but the organization is two people instead of one. While this organization is still quite small, if the organization has two people, our ideal leader slightly changes. We still want someone who has a good vision, but we also want someone who is good at listening and working with others. We want someone who maximizes the other person in the organization.\nNow lets jump to an organization of 5 people. It is even more important that our leader can listen to others. We want a leader who has good organizational skills and can run a team successfully. The leader has to be good at facilitating group conversations and resolving disagreements.\nAs our organization gets larger the skill set of an ideal leader changes. We become less concerned with the individual\u0026rsquo;s point of view and more focused on how they maximize the entire organization. Imagine an organization of 30 people (still considered small in most places). Our leader needs to be empowering the workers on the front lines to be making smart decisions. The people on the front lines have the most information and a good leader sets up a system so that she can learn from them. In a larger organization it is critical that the leader is good at hiring talent. The larger the organization the more different things matter than the leaders individual opinion. The success of a large organization is not determined by the leader\u0026rsquo;s vision but by her ability to listen and empower others.\n The government might be the largest and most unwieldy organization that we have. There is both the federal government and regional governments. The government was built specifically so that all the different parts do not work well together. Regional governments limit the power of the federal government. Our government was purposely divided into different parts so that no one part has too much power.\nHistory shows that the structure of government limits our ability to get things done. Barack Obama was in office for eight years and he was really only able to focus on getting one thing done, Obamacare. Our government was built so that it is not easy for everybody to work together.\n If we were starting from scratch, we would think about picking the leader of our government in a very different way. Yes, we would still care about their personal ideology, but we would be less focused on it. There are many politicians who have ideologies that are close enough to our own. Rather than finding the candidate who has the ideology that most closely matches ours, we would be trying to learn more about how candidates operate. We would want a candidate who listens and recognizes there own limitations. A candidate who can admit when they are wrong and hires great staff.\nIf we were less focused on ideology and more focused on these other characteristics we might be interested in the following debate questions.\n How do you get information from your staff? How do you handle when someone challenges your opinion? How do you think about who you are hiring? Is there a possibility that some of your policy ideas are wrong?  We can essentially think about the presidential election as a hiring process. We are hiring a candidate to run our organization. We have a couple of months to learn more about the candidates for the position. We do not only care about what our candidates product vision / policy positions are. We also care about how they work with others.\n Reframing picking a candidate in this way helped clarify a lot of things for me personally. It makes it a lot clearer why Trump is such a bad person to be running this country. Not only are his policy positions detrimental, but he is not a good leader. He can never admit when he is wrong and is very difficult to work with. He is terrible at hiring the right people.\n One last thought. While the ideological views of the leader may not matter as much in a larger organization, I am interested in exploring how it is important to have other people in the organization who are ideological leaders. Other people who are focused purely on ideological movement and driving ideological change. Think about AOC or Bernie. They have played such an instrumental role in moving the democratic party. While that might not be the role of the leader, it is still necessary to empower ideological leaders. I am excited to flesh out these thoughts a bit more.\n","permalink":"https://judahgnewman.netlify.com/writing/personal/politican_ideaology_leadership/","tags":["Policy","Reflections"],"title":"What are we looking for in a politician?"},{"categories":["Books"],"contents":"I have read a book by Ta-Nehisi Coates each of the past two years. I read Between the World and Me two years ago and We Were Eight Years In Power last year. Both books demonstrated Coates impressive ability to forcefully convey powerful ideas. Coates is a masterful essayist. I listened to him and his editor Chris Jackson discuss The Water Dancer on the podcast Longform. I was quite excited to read Coates first novel and see how his writing style translated to a completely different medium.\nOne theme that stuck with me is how the past weighs on us. This theme was also present in another slave narrative I read, Washington Black. Hiram goes through immense difficulty transitioning from slavery to being free. He faces difficulty transitioning from living on the street to living in the house. As Hiram goes through life transitions his past and the tendencies he has built up weigh on him. Even though his situation has changed, he brings tendencies and feelings with him from his past. It is not easy to leave your history behind.\nReading previous slave narratives it is evident how difficult it was when families were divided. The teating apart of families due to the slave trade is often described. The Water Dancer highlights this difficultly but also a slightly different separation of families. A separation where some parts of the family have become free while others are still slaves. Even if you individually are free, it is difficult to feel that way when the people you love are still enslaved.\nGrowing up in the States today it can feel like slavery is far away from us. The civil war and slavery seem like a completely distant past. In both Coates non-fiction and this piece he reinforces that slavery is an integral part of who this country is. Slavery is not something that happened previously and we have moved past. Slavery will always be a part of our history. Just as individuals need to deal with their past, the U.S. needs to recognize that slavery is part of who we are. As individuals we all have baggage, and as a country we have baggage.\n The majority of pieces I read, whether fiction or non-fiction, are about someone great. They are about someone who is special. Rarely do I read a book that I feel speaks to just the common day to day experience of living a life. It does not speak to the small little moments. The writing regularly focuses on large events and people who are important.\nI have experienced this a few times now where I am reading a novel and hoping that nothing special will happen. That the book will just be about someone who lives their life and goes through every day difficulty. But there always ends up being something extraordinary or special about the person\u0026rsquo;s circumstances.\nBooks about ordinary people who live ordinary lives might not sell. It is also possible that if looked at in a certain way everyone\u0026rsquo;s life is extraordinary and important. I am interested in exploring more how can the normalcy of the everyday be conveyed in a piece of literature that people will still enjoy. A piece of literature that focuses on the ordinary. On the everyday people living everyday lives rather than large events in special people\u0026rsquo;s lives.\nQuotes  knowing something is a far measure from truly seeing it.” “Takes some time,\n  Power makes slaves of masters, for it cuts them away from the world they claim to comprehend. But I have given up my power, you see, given it up, so that now I might begin to see\n  There is always a part of us that does not want to win, wants to stay down in the low and familiar.\n  But what you must get is that for me to be yours, I must never be yours. Do you understand what I am saying? I must never be any man’s.\n  I should not have been surprised. I knew by then how much the past weighs upon us. I knew this more than anyone\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/coates_water_dancer/","tags":["Fiction","History"],"title":"The Water Dancer"},{"categories":["Books"],"contents":"I have read a lot of \u0026ldquo;pop\u0026rdquo; science books and Good Economics for Hard Times is easily the best. Duflo and Banerjee explore very complex topics with nuance. They do an incredible job of conveying where there is uncertainty and what we know more definitively. Duflo and Banerjee both review the existing literature and expand upon potential future directions. Good Economics for Hard Times provides structure for thinking about complex problems like immigration or automation.\nThis book was so refreshing because Duflo and Banerjee were very honest about how economics has both helped and hurt the world. They highlighted how economists can often be over confident. They also explored the evidence behind trickle-down economics. Duflo and Banerjee explain that evidence for trickle-down economics is non-existent and that economic theory in isolation can lead to suboptimal Providing money to large organizations does not lead to everyone being more well off.\nAfter reading this book I now recognize how most discussions about economics use economic theory and not economic empirical work. Theory without verification through concrete evidence means nothing. We need to stop prioritizing large organizations and acting like that will help everyone. We should not care as much about growth and GDP. GDP measures one thing but it does not accurately summarize a countries well being. We need to be more focused on all individual\u0026rsquo;s quality of life. We need to care about the people in society who are struggling the most and try to directly help them.\nWhile there has been a lot of talk about UBI and how that would help people, Banerjee and Duflo suggest that just giving people money is not enough. In the U.S. having a job is more than just a source of income. It provides people with community, purpose and a sense of empowerment. Duflo and Banerjee highlight the need to build communities and institutions. Instead of directly giving individuals money, we should expand a lot of services.\nWe should expand both elder care and head start programs. This would provide new jobs for lots of people and also be a much needed service. Money should be directed towards job retraining and helping people find jobs that they want to do. Industries are going to change and people\u0026rsquo;s jobs are going to get automated. We should build a robust system of helping people find new jobs, new communities and new senses of purpose.\nQuotes  The world is a sufficiently complicated and uncertain place that the most valuable thing economists have to share is often not their conclusion, but the path they took to reach it\n  The most important question we can usefully answer in rich countries is not how to make them grow even richer, but how to improve the quality of life of their average citizen.\n  In a policy world that has mostly abandoned reason, if we do not intervene we risk becoming irrelevant, so let’s be clear. Tax cuts for the wealthy do not produce economic growth.\n  But there are also so many market failures that it makes no sense either to rely on the market alone to allocate resources to the right use; we need an industrial policy designed that keeps in mind these political constraints.\n  To demonstrate waste in government, one needs to show there is an alternative way of organizing the same activity that works better\n  Putting these families squarely on track toward productive work required more than money. It required treating them as human beings with a respect they were not used to, recognizing both their potential and the damage done to them by years of deprivation\n  The only recourse we have against bad ideas is to be vigilant, resist the seduction of the “obvious,” be skeptical of promised miracles, question the evidence, be patient with complexity and honest about what we know and what we can know.\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/economics_bad_times/","tags":["Economics","Non-Fiction"],"title":"Good Economics for Hard Times"},{"categories":["Talks"],"contents":"The weekend of February 22 2020, my partner Jenny and I attended a mediation retreat at Spirit Rock Meditation Center in Marin. I wanted to attend a meditation retreat for a while, but never put in the effort to figure out the logistics before. I was looking through different meditation centers around the Bay Area in the fall and landed on Spirit Rock because it practices insight meditation. Insight meditation is the most accessible to me because it less focused on formality or any sort of rigidity.\nThe retreat we attend was the The Inner Work of Social Justice: Socially Engaged Mindfulness led by Rhonda Magee. We were excited because the title suggested the retreat would bring a different perspective and focus to mindfulness then what we had previously been exposed to. The retreat was Saturday and Sunday from 10AM - 4:30PM. The retreat consisted of individual sits, walking meditations, lectures and some interpersonal exercises.\nHere are some assorted thoughts from the weekend:\n  One feeling that kept coming forward during my sits was fear. On a day to day basis I would not say I have a lot of conscious fear present in my life. I started to notice that fear underlies a lot of my actions and thoughts. During childhood fear is constantly present and can be seen as trivial. We have fear of monsters under the bed, fear of the house catching fire. It does not seem like all that fear goes away, but rather fear may take different forms as we get older. Instead of being scared of a robber entering my house, I have a deeper hidden fear of failing. I have a fear that my actions do not matter or that what I do will not have an impact. This fear can be so hidden that it can be hard to even recognize what it is regularly. I want to embrace that fear rather than run away from it or hide from it. Fear can be such over powering feeling that acknowledging it is difficult. I am excited about doing more work to identify how fear is subtly pushing me in different ways and accepting that fear, rather than trying to deny it.\n  I struggle to connect my sits with the rest of my life. Regularly, I will sit down to meditate, focus on my breath for ten minutes and then go back to what I was doing. My headspace while I sit can seem so disconnected from my headspace during the rest of my life. During a sit, my thinking will slow down and I will calm down, but the rest of my time I can still be quite frenzied. This weekend helped me connect my sits better with the rest of my life. Mindfulness is not only about making time to sit. It is also about bringing mindful practices to all my interactions. In meetings if I take a moment to reflect, have a deep breath and then respond that is a way of bringing mindfulness outside of the cushion. Going for a walk in nature and listening to the sounds I hear is another example of bringing mindfulness outside of meditation. This weekend helped me recognize that mindfulness is not solely focused on meditation.\n  Meditation can easily be seen as a very personal practice. The process of siting on your own, with your own can be isolating at times. This weekend opened my eyes to meditation being a cross personal experience. I have completed meditations that discussed using mindfulness to connect with others. I always viewed those kind of exercises as doing inner work, so that in other situations I can connect with others. This weekend showed me how mindfulness it self can focus on connecting with others. While that distinction might seem small that slight change in focus can be quite different. My headspace is very different doing inner work to help connect to others rather than directly sitting and focusing on working with others.\n  During the weekend we did some group exercises that involved listening to others. We did one exercise that focused on how listening can amplify a voice. When a voice is acutely listened to it is stronger. I want to bring that idea to all parts of my life. By truly listening to someone else and hearing what they are saying, I can amplify what they are trying to convey. During the exercise I also recognized how much I struggle to listen to others. When I am listening I often am already thinking about what I want to say, assuming I know what the person is saying.\n  My worldview is often centered on solving problems or finding a solutions. Whenever I am presented with a problem, I want so simplify it and understand it. I avoid holding and accepting radical complexity. Systems are so intricate and dynamic that I will not be able to understand them completely or simplify them. I will not be able to fully understand why certain outcomes occur. While it is always worthwhile to take a stab at understanding and breaking things down, it can also be a good practice to recognize that complexity. Recognize that any simplification inherently does not contain all of that complexity and therefore is an incomplete perspective. One idea that Rhonda proposed is to view all of our answers as provisional answers. We are coming to an answer that works for right now, but it is always subject to change in the future. This kind of perspective goes very well with things I like about a bayesian perspective to uncertainty in statistics.\n  During the weekend Rhonda highlighted how much wisdom there is our history. History holds lots of information about how people have joined together through difficult times. There are many instances of people joining together to enact change. We should turn to our ancestors and their practices for insight. While I do not identify as part of any religion, there is a lot of wisdom in how religion provided value to individual\u0026rsquo;s lives. Religion helped give people meaning and build important communities. Rather than turn away from religion because of the bad parts of organized religion, we can recognize the immense value it has provided. We can strive to build new institutions that provide similar communities of support. We can form communities around certain values that support each other. We can use regular practices of meeting to reflect more and bring more consistency to our lives. A world where we all have multiple communities of support connecting with different people could be quite powerful.\n  My entire life so far has been working inside the current societal structure. I went through traditional educational institutions. There has not been any decision I made where I deviated from a path that was projected for me. In my head I am working towards some later date where I will have learned enough that I will be able to do important things. I am working within the current societal structure and system to improve it slightly. I am still playing with the idea, that sometimes it is necessary to work outside of the system. That being with in the system inherently limits my worldview and does not allow me to see things outside of the system. If a system systematically hurts certain people, is it enough to just change things on the margins. Do we have to rethink the system all together? Some more thoughts on this to come at a later date.\n  Relating to my earlier reflections on fear, I found that at times through out the weekend I felt fake or like an imposter. I felt like what I am doing with regard to social justice is not enough. That I have had so much privilege in my life. I should be spending more of my time volunteering and I should be donating more of my money. While all of this might be true, I want to reflect more on where the feeling of being an imposter comes from. What makes me feel like I belong, versus what makes me feel like I do not. Is there power in recognizing that no matter what I do, there are certain ways I cannot belong to certain things and that is okay. Some more thoughts on this to come at a later date as well.\n  Rhonda posed the question to us about what a just world look likes. I came up with the following tentative answer:\n  In a just society everyone is able to fully commit themselves to their curiosity. They are supported and empowered by love from others around them.\n One exercise from the weekend was to form a vow connected to how we think about social justice. I came up with the following vow: I vow to embrace fear and discover empowerment for myself and others through communities of support  Books to Read  Biased - Jennifer Eberhardt Blindspot: Hidden Biases of Good People - Mahzarin Banaji \u0026amp; Anthony Greenwald Beloved - Toni Morrison The Color People - Alice Walker Buddhism and Whiteness: Critical Reflections White Fragility - Robin DiAngelo Radical Dharma - Jasime Syedullah, Lama Rod Owens and angel Kyodo Williams Seeing White - Amy Eshleman, Jean O\u0026rsquo;Malley Halley and Ramya Mahadevan Vijaya *How to Be an Adult in Relationships: The Five Keys to Mindful Loving: - David Richo Hardwiring Happiness - Rick Hanson Insight Dialogue: The Interpersonal Path to Freedom - Gregory Kramer Right Use of Power: The Heart of Ethics - Cedar Barstow The Worm at the Core: On the Role of Death in Life - Jeff Greenberg, Sheldon Solomon and Tom Pyszczynski  ","permalink":"https://judahgnewman.netlify.com/writing/talks/mindfulness/med_social_rhonda/","tags":["Mindfulness"],"title":"The Inner Work of Social Justice: Socially Engaged Mindfulness: Rhonda V Magee"},{"categories":["Books"],"contents":"Before reading Zen and the Art of Motorcycle Maintenance I had no knowledge about what genre the book was or what it would be about. I actually potentially had some misconceptions about the content of the book. While I reading it, I actually thought it was a piece of fiction. Only when I read the afterword did I learn that it was actually semi auto-biographical. My friend Abhisaar recommended this book and I finally got around to it after about two years.\nI quite enjoyed the entire process of reading Zen. I have not been exposed to much philosophy during my life and it was fascinating to read a first hand account of how an individual deals with different philosophers. It felt a lot more accessible then reading an essay or directly reading some of these philosophers work.\nEven though Pirsig wrote Zen in the 50s, a lot of what he discusses is still relevant today. We are still struggling with our relationship to technology. We are actually probably struggling with it even more now. While some of the ideas he espouses have become a part of the main stream discussion, we are still grappling with a lot of the questions Pirsig writes about. This persistance of relevant questions suggests to me that things do not change as much as we might think. Even though our day to day might look a lot different then it did in the 50s, a lot of the issues we struggle with are the same.\nIn Fredrick Brooks paper, No Silver Bullets he highlights the differences between accidental and essential difficulties. [(My Notes Here)] (https://judahgnewman.netlify.com/writing/papers/2020/silver_bullet/ ). Essential difficulties are ones that are inherent, we will not be able to solve them. Accidental difficulties are ones we will eventually be able to solve. Using this lense, we might argue that struggling with our relationship to technology is an essential difficulty. Due to human nature and how technology changes and updates, we are always going to struggle to maintain a healthy relationship with tech. This view point forces us to reconsider our approach. Rather than solve this problem directly, we can recognize it is inherently difficult and be more understanding of those difficulties. When we stop trying to solve a problem, but rather recognize how hard it is, that can lead to a better relationship with the problem.\nThis also made me reflect on a discussion I was recently listening to between Tim Urban and Ezra Klein. They were arguing over whether we are entering a time that is going to be radically different from anything we have seen before. Urban was making the case that technological innovation is fundamentally changing our society. Klein had the perspective that technological change is often over hyped and things will not be fundamentally different. Framing this discussion as a dichotomy is the wrong approach. By taking the perspective of accidental and essential difficulties we have a better framework to think about this discussion. While some things might be radically changing, others might not.\n The majority of the novel was focused on this idea of quality. While Pirsig did a lot of existential musing on quality, some of his reflections helped me reground my own sense of quality. It reenforced in me that in my every day happenings I know when I see quality. I know when I have done good work versus when I have not put my full effort into a project. I should maintain my own unique sense of quality and work on developing it. Developing your own unique sense of quality is an essential skill.\nPirsig briefly discusses where we derive our sense of quality. A more complete discussion of this would involve a mention of evolution. Pirsig mostly highlights quality derived from culture or individual experience. Our sense of quality in many things comes from evolution. This is easier to see in some realms and much more difficult to see in others. We know certain foods taste better because they are better for us. We like the taste of sugar because it give us energy. A higher quality food is something that is better for us evolutionarily. But in other places how our sense of quality is derived from evolution is harder to understand. How does the quality of an art piece derive from evolution? A possible line of thinking is that all of our emotions are a result of evolution pushing us to act in certain ways. When we see a painting, and it evokes a certain sense of quality, that is derived from evolution programming us in certain ways. Evolutionarily being on high ground and being able to see a lot of terrain was beneficial. Our perspective on what is a quality view, is a result of evolution. Thinking a painting is of high quality is equivalent of thinking a view is of high quality. As a result of evolution giving us emotional responses to different things, we have derived a sense of quality in all things. Our sense of quality derives from a combination of evolutionary beneficial traits and learned experience both individual passed down through culture.\n In the book Pirsig uses a knife as a metaphor for how each person brings a different worldview. We all have a knife that we use to parse the world up into smaller parts that we can comprehend. Trying to understand the entirety of something is exceedingly difficult. Using a knife allows us to understand simpler parts. We all bring our own unique knife to each situation. There is not one definitive way to break things down, we each create our own subjective parts by using our own knifes. This metaphor does a really good job of explaining how we each try to handle the complexity of the world differently. We will never avoid having knifes, the world is too complex. We should strive to recognize our own knifes / perspectives and see how they are limiting us. Ideally we might be able to use multiple knifes to view any problem in different ways.\nIn a similar thought process, Pirsig reflects on their being infinite hypothesis. That for every problem there are infinite hypothesis about what might be the solution. If we accept infinite hypothesis we will never be able to rule them all out. Even after ruling out many possible ones we will still have infinite hypotheses left to test. Accepting infinite hypothesis forces us to reframe how we think about science. Science is not about finding an endpoint or a definitive solution. It is about finding balance. Rather than we making progress down a road, lets think of science as a balancing act, like a see-saw. One one side of the see saw is the complexity of life. The other side of the see-saw is human\u0026rsquo;s understanding. As the complexity of our lives keeps increasing, we are working to learn more and keep the see-saw balanced.\n Pirsig\u0026rsquo;s decision to write from the perspective of an individual man at a given point in time made his thoughts much more accessible than writing an essay. Coming from an individual I found it easier to emotionally connect with his thoughts. When reading the voice of one man we know that his viewpoint is limited and do not need him to provide the entirety of the truth. Instead he is just providing one perspective which may be helpful or force us to reflect on our own thoughts in different ways.\nQuotes  When I think of formal scientific method an image sometimes comes to mind of an enormous juggernaut, a huge bulldozer—slow, tedious, lumbering, laborious, but invincible. It takes twice as long, five times as long, maybe a dozen times as long as informal mechanic’s techniques, but you know in the end you’re going to get it.\n  So I guess what I’m trying to say is that the solution to the problem isn’t that you abandon rationality but that you expand the nature of rationality\n  In a sense, he said, it’s the student’s choice of Quality that defines him. People differ about Quality, not because Quality is different, but because people are different in terms of experience.\n  One geometry can not be more true than another; it can only be more convenient. Geometry is not true, it is advantageous.\n  Value is the predecessor of structure. It’s the preintellectual awareness that gives rise to it.\n  Programs of a political nature are important end products of social quality that can be effective only if the underlying structure of social values is right.\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/zen_mm/","tags":["Reflections","Fiction"],"title":"Zen and the Art of Motorcycle Maintenance"},{"categories":["Books"],"contents":"Braiding Sweet Grass is a set of short stories about nature from Robin Kimmerer\u0026rsquo;s life. Kimmerer blends a scientific perspective with a perspective from from her heritage as a Native American. While each story is about an individual relationship between humans and nature, she often makes larger points about how human view themselves in the world. Kimmerer questions human\u0026rsquo;s claim of intelligent supremacy, challenges some of our economic systems and laments different ways our society is narrow minded.\nKimmerer\u0026rsquo;s stories force us to be humble. Science is discovering everything that we do not know. It is a process of recognizing how limited are inherent perspective. Every time we learn something, we learn five more things we do not know. Kimmerer constantly displays extremely intelligent and adaptive systems in nature. She shows the complexities different plants interacting with each other or how eco systems maintain themselves. We are forced to reckon with how little we understand about nature and how natural systems have evolved truly remarkable capabilities.\nI was forced to reflect on my relationship with nature. Kimmerer\u0026rsquo;s repeatedly return to the necessity of humans cultivating a symbiotic relationship with nature. Humans should not assume the best they can is leaving nature alone. Humans must have an active relationship with nature. When humans and nature act together is what allows both to flourish.\nQuotes  Science pretends to be purely rational, completely neutral, a system of knowledge-making in which the observation is independent of the observer. And yet the conclusion was drawn that plants cannot communicate because they lack the mechanisms that animals use to speak.\n  The more something is shared, the greater its value becomes. This is hard to grasp for societies steeped in notions of private property, where others are, by definition, excluded from sharing.\n  Might science and traditional knowledge be purple and yellow to one another, might they be goldenrod and asters? We see the world more fully when we use both.\n  The job is never over; it simply changes from one task to the next. What I’m looking for, I suppose, is balance, and that is a moving target. Balance is not a passive resting place—it takes work, balancing the giving and the taking, the raking out and the putting in.\n  say “I discovered X.” That’s kind of like Columbus claiming to have discovered America. It was here all along, it’s just that he didn’t know it. Experiments are not about discovery but about listening and translating the knowledge of other beings.\n  Restoration offers concrete means by which humans can once again enter into positive, creative relationship with the more-than-human world, meeting responsibilities that are simultaneously material and spiritual. It’s not enough to grieve. It’s not enough to just stop doing bad things.\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/braiding_sg/","tags":["Nature","Non-Fiction"],"title":"Braiding Sweet Grass"},{"categories":["Papers"],"contents":"By Fredrick P Brooks\nSummary This paper was published in 1986 examining the difficulties of creating software. Brooks argues that there is no silver bullet to make software development a lot easier. The main things that are making software development hard are part of the essence of software development. Rather than aiming for a silver bullet that will greatly improve software development we should be making smaller iterative improvements.\nBrooks argues that Moore\u0026rsquo;s Law and a two fold increase in productivity each year is a fools goal for software. Software development is much more complex. Most of the largest gains in software development have already been gained. From here on out it will be smaller iterative improvements that take more time.\nBrooks divides the difficulties of software development into two categories. This two category framework can actually be applied to the difficulties of any task.\n Essence → difficulties inherent in the process Accidents → difficulties that exist today but which are not inherent.  The difficulties that make up the essence of software development are how do we build something so complex. The difficulties are the specification of the design, getting concepts to work with each other and the testing of the construction. Syntactical errors are nothing relative to the difficulty of actually creating software. These are difficulties that will always exist because they are the essence of software development.\nBrooks discusses four different parts of the essence\n  Complexity No two parts of a code base are the same. If they were it would be modularized and turned into one thing. Redundancy or repetition often makes it easier for us to understand things. The number of possible states in a program grows exponentially. This complexity is a key part of software systems and so we cannot just abstract this complexity away.\n  Conformity\n The complexity of a software system is not explained by some underlying principle. The complexities are often driven by various complex human needs interacting with each other. There is not some underlying mechanism which if we understood, we would then understand the complexity software.    Changeability\n Software is constantly changing and has changing requirements. This is dissimilar from most other manufactured products. With physical things, rather than changing them, they are often just superseded by a new model. While this can be an advantage of software it also adds lots of difficulties.    Invisibility\n Software cannot be represented in the physical space. When we manufacture other goods we can represent them in physical space. We can make a diagram or a prototype. This makes it easier for humans to reason about it. While we could make a flow diagram for software it is nothing like being able to represent the full complexity software in physical space.     Previous Breakthroughs The previous solutions that made software development easier solved accident difficulties rather than essential difficulties.\n  High Level Languages allowed us to think in layers of abstraction. They get us closer to thinking in terms that humans can understand but it does not get rid of the inherent complexity in reasoning about software. No matter the language we are using, the complexity of software will always be there. There is now just less work turning our thoughts into something the machine can understand.\n  Time-sharing is actually a concept I had not heard of before. This is because this break through was so much before my time that I cannot imagine computing without it. Time-sharing means that multiple users can access the same resource at a time. This means there is less waiting time to use a resource and shortens the systems response time which makes it easier to develop.\n  Unified Programming Environments such as unix allow us to use multiple programs together rather than having to use them all separately. They make it much easier for different parts to interact.\n   Proposals for Future Fixes Brooks next discusses some proposals for future fixes and explains while they might be helpful none of them are silver bullets.\n  The language ADA. I have actually never even heard of this language so clearly it did not end up being a silver bullet. Brooks argues that while this language might help make programming easier, it is no silver bullet. Each step we go higher in languages will be less of a gain than the step before it. Higher level languages are solving accidental difficulties and not essential difficulties.\n  Object Oriented Programming is another fix that allows the designer to express an idea in a more native form. While this also will improve software development it will not be an enormous gain.\n  Artificial Intelligence while a promising idea, is not going to solve all of our software development problems. Creating the data necessary for AI to create programs is quite difficult and we have not seen any promise of AI being able to do anything nearly as complicated as this. Brooks believes AI will be helpful by being supplemental to the coder and providing advice to them. There is too much heterogeneity in coding tasks for us to create an AI anytime soon that will be able to do software development for us.\n  Automatic Programming is really just a higher level language. We still need to provide information for the program to be created. The information we are providing is just at a higher level.\n  Graphical or visual programming is not a realistic goal because software is too difficult to visualize. While some parts of software can be broken down into a flow chart, this visualization will not be able to encompass everything. Generally, when programmers code, they do not make flow charts before hand but only afterwards forcing their program into an abstraction that is not well suited.\n  Program verification will ease the testing load of the program but it will not be able to get rid of it. For very few programs are we actually able to verify that their behavior is correct. Testing is instead about providing evidence and we will still always want to do testing on our own.\n  Environments and tools will help make programming easier in language specific contexts bu do not solve the inherent problem of programming. These returns will continue to be marginal and each continued return will be less and less.\n  More powerful work stations might be helpful in speeding up the computation time but the most time intensive part of any developers day is thinking. A more powerful work station does not help with that.\n   Areas to Focus on While Brooks has argued that there will not be a breakthrough that makes software development a lot easier, there will be steady progress over time. He highlights some areas that should lead to steady progress in our ability to develop software.\n  When we can, we should buy and not build. We do not have to build everything from scratch but rather can re-use other things that people have built. This is a key part of being a software engineer. Recognizing when there are commonalities between tasks and being able to use other code.\n  Refining requirements through rapid prototyping is the best way to build good software. The hardest part of any software project is defining the scope and specs. Clients often do not know what they want or need until after they interact with it. The best way to deal with these challenges is to put prototypes in front of clients and get there feedback.\n  We should grow rather than build software. Software grows slowly piece by piece over time. Looking at how plants grow in nature is a better metaphor than how humans build. Humans build with a ton of planning and then rote execution. Growing is not completely planned out but rather changes based on the environment around it.\n  It is critical we cultivate an environment that promotes great designers. We should recognize we have an active role to play in inspiring people to be creative. We can teach good design, but it is much harder to teach great design. A lot of the most incredible software was developed by individuals in isolation because there ideas were not molded to what had come before them. Too much of the business world is focused on making managers and rewards managers a lot more than great designers. We want to develop a culture that rewards and promotes great design.\n  Thoughts   I understand why this is such a seminal piece and in a lot of ways feels very ahead of it\u0026rsquo;s time. I am very oblivious to what the state of computing was in 1986 but so much of what Brooks argues still rings true today. This is just further confirmation about the essential complexities in software design. Brooks is able to pick apart what parts of software design are always going to be challenges.\n  It often seems like our society has such a focus on making progress that we do not take a step back and think about where it is possible to make progress. Brooks identifies areas in software development we will struggle to make progress on because they are inherent difficulties. By taking the approach that Brooks does, we would be able to find what are the areas that make the most sense for us to invest our time in. We should take this approach with almost all things. People are constantly calling for quick fixes that can make large differences. This view probably over states the impact of any fix. It would be really good practice for me to examine something in more detail and find what are the accidental difficulties and what are the essential difficulties. I might try to do this with a different environment or task at some point.\n  I was really interested in Brooks advise to buy and not build because it comes up constantly in my day to day. When I use a python library, I am using something that someone else built rather than creating something completely from scratch. We also see this all the time with Software as a Service or the Cloud being such a huge business now. Rather than building our own version from scratch we buy it from someone else. I have been struggling with this for a bit because so many places are trying to sell their own Machine Learning Platform. I am tempted to maybe follow Brooks\u0026rsquo; advise but I have also not really seen a platform that fits any of my use cases.\n  An idea that Brooks hints at but does not discuss in depth because it is outside his scope is why humans struggle so much with the complexity of software. Software is so different from the physical world that we interact with on a daily basis. We are used to thinking about concepts in the physical world and interaction in our day-to-day. Due to software being so different from our day-to-day we cannot reason about it. But just because humans are not able to reason about software does not mean that will be the case for all forms of intelligence. While artificial intelligence is still very far away from being able to reason about software, I do believe there is some form of intelligence that could create much more beautiful and impressive software than humans.\n  The way Brooks is taking about making improvements reminds me of the 80-20 law. For the first 80 percent gain it takes 20 percent of the effort and then the last 20 percent gain comes with 80 percent of the effort. Essentially the beginning gains we make are the easiest and then each gain after that is harder and harder. This is true in so many places not just the software world. Even when it seems like there was some large break through that made a huge gain often that break through comes from a lot of unseen work over time. This really enforces why Moore\u0026rsquo;s Law is do different. I cannot think of many other examples of where progress was similar to Moore\u0026rsquo;s Law. I would be interested to explore what makes Moore\u0026rsquo;s Law so unique and how it has sustained progress in that way.\n  ","permalink":"https://judahgnewman.netlify.com/writing/papers/2020/silver_bullet/","tags":["Software Engineering"],"title":"No Silver Bullet - Essence and Accident in Software Engineering"},{"categories":["Books"],"contents":"Sing, Unburied Sing is a novel by Jesmyn Ward set in the American South. As with Ward\u0026rsquo;s other books, the novel does such an incredible job of getting inside characters heads. Ward is able to convey characters as nuanced individuals. She shows the darkest parts of our internal minds, how complex and convoluted they can be.\nSing, Unburied Sing feels like it is building towards something for the first half of the book. The reader does not know where we are going, but it feels like we have to be building towards something. There are two scenes which serve as climaxes for the novel. They are both quite moving. During the first half of the book I was kind of lost. Ward would often introduce something or someone without fully explaining them. The reader would only get the information later on through bits and pieces. The climactic scenes hit me differently and with more power because of the lead up to them.\nOne of my favorite parts about Ward\u0026rsquo;s writing is that it never feels like she is leading you to a specified destination. Many books feel like they are written with a specific lesson in mind. After reading the book we are supposed to have learned something. Ward\u0026rsquo;s approach seems to be quite different. Rather than write to a message, she attempts to write characters that are as truthful as possible. Characters that are messy and distraught. Characters that are still learning more about themselves and about the world.\nA less skillful writer would have made Leonie the villain of this novel. Leonie is quite often mean and neglectful of her children. She does things that hurt her parents and make their lives more difficult. It is especially easy to have a negative opinion of her because of the chapters from JoJo\u0026rsquo;s perspective. Ward is able to capture Leonie\u0026rsquo;s feelings of want and desire so well. I was overcome by the strength of Leonie\u0026rsquo;s desire for escape. It is not as simple as looking down on Leonie because she can be neglectful.\nWhile the book was only from the perspective of Leonie, JoJo and Richie, River and Michael are the real driving forces behind the novel. For all three of the characters, there was such intense desire to connect with either River or Michael. There were a number of passages describing how Leonie responded to Michael\u0026rsquo;s presence. We also heard about how Jojo and Richie both wanted to connect with Pop. The novel presents an intriguing look at how different characters can serve different roles based on whose perspective the book is written from.\nQuotes  But as the smell of onions and garlic, bell pepper, and celery cooked in butter clouds the air, Kayla rises and falls, her arms and legs flung out, her eyes shining, her mouth open in a smile so wide it looks like she could be screaming\n  getting grown means learning how to work that current: learning when to hold fast, when to drop anchor, when to let it sweep you up\n  I didn’t understand time, either, when I was young.\n  I shrug, but the memory comes anyway, like someone pouring a bottle of water over my head.\n  Time floods the room in a storm surge\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/sing_unburied/","tags":["Fiction"],"title":"Sing Unburied Sing"},{"categories":["Books"],"contents":"Silent Spring is the environmentalist Rachel Carson\u0026rsquo;s classic piece covering the dangers of pesticides. Carson explores the dangers of pesticides from so many different angles building up an intense amount of evidence of the dangers of pesticides. She highlights how harmful pesticides are to many different parts of our planet. The book is nothing if not repetitive show casing over and over again how we are doing damage to the planet with pesticides.\nWhile Carson\u0026rsquo;s book was written a while ago there are many lessons that can be learned from her writing and the situation with pesticides. Many parallels can be drawn between other ways we interact with our environment and how we use pesticides.\nWhile reading this book, I was thinking a lot about the terms progressive and conservative. While both of these terms are very loaded because of the USA political system, we can think of them as simple terms. Progressives are people who are constantly wanting change, looking for opportunities to make things better. Conservatives are people who are scared of making things worse, they recognize that change is not always good and there are things that might have bad outcomes. When I was reading the book, some of my initial thoughts were about how it is important to have both types of people. We need the people who are excited about the next change and looking for how we can make things better. We also need people who are skeptical about these changes and will put them under intense scrutiny.\nAfter a while, this kind of dichotomy made me feel uncomfortable. It seemed like an over simplification and that I was actually missing the point. It is not that there are always going to be both progressives and conservatives and we need them to balance each other out. Thats too convenient of a narrative that gives us an out. It says I can be a conservative because there will also be progressives or vice versa. Rather than relying on this idea of conservatives and progressives, what we learn from this book is that we all have to be more careful.\nWe should not aim to be conservatives or progressives. We should aim to be thoughtful and understand how little we know. We should look for opportunities to make improvements but also recognize that there is so much we cannot for see. There is so much conventional wisdom which was later shown to be false. We should be constantly pushing ourselves to think more objectively and understand what is going on. It is not about being a conservative or a progressive but having a greater understanding. If our pursuit is learning more and understanding more that will guide us in the right direction.\n Reading books provides an opportunity for me to learn from different mediums or people that I would otherwise learn from in my day to day. In my day to day I am regularly reaching about Machine Learning or Statistics. I am reading from software engineers and from people in the tech world. While in some respects that is quite a large world, in other respects it is actually quite narrow. Reading books provides me with that opportunity to learn from people outside of my career. Learning from this people often provides insights that I would not be able to obtain otherwise.\nWhile it is not a perfect parallel let us compare the current hype with Artificial Intelligence with pesticides. People say that pesticides have a good use case and then started applying them everywhere. This is something similar to what has happened with AI. Business leaders want to use AI everywhere without a clear understanding of how AI works, where it is successful and where it is actually dangerous. We can learn so much by looking at other trends in other fields and should apply lessons from them.\n There are many different ways to make an argument. There are different ways to weave evidence and rhetoric together. In Silent Spring Rachel Carson makes such a compelling argument but hitting us on the head over and over again with evidence. If every piece of evidence has us update our beliefs a little bit, but the end of the book it is very difficult to belief that pesticides do not due an intense amount of damage.\nQuotes  One important natural check is a limit on the amount of suitable habitat for each species.\n  This is an era of specialists, each of whom sees his own problem and is unaware of or intolerant of the larger frame into which it fits.\n  In the normal chemistry of the human body there is just such a disparity between cause and effect.\n  The trouble is that we are seldom aware of the protection afforded by natural enemies until it fails. Most of us walk unseeing through the world, unaware alike of its beauties, its wonders, and the strange and sometimes terrible intensity of the lives that are being lived about us\n  “Any science may be likened to a river,” says a Johns Hopkins biologist, Professor Carl P. Swanson. “It has its obscure and unpretentious beginning; its quiet stretches as well as its rapids; its periods of drought as well as of fullness. It gathers momentum with the work of many investigators and as it is fed by other streams of thought; it is deepened and broadened by the concepts and generalizations that are gradually evolved.”\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/silent_spring/","tags":["Nature","History"],"title":"Silent Spring"},{"categories":["Talks"],"contents":"This was a talk given at the American Economic Association (AEA) conference at the beginning of January 2020 by Marianne Bertrand. I was recommended this talk by Jenny Wang.\nSummary Bertrand begins her talk by highlighting that overall gender differences / bias are heading in the right direction. We see an optimistic story when we look at the wrong trends in gender differences. There are more females in the labor force and less of a difference in girls schooling. Currently, women make on average 84 cents for every dollar that a man makes in OECD countries. That is the smallest difference in history.\nBertrand reminds us to to remember this background as she talks about some of the remaining pain points. There are two key factors that play a part in this difference between men and women\u0026rsquo;s pay.\nEducational Choices While there is a smaller gap between genders in level of educational attainment there is still a very large gap in educational field distribution. There is a much higher proportion of men who get degrees in STEM fields than women. This is important because STEM educations generally lead to well paying careers. Differences in education choices between the two genders explain 6 per cent of the difference in average income. There is also evidence that within STEM jobs there is less of a pay gap between men and women then in other fields.\nPenalty of Parenthood Becoming a parent has a much larger penalty on income for women than it does for men. Becoming a father has essentially a non material impact on a mens income where as become a mother brings down a women\u0026rsquo;s income by a fair amount. In Denmark 80 to 85 percent of the earnings difference can be explain by this difference in penalty of a child.\nSo what does the literature say about why this parenthood penalty exists? There are a couple of different contributing factors\n Women will often exit the workforce or switch to part time after becoming a mother The school day has a different timing then the work day and therefore women need more flexible jobs. More flexible jobs often lead to a decrease in pay. Women look for jobs that are closer to home because they want shorter commutes. This depresses their earnings potential by limiting the pool of available jobs.   Now that we understand what are the two driving differences behind this gap, Bertrand tries to give us a structured way to think about this differences. She highlights two possible explanations.\n Men and women make different choices because of differences in skills or preferences. This is due to an inherent difference in the gender. The differences we observe above are a result of very powerful stereotypes that are engrained in our culture.  It is quite difficult to observe differences in skills or choices when those differences might be a result of these stereotypes.    Bertrand then highlighted two different approaches to thinking about stereotypes. One is the traditional economic perspective and one is the perspective from social psychology.\nThe traditional economic perspective focus on the descriptive nature of stereotypes. Stereotypes are believes about who women and men are and what they will be. This perspective argues that agents are still rational and that the stereotypes are because of imperfect information. We have stereotypes because we do not have all of the information and are making an inference with only partial or incorrect information.\nThe social psychology perspective argues that stereotypes are cognitive generalizations so that we can save cognitive resources. Stereotypes do not need to be accurate or rational. It also highlights that stereotypes are not only descriptive but prescriptive. They are making an argument about how people out to be. This prescriptive view is arguing that stereotypes are reinforcing. People act in certain ways because that is what is deemed as normal. There is a fair amount of evidence that stereotypes are not only descriptive but in fact are prescriptive.\nThe prescriptive view of stereotypes highlights that rational self interest plays a part in gender norms. When a rational agent is making a choice they are weighing the cost and benefits. They calculate that there is a consequence of acting in consistently with gender stereotypes. If we accept this conclusion it means that preferences are ambiguous because they are effect by gender norms. We therefore cannot conclude that these differences are driven by preferences because preferences are dictated by gender norms. In this context talking about preferences is really meaningless.\nSimilarly to preferences, skill can also be a result of existing stereotypes. When math teachers have gender stereotypes about girls being worse at math, that leads to girls actually performing worse at math. There is empirical evidence that the larger gender stereotypes are in a country, the larger the gender gap in math is. Bertrand also brought up the point that it is very difficult to use self reported data when people\u0026rsquo;s behavior is being driven by gender norms.\n Bertrand next discussed why there might be some gender gaps. A hypothesis that she highlighted is that even if girls have equal levels of ability with boys at math, they choose to go into a non-stem field because that is where they have a comparative advantage. This mean that even if women are equal with boys at math, if they are better at literature they will go into the literature fields.\nBertrand also wanted to see if our current believes about gender are even accurate. If you at almost any given trait, the within gender variation for both men and women is a lot larger than the between gender variation. This means that the are much larger differences between men in most traits then the differences between men and women are.\nNext she discussed how certain types of stereotypes are stronger than others. She examined the difference between the strength of a stereotype and measurable difference between two genders. For example, how much do people perceive women to be kinder than men, and how much kinder are women than men. She found that the stereotypes which first came to people\u0026rsquo;s mind were the ones with the largest gap between perceived differences and actual differences.\nShe next showed graphs of trends in different types of stereotypes over time. It appears the stereotypes about competence and intelligence are going away. For example, a lot less people now think that men are inherently smarter than women. Stereotypes are about agency and communion though are decreasing. It is very difficult to talk about gender stereotypes in general without examining specific gender stereotypes.\nThe last point she made in this section is that positive gendered stereotypes might be less likely to change but still can have negative impacts. People might think you are a bigot for saying women are less intelligent than men, but might not a problem if you say women are kinder than men. This type of stereotype can still negatively impact women though.\n Bertrand ended her presentation discussing how all of these findings should impact how we think about stereotypes. SHe began the section that more research on a lot of this topics is needed and that there needs to be more incentives for people in the field to do different types of new research.\nResearch has shown that the most powerful counter to stereotypes is exposure to thing which go against the stereotype. This has the implications that we should avoid policies that take women out of the work force. While a more generous maternity leave sounds good in principle, it has the potentially negative effect of taking women out of the work force. Instead we could think of a policy that puts more of the parental burden on men and provides paternal leave. She also highlighted that joint taxation can depress women\u0026rsquo;s work by putting them in a higher tax bracket than individual taxation.\nIn general, we need more policies that are gender neutral. There should also be more government spending on child care and education. We should institute polices that counter stereotypes by ensuring father take time off around their child\u0026rsquo;s birth.\nBertrand then discussed how we can think about organizational practices and gender roles. In a lot of organizations there is an increased reward for inflexible or unstable work. This type of work is often more of a burden on women who are tasked with child chare. This is a collective action problem where each firm is solving their own cost minimization problem, but no one is internalizing the costs to society as a whole.There is also evidence that sexual harassment is more common with unstable or inconsistent work.\nRather than talking about diversity and inclusion, we should discuss inclusion and diversity. Diversity does not mean anything if it does not include people. The idea of diversity reinforces us seeing gender rather than being inclusive of all. We should constantly be vigilant that stereotypes could be anywhere. Fast systems where people are making quick decisions is more likely to have stereotypes that slower systems.\nThoughts   Wow, Wow, Wow. So much ground was covered in this talk it was kind of incredible. Bertrand was able to discuss so much different literature and connect it to tell a story so well. It is very impressive to me when people are able to connect so much work to tell cohesive narratives.\n  Stereotypes are very difficult to think about and Bertrand helped the audience build that tool kit from the ground up. So much discussion in day to day life about stereotypes can feel kind of wishy washy. Bertrand was able to highlight so much research and gives us a tool kit to think about stereotypes. I feel much more equipped to think about why stereotypes exist and how to combat them.\n  I really appreciated her highlighting the limitations of the economic perspective anc how much she has learned from social psychology. She also called out the need for different types of research.\n  One thing that really stuck with me was some of her policy implications. Like the idea that increasing maternal leave is actually potentially harmful. Or that girls choose to go into fields of literature because that is where their comparative advantage is. A solution to this might be to make boys better at reading. A lot of interesting policy discussions. I loved that she went from initial research all the way to how we can create better policy.\n  ","permalink":"https://judahgnewman.netlify.com/writing/talks/bertrand_gender/","tags":["Economics"],"title":"Gender in the 21st Century: Marianne Bertrand"},{"categories":["Papers"],"contents":"Summary This paper explores matching in two sided markets through the multi armed bandit problem. An example of a two sided market we can model using multi armed bandits is matching workers to employees. There is a pool of workers and a pool of employees. We want to find the optimal matching of employees and workers. Employees have preferences over workers, and workers have preferences over employees. There has been a lot of working that examines how to achieve an optimal matching. The most well known example of this is the Gale-Shapley algorithm.\nThe novel contribution of this paper is it examines the setting where we do not know agents preferences. We have to learn them over time. In Gale-Shapley we know everyone\u0026rsquo;s preferences and have to find a matching that is optimal according to those preferences. In this new situation, rather than making one matching using known preferences, there are multiple time steps and at each time step we have to make a matching. As we make matches we see the outcome of a match and learn agents preferences.\nThe paper sets up the scenario according the the following rules:\n There are N agents and K arms. (An arm is just a matching. So in the example of employees and workers, employers are agents and picking a worker is selecting an arm) N \u0026lt; K → every agent gets an arm At time step t each agent selects an arm Arms have preferences over agents which are known When multiple agents select the same arm, the agent which the arm has a higher preference for gets matched The agent receives a reward from matching to arm that is sampled from a gaussian with a mean based on the specific arm-agent pairing. An optimal match is defined as when an agent gets it preferred arm. A pessimal match is when an agent gets its least preferred arm.  This setting is difficult because there is a trade off between exploration and exploitation. If you know that an arm gives you a high reward you may want to keep pulling it over and over again. If you have find a worker that is good you want to keep hiring them. On the other hand, it is possible there is a worker that is even better that you do not know about. In order to find that worker that is even better you have to do exploring, rather than continuing to higher your known optimal. There is a trade-off between maximizing your pay-off with what you already know, versus being able to get a higher pay off from what you do not know.\nThe authors present two different platform that could facilitate the matching: decentralized and centralized.\n In a centralized platform agents send in a ranking of arms and then the platform returns a matching vector. With a centralized platform we never have any conflicts where two agents select the same arm.\nFor a centralized platform one approach is the explore-then-commit approach. The explore section is devoted to learning agents preferences. The platform would return random matchings sampling from the space of all possible matchings. This would allow the platform to learn about agents preferences. The platform would then transition to the commit phase where it would create the best matching learned from the explore phase.\nThe authors argue this approach is not optimal because we are not taking advantage of what we learning during the explore phase. We only use what we have learned when we get to the commit phase. This could lead to us exploring matches that we know are not less optimal then when we have already tried. The authors propose another approach where at each time step the platform uses agents preferences to return a stable matching. The platform sees the results of this matching and learns more about preferences before creating the next stable matching at the next time step. The authors call this method Upper Confidence Bound (UCB) matching.\n There are two types of decentralized platforms. In a decentralized platform with partial information agents observe each other actions and conflicts but cannot coordinate. In a decentralized platform with no information agents cannot even observe others actions.\nUsing Gale-Shapley matching it has been shown that agents cannot do better by faking their preferences. The authors also explore honesty in this setting. They find that under the assumption everyone else is honestly presenting their preferences, an agent cannot achieve a better outcome by submitting false preferences.\nThe authors look at the performance of both policies for centralized platforms and show that UCB approach performs better than explore-then-commit. They briefly look at the explore-then-commit strategy for decentralized platforms mostly as a way of motivating the necessity of further work.\nThoughts   I really enjoyed this paper! I have been thinking a lot about tradeoffs of late. In almost all things there is an inherent tradeoff between different approaches. We want to find a solution that takes the benefits of the different approaches into account and is able to act optimally. In this setting the tradeoff between exploitation and exploration is quite clear. This paper does a good job of taking something complicated and simplifying it to a place where we can see the core trade off.\n  The authors claim that this paper is a blend of machine learning and economics. In this paper I do not see many aspects of machine learning but here is why I imagine the authors make that claim. Machine Learning very rarely gives us to tools to handle scarcity. A classic machine learning setting would be learning agents preferences over arms. Machine learning would allow us to learn fairly good estimates of preferences without each agent having to sample every arm. By sampling a subset of the arms we would be able to create preferences for each agent. An example of this is Netflix learning users preferences of videos. Netflix is able to create estimates for a user\u0026rsquo;s preference for a video, without them watching the video based on their preferences over other videos. In the Netflix situation though, multiple agents can watch the same video. We do not have to worry about finding an optimal matching because everyone can be watching the same video. But imagine a situation with scarcity. Only one person can watch each video. Machine Learning does not give us the tools to solve this problem. To solve this we need to turn towards economics. The first thing I learned in economics classes was how to think about an agent with a budget constraint. An agent only has so many resources and then has to allocate them in an optimal way. Economics give us the framework to answer this type of question by building a model of human behavior. We know users preferences and using this model we can derive what they will do. This paper blends machine learning and economics because we can use machine learning to learn agents preferences and then economics to predict behavior once we know preferences.\n  Another way machine learning could be really powerful in this setting is to create individual policies for a decentralized platform. Imagine you are an agent in a decentralized platform and you have to make a decision between exploitation of what you know or exploring more. You could use the data you have already seen to help make this decision. If you assume rewards follow a known distribution, you could use the rewards you have seen to make an inference about what that distribution is. You can then make an educated decision about whether you are at a point where it makes sense to explore or to exploit.\n  Another interesting line of research is examining how much worse a decentralized platform is from a centralized platform. While a centralized platform is nice because we have more information when we make decisions, there are also dangers of a centralized platform. It gives one entity a lot of control over the market. Imagine the centralized platform decided that it did not want to make matchings that were optimal for agents, but rather matchings that it found to be optimal. It will be important to understand how much worse decentralized platforms are at making matchings under different assumptions.\n  This paper only looks at rewards coming from certain matchings. It does not consider that there might be socially optimal matchings that incorporate things outside of the individual rewards. It is possible that there are certain rewards we only receive when a set of matchings is made. For example if a certain ten matches are met, then there is an additional reward to each of the agents in those ten matches. This increases the complexity of the problem because it is not only about the individual match but about groups of matches.\n  Another interesting direction is exploring a platform in between centralized and decentralized. I can imagine a scenario where small groups can coordinate and communicate but the entire set of agents cannot. What would happen if groups of ten agents could communicate?\n  ","permalink":"https://judahgnewman.netlify.com/writing/papers/2020/bandits_matching_markets/","tags":["Economics"],"title":"Competing Bandits in Matching Markets"},{"categories":["Books"],"contents":"American Marriage by Tayari Jones is an absolutely gripping novel about a couple where the husband goes to jail for a crime he did not commit. The novel is written from the perspective of three different characters in long form first person monologues. I have not read a book in a while that was as gripping as American Marriage. There is something so powerful about first person internal monologues. The book is paced in a way that things are constantly turning and changing in different directions. The reader is constantly on the edge of their seat about what is going to happen next.\nNormally, when I read a book there are certain characters I really resonate with and that I am rooting for. One of the most powerful things about American Marriage is how each character has a point of view that resonated with me. I was not clearly rooting for any of the characters. They all felt like real people that were selfish and difficult at times. I could relate to the different emotions they were feeling. This was not a hero versus villain narrative.\nWhile I was reading the book, it was so easy to get caught up in the characters emotions and relationships that I forgot there was a real villain in this story. The villain is the American justice system and how it tears people apart. The criminal justice system tears families apart and disproportional affects people of color.\nQuotes  It was a wonderful feeling to be grown and yet young\n  Sweets are curious, temperamental, and moody. Any cake mixed by hand on this day would slump in the oven, refusing to rise\n  Much of life is timing and circumstance\n  Is motherhood really optional when you’re a perfectly normal woman married to a perfectly normal man?\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/am/","tags":["Fiction"],"title":"American Marriage"},{"categories":["Books"],"contents":"The Art of Learning by Josh Waitzkin is a deep dive about how to reach the top of your field in any discipline. Josh was consistently one of the best chess players in his age group growing up. He discusses why he excelled at chess and the generalizable principles he learned. In his twenties Josh transitioned away from playing chess to studying Tai Chi. Josh became the world champion of Push Hands Tai Chi. Josh connects his mastery of chess to his study of Tai Chi.\nThe Art of Learning talked about principles I had heard of before in greater detail. Josh discusses how to reach a mental state of high performance. He also explains how to get iteratively better at something and how to become more aware of your own faults. The Art of Learning is well worth the read just to see how someone at the top of two different fields thinks about improvement.\nAfter reading the book I was reflecting on how to incorporate some of the lessons in the book into my own day to day. It feels like I will often read something, really enjoy it, feel like it has a lot of wisdom, and then just keep moving on. I think there are two problems with this way of looking at things. The first is that after reading a book, I will just be able to incorporate all of the information from it. Change takes a while and I will not be a different person all of a sudden. If I really want to take knowledge away from this book it will involve slowly incorporating some of its lessons. The other problem is that even if I am not directly implementing something from the book, I might still have learned a lot from it. It is possible that some of the learnings from the book have gotten into my head and I will use them without even realizing it.\nThe Art of Learning also made me think about how much I enjoy reading about someone who inspires me. Reading about people who we look up to inspires us and makes us feel like we could be great. We see people who inspire us are just normal people too. They worry about the same things as us and struggle with the same things as us.\nQuotes  The key to pursuing excellence is to embrace an organic, long-term learning process, and not to live in a shell of static, safe mediocrity. Usually, growth comes at the expense of previous comfort or safety.\n  The first mistake rarely proves disastrous, but the downward spiral of the second, third, and fourth error creates a devastating chain reaction..\n  The human mind defines things in relation to one another—without light the notion of darkness would be unintelligible\n  In all disciplines, there are times when a performer is ready for action, and times when he or she is soft, in flux, broken-down or in a period of growth.\n  We have to be able to do something slowly before we can have any hope of doing it correctly with speed.\n  Not only do we have to be good at waiting, we have to love it. Because waiting is not waiting, it is life. Too many of us live without fully engaging our minds, waiting for that moment when our real lives begin.\n  Once you know what good feels like, you can zero in on it, search it out regardless of the pursuit.\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2020/a_of_learning/","tags":["Reflections","Non-Fiction"],"title":"The Art of Learning"},{"categories":["Papers"],"contents":"Summary The authors outline that due to decreasing performance of phone polls new approaches to political polling are becoming prominent. These new methods of getting survey responses do not get perfectly random samples. The authors argue we can use new statistical methods to make accurate inference without random samples.\nThe authors lay out the following work flow:\n Draw a survey sample directly from the voter database or matching responses from an already existing survey to the database Use auxiliary data from the database to construct a flexible statistical model on the quantity (or quantities) of interest Project inferences from the statistical model to the full target population on an individual level Use the projected individual-level inferences, which can be seen as a pseudo-survey dataset, to construct larger group-level inferences about either the electorate as a whole or subgroups within the electorate.  The authors outline this with an example of creating a model for predicting 2012 Presidential Vote. They use a logistic regression model to predict vote for either Obama or Romney. The response variable comes from the survey data and all of the predictive variables come from the voter file data. They use some individual level variables such as gender and race. They interact all of the different variables with each other. They also use geographic level information about where an individual lives.\nOnce the authors have fit their individual model, they do post-stratification to ensure in each state the correct number of votes were assigned to each candidate. After doing this post stratification the authors then look at different aggregations of the data to examine trends in voting.\nThe authors end the paper by discussing different limitations and further opportunities for research. I will touch on a lot of these below.\nThoughts   The authors were able to use actual voting results to do post stratification on their model. They talk about how if you were doing pre-election modeling that would not be possible and you might get more biased results. In a lot of ways they picked an example that would work out very nicely. If we are building a model for 2020 vote, we would not be able to do post stratification. We would need to get a better sense of how much this post stratification changes the results. We could test out our model with a variable we do know the answer for to help understand this better.\n  In the paper the authors only use one variable from the survey, the variable of interest, and then all other variables are in the voter database. But what if we wanted to use dependent variables from the survey. Imagine we asked a question about what is your primary news source and then wanted to use that. We would have to first create a model for news source and model that onto the voter file data. How would we propagate the uncertainty from that model into the model for vote? We also would probably have to make a model for turnout. The question boils down to how do we combine multiple different models?\n  In the paper the authors mention that there is a limitation of their matching algorithm not being perfect. They also mention that some of the variables in the voter file are modeled. To start with we would want to use the existing infrastructure of matching and the model data. Eventually it could make sense to make models of these ourselves. This could be really beneficial if we find a way to combine models that is elegant and combines uncertainty.\n  In the paper they report their results, but do not really explain how they reached their selection of dependent variables. They absolutely tried out multiple models with different dependent variables. It would be great to know what other models they tried out and what their performance was. It is a shame that academic papers generally only report the results of the final model. It would be interesting to see how much of a degradation there was in accuracy if they removed one of their variables. Predicting presidential vote is one of the easier things to predict, could they have reached a model of similar accuracy with a lot fewer variables?\n  In this paper they just modeled one variable, which was a binary value. If we wanted to model two variables would it be more beneficial to make two different models. It might make more sense to have one model that jointly predicts both. I would lean towards the latter making more sense.\n  In the paper they called out the fact that they did not consider how the survey data had come from different time periods. Do we weight data from three months ago less than data more recently? How do we do that? Would the model be substantially different if we trained it on data from different time periods. Could we use that to see how opinions have changed?\n  Catalist seems like they do really good work. The paper that looks at the accuracy of their modeled variables is nice. It could be nice to work with them.\n  ","permalink":"https://judahgnewman.netlify.com/writing/papers/2020/gelman_vf_mrp/","tags":["Statistics"],"title":"Voter Registration Databases and MRP"},{"categories":["Papers"],"contents":"By Paul H. Garthwaite, Joseph B. Kadane and Anthony O’Hagan\nSummary This paper is an overview of how to take experts believes and turn them into statistical distributions. It combines a long line of psychology research with statistics research. The situation we are examining is when we want to create a distribution over a value from an experts opinion. It is impossible to ask an expert to give us a probability distribution so we need a methodology to query an expert that allows us to create the distribution ourselves. An expert can be anyone who has information about the value we are interested in. If I was modeling how much time my friend sleeps, than my friend would be the expert.\nSo what does it mean for an elicitation to be successful? The authors argue that there is not a true distribution. We want to find the distribution that most closely matches the experts beliefs. An example of the setting the authors are describing is asking for an experts advice on an input to a model. Imagine we are modeling house prices. We have a couple of different variables and want to ask an expert how to weigh different variables. In this case the input is the weight of the variable and we want to create a distribution for that weight from the expert.\nThe paper defines four parts of the elicitation process:\n The setup: selecting experts; training experts; identifying what to elicit. We then elicit from the experts We fit a distribution using the information we have elicited We assess validity of elicitation and return to stage 2 if deemed necessary  The authors caution that if the expert cares about the end goal, the information they provide might be biased. The paper uses the example of asking a radiation expert about the impacts of Chernobyl. An expert might be incentivized to over play the impact because it increases the social attention to their field.\nThe authors highlight that whenever we do belief elicitation we should attempt to do the following things:\n Understand what an experts knowledge is based off of. Understand if the expert has any biases Train the expert so they understand how to accurately give information Keep a record of the elicitation  The authors review a long series of literature highlighting different ways our experts answers might be biased. Some of these are biases due to how we ask questions and some of them are biases due to human nature. Understanding these biases allows us to ask questions in ways that mitigate these biases. For example, research has shown humans are bad at estimating variances and bad at estimating means. Humans are much better at estimating medians, proportions and credible intervals. We should ask questions that line up with human\u0026rsquo;s natural intuitions. Rather than asking an expert for the variance parameter of a distribution, we should ask them a question about proportions in an interval.\nThe next step is turning the experts answers into distributions. While it is nice to ask question using words, words can be very hard to turn into distributions. \u0026ldquo;quite likely\u0026rdquo; or \u0026ldquo;extremely likely\u0026rdquo; will mean different things to different people. It can be difficult to decide what distribution to use once we have information from the expert. We can choose a distribution from a family of distributions that most closely matches an expert\u0026rsquo;s answers.\nAnother approach the authors mention is to use a hierarchical model and directly model the accuracy of the expert. This allows us to incorporate uncertainty and make less strong assumptions.\nThere is another section of the paper where the authors discuss how to pool together opinions from multiple experts. They discuss situations in which we get information from the experts together and separately. This is difficult because there is both the statistical question of how to combine different experts beliefs and the psychological question of how different experts might impact each other.\nMy Thoughts   The setting this papers talks about is a critical one. When we do not have a lot of data, it is often necessary to combine data with some human knowledge. We have much more flexibility in our modeling if we can create a distribution for a parameter rather than just a fixed value. This type of setting should happen a lot more when people are creating models. We should actively combine data with experts knowledge. This can be especially powerful in situations with very sparse data. What if we want to model the probability that an algorithm discriminates against people of color? We might only have very minimal data about this, but we will be able to ask experts opinions.\n  That last example is a setting where we want to use people\u0026rsquo;s answers directly as data in our model. Imagine we want to model the likelihood of the Lakers winning the NBA Championship. A powerful way to do this would be to ask all of the coaches their beliefs. Following the authors advice gives us a structured and systematic way to do this. Another example is asking someone how likely they are to vote. There is a distribution over this value. Finding an accurate way to get this distribution from a survey taker is very important.\n  I really like the idea of adding another hierarchical step in our model and using that to model accuracy from our experts. This adds more complexity to our model but it gets rid of an assumption. This trade off between assumptions and complexity can be a difficult one to make. We diminish our complexity by making more assumptions. With the correct data and model this approach could be very powerful by reducing our assumptions.\n  This paper was a bit too long for my liking and went into too much detail on the examples. I know it was a survey paper and so it condensed a lot of research but it did not do a great job of summing up general principles. The best way to write a survey paper is to combine summarizing of generalizable principles with examples that show case those principles.\n  I am always wary when I read older papers. I am scared that there is a newer one that is more up to date and contains better information. Depending on the field a paper from 2005, could be ages ago or still quite relevant. It would be incredible if when reading a paper it told you if there was more recent research and this paper was now out of date.\n  ","permalink":"https://judahgnewman.netlify.com/writing/papers/2020/eliciting_beliefs/","tags":["Statistics"],"title":"Statistical Methods for Eliciting Probability Distributions"},{"categories":["Books"],"contents":"As another year passes I reflect on what I did over the past year and what I learned. Looking back it is tempting to focus on what has changed over the past year. It is tempting to compare where I am now and where I was a year ago. I have moved to a new city and started a new job. But comparing one year to another is so divorced from the actual experience of a year. Switching jobs was something that happened gradually over time. Moving cities was also a process that happened gradually over time. While there is a sharp discontinuity around each of these events, they were both actually long processes.\nThe best way for me to accurately sum up my year is too look back at the books I read. Doing this helps me remember what I was thinking about and what I learned. It allows me to see that my thinking trends in certain directions. That there are thoughts I return to. It gives me reference material that I can refer back to about a period of time in my life.\nWhen I did this in 2018, I looked back through all my highlights and found the quotes that were the most meaningful. For 2019, it was mostly the same process. Once, I started this website though, I started writing about each book after I finished it and keeping track of the quotes. For the books I read towards the end of the year, you will see a link rather than the quotes directly below.\nHaving gone through this process, there are a couple of themes in my thinking that stick out.\n There were a couple of different topics I read about a lot. During the summer I read a fair amount about health. Through out the year I read quite a bit about race and how race impacts society. I also read a number of books about the environment and climate change. These are three different subjects that interest me a lot and I am excited to continue learning more about them.\nThe largest theme that seemed to purvey all that I read, is how everything is more complicated then we make it out to be. We are constantly trying to simplify and categorize things. The amount of randomness and information out there is so large, that we have to simplify it to get any understanding. It is in human nature to view things as binary rather than on a spectrum. We seek out clear boundaries rather than fuzziness. Telling attractive narratives and not seeing randomness. It is a constant fight against our own nature to see the truth and not give in to oversimplifications.\nWe can objectively know all of this and still be susceptible to it. I have personally struggled a lot this year with the idea that I am insignificant. There is nothing special about me and that I am just one being. I find myself constantly yearning to be special, to be different. I have also been thinking about this on the macro level of the human race. What makes humans any more special than any other creature? We have defined our existence as being more important. But are our definitions correct? Would a different species not make different definitions?\nWorking through this question through out the year I have come to a place of some acceptance. While it will be something I continue to work through for a lifetime, I have come to a place that makes some sense to me. I am not special because of my skill at anything. I am not special because I am smart or because I particularly excel at anything. I am special because of my own personal history and existence. No other person has the same history or experience as me. I am a unique individual that brings a different history and different perspective than anyone else. This is what makes me special. But it is also what makes all individuals special.\nPreviously, being special has meant differentiating myself from others. I am special if there is something that I am that other individuals are not. For me to be special, other people must be normal. But this new meaning of being special does not require that distinction. It does not compare me to others. It understands that everyone is special. I am not special because I am in someway differentiated from others but rather recognizes the inherent uniqueness of an individual experience.\nThe Three Body Problem - Liu Cixin  It’s important to take the time out of our busy schedules to do something entirely unrelated to our immediate needs. This project has allowed us to give some thought to issues we have never had time for\n  The long years had ground away all the hardness and fierceness in their personalities, until all that was left was a gentleness like that of water\n The Autobiography of Malcom X - Malcolm X, Alex Haley  But people are always speculating—why am I as I am? To understand that of any person, his whole life, from birth, must be reviewed. All of our experiences fuse into our personality. Everything that ever happened to us is an ingredient.\n  Behind bars, a man never reforms. He will never forget. He never will get completely over the memory of the bars.\n  “Don’t condemn if you see a person has a dirty glass of water,” he said, “just show them the clean glass of water that you have. When they inspect it, you won’t have to say that yours is better.”\n  The Holy Bible in the white man’s hands and his interpretations of it have been the greatest single ideological weapon for enslaving millions of non-white human beings.\n  “I’m man enough to tell you that I can’t put my finger on exactly what my philosophy is now, but I’m flexible.”\n Salvage The Bones - Jesmyn Ward  there is a great split between now and then, and I wonder where the world where that day happened has gone, because we are not in it.\n Zone One - Colson Whitehead  New York City in death was very much like New York City in life. It was still hard to get a cab, for example.\n  We never see other people anyway, only the monsters we make of them\n The New Geography of Jobs - Enrico Moretti  The focus on short-term events often results in information that is incomplete, irrelevant, or both\n  But trade is not a zero-sum game like a football match, where if your opponent wins you lose. The reality is that if one of our trading partners becomes more productive, the goods we are buying from that country become cheaper. This makes us—the consumers—a little richer.\n  Because of the self-sustaining nature of economic development, cities that are similar initially can become very different over time as small differences become magnified.\n  It is in our own interest to subsidize other people’s education, as it ends up indirectly benefiting us.\n  Location is more important than ever, in part because knowledge spillovers are more important than ever. This is a key reason for the accelerating divergence in the fortunes of the three Americas.\n The Parable of the Sower - Octavia Butler  CIVILIZATION IS TO GROUPS what intelligence is to individuals. It is a means of combining the intelligence of many to achieve ongoing group adaptation. Civilization, like intelligence, may serve well, serve adequately, or fail to serve its adaptive function. When civilization fails to serve, it must disintegrate unless it is acted upon by unifying internal or external forces.\n  But things have changed a lot, and they’ll change more. Things are always changing. This is just one of the big jumps instead of the little step-by-step changes that are easier to take.\n The Black Swan - Nicholas Nassim Taleb  What is surprising is not the magnitude of our forecast errors, but our absence of awareness of it.\n  I developed the governing impression that our minds are wonderful explanation machines, capable of making sense out of almost anything, capable of mounting explanations for all manner of phenomena, and generally incapable of accepting the idea of unpredictability.\n  Categorizing always produces reduction in true complexity.\n  Our minds are like inmates, captive to our biology, unless we manage a cunning escape\n  Remember that we treat ideas like possessions, and it will be hard for us to part with them\n The Orphan Master\u0026rsquo;s Son - Adam Johnson  Then sleep, communal, a hundred boys bunked four tiers deep, all their common exhaustion articulated as a singularity. It was nothing short of belonging, a feeling that wasn’t particularly profound or intense, it was just the best he tended to get\n  how difficult it was to come to see the lies you told yourself, the ones that allowed you to function and move forward. To really do it, you needed someone’s help\n Breakfast of Champions - Kurt Vonnegut  They were doing their best to live like people invented in story books. This was the reason Americans shot each other so often: It was a convenient literary device for ending short stories and books.\n  I resolved to shun storytelling. I would write about life. Every person would be exactly as important as any other. All facts would also be given equal weightiness. Nothing would be left out. Let others bring order to chaos. I would bring chaos to order, instead, which I think I have done.\n Think Like a Commoner - David Bollier  The traditional advocates of reform, liberals and social democrats, while generally concerned with market abuses and government malfeasance, are themselves too exhausted to imagine new paths forward\n  Put another way, a commons is a resource + a community + a set of social protocols. The three are an integrated, interdependent whole.\n  If we are to understand the law of the commons, we must start by expanding our notion of law itself. Law is not just formal, written and institutional; it is also informal, oral and social.\n  The price system typically fails to take account of all sorts of value that are external to the marketplace.\n  On the other hand, a certain redundancy and inefficiency are essential to a system’s long-term resilience\n I Know Why The Caged Bird Sings - Maya Angelou  I find it interesting that the meanest life, the poorest existence, is attributed to God\u0026rsquo;s will, but as human beings become more affluent, as their living standard and style begin to ascend the material scale, God descends the scale of responsibility at a commensurate\n  Never heard the words, despite the thousands of times I had sung them. Never thought they had anything to do with me.\n  To be left alone on the tightrope of youthful unknowing is to experience the excruciating beauty of full freedom and the threat of eternal indecision. Few, if any, survive their teens. Most surrender to the vague but murderous pressure of adult conformity. It becomes easier to die and avoid conflicts than to maintain a constant battle with the superior forces of maturity\n The Dispossessed - Ursula K. Le Guin  Like all walls it was ambiguous, two-faced. What was inside it and what was outside it depended upon which side of it you were on\n  No matter how intelligent a man is, he can’t see what he doesn’t know how to see\n  To make a thief, make an owner; to create crime, create laws\n  They say there is nothing new under any sun. But if each life is not new, each single life, then why are we born?\n The Dog of the South - Charles Portis Born a Crime - Trevor Noah  In America the dream is to make it out of the ghetto. In Soweto, because there was no leaving the ghetto, the dream was to transform the ghetto.\n  I believed that Fufi was my dog, but of course that wasn’t true. Fufi was a dog. I was a boy. We got along well. She happened to live in my house. That experience shaped what I’ve felt about relationships for the rest of my life: You do not own the thing that you love.\n Less - Andrew Sean Greer  That the mind cannot be trusted is a certainty\n  It’s that they’ve survived everything in life, humiliations and disappointments and heartaches and missed opportunities, bad dads and bad jobs and bad sex and bad drugs, all the trips and mistakes and face-plants of life, to have made it to fifty and to have made it here\n Homegoing - Yaa Gyasi  Strength is knowing that everyone belongs to themselves\n  A poet’s got to spend more time livin’ than he does studyin\n  So when you study history, you must always ask yourself, Whose story am I missing? Whose voice was suppressed so that this voice could come forth?\n  Sonny had never loved her, not really. But he had always wanted her. It took him a while to learn the difference between those two things.\n The Overstory - Richard Powers  A kind of awareness—something so different from human intelligence that intelligence thinks it’s nothing\n  The psyche’s job is to keep us blissfully ignorant of who we are, what we think, and how we’ll behave in any situation. We’re all operating in a dense fog of mutual reinforcement. Our thoughts are shaped primarily by legacy hardware that evolved to assume that everyone else must be right. But even when the fog is pointed out, we’re no better at navigating through it\n  To be human is to confuse a satisfying story with a meaningful one, and to mistake life for something huge with two legs\n The Book of Why - Judea Pearl  You cannot answer a question that you cannot ask, and you cannot ask a question that you have no words for\n  while probabilities encode our beliefs about a static world, causality tells us whether and how probabilities change when the world changes, be it by intervention or by act of imagination.\n  causal analysis requires the user to make a subjective commitment. She must draw a causal diagram that reflects her qualitative belief—or, better yet, the consensus belief of researchers in her field of expertise—\n  The questions I have just asked are all causal, and causal questions can never be answered from data alone. They require us to formulate a model of the process that generates the data, or at least some aspects of that process\n The Emperor of All Maladies - Siddhartha Mukherjee  Scientists often study the past as obsessively as historians because few other professions depend so acutely on it. Every experiment is a conversation with a prior experiment, every new theory a refutation of the old\n  “Basic research,” Bush wrote, “is performed without thought of practical ends. It results in general knowledge and an understanding of nature and its laws. This general knowledge provides the means of answering a large number of important practical problems, though it may not give a complete specific answer to any one of them.… “Basic research leads to new knowledge. It provides scientific capital. It creates the fund from which the practical applications of knowledge must be drawn.\n  “The death rates from malaria, cholera, typhus, tuberculosis, scurvy, pellagra and other scourges of the past have dwindled in the US because humankind has learned how to prevent these diseases.… To put most of the effort into treatment is to deny all precedent.”\n  Cancer’ is, in truth, a variety of diseases. Viewing it as a single disease that will yield to a single approach is no more logical than viewing neuropsychiatric disease as a single entity\n There There - Tommy Orange  You don’t have to defend all white people you think aren’t a part of the problem just because I said something negative about white culture\n  Everything here is formed in relation to every other living and nonliving thing from the earth….Nothing is original, everything comes from something that came before, which was once nothing\n The Breakthrough - Charles Graeber  Scientists are people; they have beliefs, and are personally invested in them. And that can sometimes lead to unintentional and often unconscious bias, and a sort of intellectual blindness. It can, in other words, make even scientists unscientific.\n  the goal isn’t necessarily beating cancer today; it’s staying alive long enough to take advantage of the next advances, the ones right around the corner.\n The Fire This Time - Jesmyn Warn  I believe there is power in words, power in asserting our existence, our experience, our lives, through words. That sharing our stories confirms our humanity. That it creates community, both within our own community and beyond it\n  What my English teachers didn’t say was that voices aren’t discovered fully formed, they are built and shaped—and not just by words, punctuation, and sentences, but by the author’s intended audience, by the composition’s form, and by subject\n Snow Crash - Neal Stephenson  the human mind can absorb and process an incredible amount of information—if it comes in the right format. The right interface. If you put the right face on it\n  Besides, interesting things happen along borders—transitions—not in the middle where everything is the same\n  Computers rely on the one and the zero to represent all things. This distinction between something and nothing—this pivotal separation between being and nonbeing—is quite fundamental and underlies many Creation myth\n  That was part of Enki’s plan. Monocultures, like a field of corn, are susceptible to infections, but genetically diverse cultures, like a prairie, are extremely robust\n Why We Sleep - Matthew Walker  A theory that cannot be discerned true or false in this way will always be abandoned by science, and that is precisely what happened to Freud and his psychoanalytic practices\n  This mentality has persisted, in part, because certain business leaders mistakenly believe that time on-task equates with task completion and productivity. Even in the industrial era of rote factory work, this was untrue. It is a misguided fallacy, and an expensive one, too\n Eileen - Otessa Moshfegh  I didn’t want anyone to think I was susceptible to bad breath, or that there were any organic processes occurring inside my body at all. Having to breathe was an embarrassment in itself. This was the kind of girl I was.\n  I had no idea that other people—men or women—felt things as deeply as I did. I had no compassion for anyone unless his suffering allowed me to indulge in my own\n Palaces for the People - Eric Klinberg  Social infrastructure is crucially important, because local, face-to-face interactions—at the school, the playground, and the corner diner—are the building blocks of all public life. People forge bonds in places that have healthy social infrastructures—not because they set out to build community, but because when people engage in sustained, recurrent interaction, particularly while doing things they enjoy, relationships inevitably grow\n  Social infrastructures that promote efficiency tend to discourage interaction and the formation of strong ties.\n The Brothers Karamazov - Fyodor Dostoyevsky  It’s still possible to love one’s neighbor abstractly, and even occasionally from a distance, but hardly ever up close.\n  Everything is habit with people, everything, even state and political relations. Habit is the chief motive force.\n  in life that when there are two opposites one must look for truth in the middle\n  There are souls that in their narrowness blame the whole world. But overwhelm such a soul with mercy, give it love, and it will curse what it has done, for there are so many germs of good\n We Were Eight Years in Power - Ta Nehisi-Coates  I knew, even then, that whenever I nodded along in ignorance, I lost an opportunity, betrayed the wonder in me by privileging the appearance of knowing over the work of finding out\n  The world might fall off a cliff, but I did not have to be among those pushing it and more, I did not have to nod along while fools insisted that gravity was debatable\n  It did not occur to me that writing is always some form of interpretation, some form of translating the specificity of one’s roots or expertise or even one’s own mind into language that can be absorbed and assimilated into the consciousness of a broader audience\n  Racism is not merely a simplistic hatred. It is, more often, broad sympathy toward some and broader skepticism toward others.\n  The idea of reparations threatens something much deeper—America’s heritage, history, and standing in the world\n  White people in this country will have quite enough to do in learning how to accept and love themselves and each other, and when they have achieved this—which will not be tomorrow and may very well be never—the Negro problem will no longer exist, for it will no longer be needed\n A Wizard of Earthsea - Ursula K Le Guin The Omnivore\u0026rsquo;s Dilemma - Michael Pollan Washington Black - Esi Edugyan Creativity Inc. - Ed Catmull My Year of Rest and Relaxation - Otessa Moshfegh The Lords of the Realm - John Helyar Their Eyes Were Watching God - Zora Neale Hurston ","permalink":"https://judahgnewman.netlify.com/writing/books/year_review_2019/","tags":["Reflections"],"title":"2019 in Review: The Books I Read"},{"categories":["Books"],"contents":"There Eyes Were Watching God is a novel about Janie, an African American Women born shortly after the end of slavery. The novel follows her she moves through three different husbands and living situations. This book is the most well known piece of literature by Zora Neale Hurston. The book is often read in English classes in either high school or college. I never read There Eyes Were Watching God in school; I was in the other 10th grade english class.\nThe version of the book that I read contained a forward, an introduction and an afterword. While the forward and the introduction probably should have been at the end of the book, it was nice to read scholars thoughts on Hurston\u0026rsquo;s work. It helped me contextualize what I was reading. I was able to better understand the circumstances which created this and why this story is important. I was fascinated by Hurston\u0026rsquo;s interactions with other writers and how other writers felt about her work.\nThe largest theme or lesson that spoke to me from this book was the necessity of learning from experience and living our own life. Hurston does an incredible job conveying Janie\u0026rsquo;s internal monologue and her feelings of want. Janie marries two men that do not leave her happy or satisfied. The first marriage was set up by her grandmother, in an attempt to provide Janie with security. Janie jumps into the second marriage attempting to run away is trying from the first marriage. She wants to find a better situation, leading her to run away with a man shortly after first meeting him.\nThis made me reflect on the importance of dating while you are young. We learn so much from experiences with different partners. Through each partner we will learn more about ourselves and what it means to be in a relationship. These experiences make us smarter and wiser for future relationships. This is a two pronged sword though. We often carry around the baggage and hurt from previous relationships. We also have to learn how to work through trauma / history that we are carrying around with us.\nThis thought highlights the importance of everyone living their own life and learning from their own experience. While it is important to convey wisdom and learn from others, everyone has to learn on their own. Individuals will learn the most from their own experiences, not just being told by others.\nQuotes  Many of us argued that Janie did not have to be a role model at all. She simply had to be a fully realized and complex character, which she was.\n  There are years that ask questions and years that answer.\n  Then there is a depth of thought untouched by words, and deeper still a gulf of formless feelings untouched by thought.\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2019/their_eyes/","tags":["Reflections","Fiction","History"],"title":"Their Eyes Were Watching God"},{"categories":["Books"],"contents":"Lords of the Realm by John Helyar chronicles the history of baseball through the eyes of the owners. We generally learn about the history of sports through the great players and their influence on the game. This book looks at the owners and how they shaped the game of baseball as it developed. The book is particularly focused on the labor relations between the players and the owners.\nThis book showed a completely different side of baseball history than I am used to. I have heard about all the great players, but never learned about how free agency developed. I did not know how farm systems developed or how expansion took place. Before reading this book I rarely gave the owners much thought. I did not consider how they shaped the game and how different sports have different models of free agency.\nWhile reading Lords of the Realm I reflected on how the process of change is slow and how trying to change everything at once can actually be counter productive. It is important to take the long run view of change. Do not set your sights on getting everything accomplished in one go. A really powerful tactic can be to ask yourself where you want to be in a couple of years and then set smaller steps that are necessary to get there.\nI also reflected on the economics of Baseball a fair amount and how there are more general principles we that can be applied to other scenarios. For Baseball teams there is a competitive balance problem. It took a very long time, but the owners recognized the need for wealth redistribution. They recognized that largest markets will do a lot better than teams in the smallest markets. For the good of the entire game it is important that all teams are thriving. To me this sounds a lot like general problems that we have with inequality in society. We can accept that redistribution of wealth should happen for baseball teams, but it is still very difficult to accept at a personal level.\nI am interested in exploring sports contracts and how market dynamics are in play for players contracts in more depth at a later date. The sports market provides a lot of interesting principles that can be studied and then applied more generally. I am specifically interested in some of the more unique principles that different sports have put in to play and the impact they have. I would be interested in exploring the salary cap in Basketball and the franchise tag in Football.\nQuotes  If they were united, there was nothing they could not do. He used the same words over and over: “Together you are irreplaceable.”\n  “The owners themselves were responsible for it working so badly. What happened was that baseball wasn’t governed by supply and demand. The owners wanted to win so badly they didn’t care about breaking even. They always thought, ‘We’re just one player away.’ ”\n  But he did think he had to start doing what Miller had once done so brilliantly: look a few negotiations out, decide where he wanted to go, and try to get there a bit at a time. He probably couldn’t get as much “meaningful\n  “If the game is lost to the economics that drives it, then we’ll lose the humanity that is unique to the game. We all must feel it and live it in our way and be mindful of its vulnerability to abuse.\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2019/lords/","tags":["Economics","History","Non-Fiction"],"title":"Lords of the Realm"},{"categories":["Industry","Research"],"contents":"In the past year there has been a large focus on what is wrong with the fields of Artificial Intelligence and Machine Learning. Coverage has focused on limitations of current deep learning and machine learning systems. Pieces have highlighted the dangers of implementing systems without understanding their implications or who they are going to impact. These pieces have highlighted areas where these fields need to grow. Machine Learning is still a relatively young field that needs to grow and learn. This piece is going to be a personal reflection on my experience in the field and some of the structural problems that I have not heard people talk about before. I will end the piece with some concrete recommendations about how Data Scientists can change their workflow which I believe will lead to better outcomes.\nIf you talk to Data Scientists or ML Engineers you might hear them talk about the different models they are building. They are building models to recommend movies you might like. They are building a new model that will tell you what type of wine you will enjoy. Building a model is a term that has come to be applied every where. But what does it actually mean to build a model? Does it just involve building a system that makes accurate predictions? From my experience in the field, practitioners have defined building a model as creating a system that can accomplish some task on its own. Building a model should be focused on gaining a greater understanding rather than building a system.\n At times during my short lived career as a Data Scientist I have struggled to be engaged with what I am working on. I have struggled to feel like what I am doing matters. It feels like I am going through the motions rather than critically thinking. This is an issue that I brought upon myself rather than something inherent of my jobs. But, it is an issue that is exacerbated by current practices in the field of data science.\nI was / am attracted to the field of Machine Learning because it helps me better understand the nature of intelligence. Studying what makes an intelligent system and how to build intelligence are problems that motivate and inspire me. They are questions that force me to reflect on my own intelligence and life in the world around me. My introduction to the field was doing academic research during undergrad. We were regularly grappling with these questions; What is intelligence and how is it represented? Since I began working in industry my day to day can feel far away from these basic questions.\nMy experience of being a data scientist at a corporation has been quite different from doing academic research. While this was to be expected, and there are parts about this that are important, losing sight of these basic questions can be dangerous. Being a data scientist in industry can be limited to taking a known algorithm and just throwing data at it. It is about achieving a high accuracy rather than understanding mechanisms. I was solely focused on building a system that accomplishes a task by putting data into a random forest and maximizing accuracy. Will including these features make our model more accurate? What if I change some of the hyper parameters? I was focused on improving accuracy and had lost sight of what it meant to build a model.\nBuilding a model should not mean putting data into an existing algorithm and hoping it has high accuracy. Model building should be a process of trying to understand mechanisms and how they drive outcomes. I was re-introduced to this direction through bayesian modeling and specifically the work of Andrew Gelman. Building a model that helps you understand the world is a much more helpful direction than focusing on accuracy.\nA model that helps you understand the world will be more valuable than a model that can accomplish a task. While automating tasks can save money in the short run, current approaches can miss the larger picture. Rarely do we solely care about the outcome of a specific task. What we actually care about is a larger question, which are specified task is only one part of.\nIf we are building a system to predict house prices, our end goal is normally not to just accurately predict house prices. We want to understand what are the factors that drive house prices. Why does one house cost more than another? Why do some houses in different regions cost more? What does differential house prices tell us about human preferences? Can we use this tool to make more affordable housing?\nIf we are building a system to recommend vacation destinations what is our actual end goal? It is not to just predict destinations but provide a meaningful service to customers. We want to make it easier for people to find vacations that they will enjoy. We should not focus on achieving a high prediction accuracy but on understanding why do people enjoy certain vacations? Why do different people enjoy different types of vacations? Why would a vacation to Iceland be enjoyable during one time of the year but terrible during another time of the year?\nOur desired outcome is rarely accomplished by automating a task. What we care about is making decisions and understanding how different decisions lead to different outcomes.\n So why is data science as a field struggling? Why is it focusing on the short term and the narrow rather than a greater understanding. There is a beautiful simplicity in optimizing a metric. Defining success in terms of error/accuracy leads us to forget about the larger picture. Practitioners need a better work flow that keeps the larger systems and questions in mind.\nData science as a field is missing the scientific method. Rather than forming hypothesis and then testing them, data scientists often just dive in head first. We have identified a task we want to solve and we write some code to create an automated solution. The scientific method though, provides a workflow that keeps the larger picture in mind. It forces us to form a theory about what the underlying mechanisms are and then test them out. The scientific method forces us to build a model. Only with this model can we understand the driving forces.\nLets return back to our example of predicting housing prices. Rather than taking a lot of data and putting it into an existing algorithm, lets imagine a different workflow. Without even seeing the data, lets form a hypothesis. We believe that houses in good school districts will cost more than similar houses in worse school districts. Now that we have a hypothesis about what drives housing prices we can test that hypothesis. If we find it to be true, we will have a better understanding of what drives housing prices. Even if it is false, we have gained more information about driving mechanisms. We will now have on record that a certain hypothesis is false and can test out a new one.\nData Science as a field is still young and evolving. We see the promise and potential of automating a task and forget to be systematic thinkers. We forget the larger goals and focus on one metric. There is a metric we can measure, so we optimize it. Using the scientific method forces us to build a model of the world. We have to form hypothesis and then test them out.\n So what would a different workflow look like? Rather than just directly working with data lets begin by forming some hypothesis. We write down these hypothesis so that we can keep track of them. A great way to form hypothesis is by talking to subject matter experts. If we are creating a vacation destination model, lets talk to some travel agents.\nOnce we have our hypothesis lets test them out. After testing a couple of hypothesis, we use those results to form new ones. We will have a record of our different theories and whether or not they were correct. When we begin to automate our task we are no longer focused on optimizing accuracy but rather have an understanding of a system.\nIt is harder to measure how much of a system we understand than accuracy on a defined task. Our goal should be understanding of a system rather than optimizing a certain metric. Model building should be a process of using the scientific method to better understand the driving forces in the world around us.\n ","permalink":"https://judahgnewman.netlify.com/writing/personal/model_building/","tags":["Machine Learning","Reflections"],"title":"Building Models and the Scientific Method"},{"categories":["Personal"],"contents":"Jenny, my wonderful partner in crime and teammate, had her birthday last month. I got her a Nintendo Switch, with the game Overcooked. Overcooked is a cooking simulation game. You are in a kitchen with other chefs and you have to collaborate to make customers dishes. While simple in concept, Overcooked is quite deep in how it forces you to work together. The levels are set up so that the only way you can succeed is by delegating and working together. This aspect of collaborating is so essential that researchers at U.C. Berkeley actually used the game as a model for building agents that can work with humans. Jenny and I have been playing the game together over the past month and it has forced me to reflect on how I work with other people.\nPlaying Overcooked has highlighted some of the ways that I struggle to work with others. During gameplay, I will often just expect the other person to know what I am doing or understand what I am thinking. Rather than communicating what I am trying to do, I just start doing something and then am surprised when the other person does not understand. This is a problem that I also struggle with in contexts outside of a cooking video game. I will expect that the other person is on the same wave length as me, rather than communicating my thoughts.\nAnother tendency I noticed is that I break from the script. We will have decided a division of labor that works, but sometimes I will feel like I can do more, and do something else. My teammate is expecting a certain behavior from me, and so even if I am recognizing an extra opportunity to contribute, if I do not communicate it, our system breaks down. Once again, rather than communicating with my teammates, I am just playing my own game and expecting them to adapt as necessary.\nI also at times struggle to trust my teammates. I do not trust that they are going to do, what in my mind, is the right thing. I am expecting them to make a mistake or preparing for when they do make a mistake. An essential part of being a good teammate is trusting your teammates and putting your faith in them.\n Reflecting on Overcooked gave me a lot of areas that I can grow in all contexts. I believe this is one of the best parts about playing games. While yes, we play games because they are fun and they connect us with others, they can also often be insightful. One of the largest benefits I have gotten out of playing Frisbee is learning more about myself by being on a team. I have learned more about forming a inclusive community and being a good leader. I have learned about creating strong bonds with your teammates and how important it is to become friends with them. Playing frisbee has taught me so much about all aspects of my life, not just the playing sports part.\nWhile I have known for a while how much I have learned from playing frisbee, playing Overcooked made me reflect on how the the places I struggle in a game can often mirror the places I struggle in real life. Now that Overcooked made me see that, I also see that all over the place in Frisbee.\nIn Frisbee, I also struggle with expecting people to do certain things and expecting people to understand what I am doing. When players do something different than I am expecting, I can struggle. I also often try to do too much when I play Frisbee and sometimes do not trust my teammates as much as I should.\nI believe that playing games is an important thing for everyone to do. Whether it is playing a team sport, playing board games, or playing video games. Games give us the opportunity to learn more about ourselves in contrived situations. It is hard to recognize your own weaknesses in your day to day life. It can feel like a falling of you as a person and hard to change. But we often do not feel like that when we fail in games. We know that it is something that we can work on and get better at.\nGames also help us recognize that just as I can get better at Overcooked by explaining why I am cooking rice instead of chopping fish, I can also improve other interactions in my day-to-day life by improving my communication.\n","permalink":"https://judahgnewman.netlify.com/writing/personal/games/","tags":["Reflections","Frisbee"],"title":"Reflections on Playing Games "},{"categories":["Research"],"contents":"This is list of professors who are doing really cool work:\n Cynthia Rudin Andrew Gelman Michael L. Jordan Moritz Hardt Jacob Andreas Rachel Thomas Jeffery Heer Brent Hecht  ","permalink":"https://judahgnewman.netlify.com/writing/research/professors/","tags":null,"title":"Professors"},{"categories":["Books"],"contents":"My Year of Rest and Relaxation by Otessa Moshfegh is quite a trip. The reader is taken inside the mind of a young adult living in New York who is trying to sleep for a entire year. The main character\u0026rsquo;s parents died and she recently left her job. She is using medication to sleep as much as possible. She hopes that after a year of sleeping she will be able to have a new start and be divorced from the darkness of her previous life.\nThe book is dark, biting, and very funny at times. The book reminded me how complicated everyone\u0026rsquo;s internal monologue can be. I am so often wrapped up in my internal monologue that I forget everyone around me is having an equally complicated one at the same time.\nThe book also made me think about how easy it is for people to feel lonely and fall through the cracks post college. Up until college everyone has a strong support system with people all around them. Post college that can change a lot. There is no one checking up on you or looking out for you in the same way.\nQuotes  The bodega coffee was working-class coffee—coffee for doormen and deliverymen and handymen and busboys and housekeepers.\n  my emotions passing like headlights that shine softly through a window, sweep past me, illuminate something vaguely familiar, then fade and leave me in the dark again.\n  But I was terrified. It was lunacy, this idea, that I could sleep myself into a new life. Preposterous. But there\n  She was beautiful, with all her nerves and all her complicated, circuitous feelings and contradictions and fears\n  The notion of my future suddenly snapped into focus: it didn’t exist yet. I was making it, standing there, breathing, fixing the air around my body with stillness, trying to capture something—a thought, I guess—as though such a thing were possible, as though I believed in the delusion described in those paintings—that time could be contained, held captive. I didn’t know what was true.\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2019/r-r/","tags":["Fiction"],"title":"My Year of Rest and Relaxation"},{"categories":["Books"],"contents":"Creativity Inc was a fantastic book looking at the culture of Pixar. It explained the founding of Pixar and how the founders maintain an intense focus on developing a fantastic culture. The founders believe that if they maintain a good culture, their business will thrive. A successful business is not about having a great product plan, but developing a good culture and empowering smart people.\nI appreciated that the book did not prescribe easy solutions but discussed everything with nuance. It recognized that there are tradeoffs between different approaches. It was honest about how problems are difficult and that there will always be problems. It never felt like it was oversimplifying how difficult running a company can be.\nI am excited to return to this book at a later date and see how my thoughts about it have changed. So much of what the book talked about aligned deeply with my personal values. I loved the intense focus on putting people in a situation to succeed and trusting people. That everything else will work itself out, if your core values are good and you follow them.\nWhile this book was about how to create a good company culture, almost everything the book talked about applies to how to live a good life. It is so important to take some of the things that the book talked about and implement them in your own live. There are going to be problems and things are going to be difficult at times. It is not about avoiding problems or avoiding fear, it is about how we respond to them and how we work through them.\nI would highly recommend this book to anyone.\nQuotes  Even though we were conscious that a room’s dynamics are critical to any good discussion, even though we believed that we were constantly on the lookout for problems, our vantage point blinded us to what was right before our eyes.\n  Kids are instinctively there. But a lot of them unlearn it. Or people tell them they can’t or it’s impractical. So yes, kids have to grow up, but maybe there’s a way to suggest that they could be better off if they held onto some of their childish ideas.\n  You need storms. It’s like an ecology. To view lack of conflict as optimum is like saying a sunny day is optimum. A sunny day is when the sun wins out over the rain. There’s no conflict. You have a clear winner. But if every day is sunny and it doesn’t rain, things don’t grow. And if it’s sunny all the time—if, in fact, we don’t ever even have night—all kinds of things don’t happen and the planet dries up. The key is to view conflict as essential, because that’s how we know the best ideas will be tested and survive. You know, it can’t only be sunlight\n  Our view of the past, in fact, is hardly clearer than our view of the future. While we know more about a past event than a future one, our understanding of the factors that shaped it is severely limited.\n  Instead, I prefer to think of data as one way of seeing, one of many tools we can use to look for what’s hidden.\n  Trust doesn’t mean that you trust that someone won’t screw up—it means you trust them even when they do screw up.\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2019/creativity/","tags":["Design","Non-Fiction"],"title":"Creativity Inc."},{"categories":["Books"],"contents":"Washington Black by Esi Edugyan was quite an enjoyable piece of historical fiction. Here are a couple of different themes that personally stuck out to me.\nScience as an equalizer The book played around a couple of different times with the idea that science and viewing the world through a scientific lens is an equalizer. That if you viewed the world through a scientific lens then you would not discriminate against certain people and you would be better able to see the truth. In the end the book did not really take a definitive stance on this. Sometimes it seemed to champion this view but in other ways it actually argued against it.\nYour thoughts on the character Titch relate a great deal to how you think the book answered this question. If you view Titch as someone who took advantage of Washington and was just using him, then the book does not show science to be an equalizer. But if you believe that Titch truly cared about Washington and was not using slaves as an equalizer than science it would seem to be an equalizer.\nTrauma For me, this entire book was about trauma; how trauma continues to affects us after it has happened and how we can begin to move past trauma. Washington had a number of very traumatic events happen to him when he was quite young. The whole book is about him trying to come to terms with this trauma. Washington\u0026rsquo;s approach was trying to seek out the people who were part of that trauma. He feels like he has to find Titch or that he needs to go talk to Kit.\nI think putting the ability to work through trauma on others is a dangerous game. It would be nice if when Washington went and talked to Titch, Titch said things that helped him. But you cannot expect that off other people. While it might be harder, it is such an important skill to learn how to work through trauma on your own.\nNot having to make an argument One of the most meaningful things about fictions and the arts is that each piece does not have to make a coherent argument. When you write an essay you are supposed to make an argument and have a definitive stance. But pieces of fiction can explore topics and show all the gray areas. They do not have to take a definitive stance but rather can affect each reader differently and have ambiguities. Often pieces of fiction can actually have a larger impact than a persuasive essay because of this.\nQuotes  But it is also true that the nature of what happened isn’t fixed; it shifts and warps with the years.\n  In any case, it was then I recognized that my own values—the tenets I hold dear as an Englishman—they are not the only, nor the best, values in existence.\n  Everything is bizarre, and everything has value. Or if not value, at least merits investigation.\n  that destruction was within us, and nothing we could hide from.\n  She wanted to know if anything would be laid to rest, or if we’d continue to drift through the world together, going from place to place until I made her like me, so lacking a foothold anywhere that nowhere felt like home.\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2019/washington_black/","tags":["Reflections","Fiction","History"],"title":"Washington Black"},{"categories":["Industry"],"contents":"Why I am leaving Nielsen Nielsen was my first experience working at a full time job and in a lot of ways it was a good one. I learned an immense amount and became a much better data scientist. I met a lot of very smart people who helped me grow and taught me a lot. There were a lot of frustrations though during my time at Nielsen and I thought it would be worthwhile to expand upon why I am leaving. All of these things together made Nielsen a difficult place to work at times. If I had not had such great managers I would have probably looked to leave Nielsen a lot sooner.\nCompany being aligned with my values The main driving force behind leaving Nielsen is that the company does not align with my values. I am not sure what the company\u0026rsquo;s stated mission is, but from working inside Nielsen, I can tell you that Nielsen helps create an eco-system that gives large corporations the ability to take advantage of individuals. While Nielsen, might say that they are helping consumers express their preferences, that is not accurate. Nielsen is funded by large corporations. Because Nielsen is paid by their clients, in the end their main priority will always be making those clients happy.\nNielsen provides corporations with the information they need to get consumers to buy more of their product. I believe Nielsen has the opportunity to be a truly amazing company. A company that uses it\u0026rsquo;s panels to understand more about the human condition and what drives decision making. Nielsen does have incredible data and that data is very powerful. The power of this data is currently not being unlocked at all. This data is being used to marginally help the revenue of the largest corporations.\nDuring my time at Nielsen, I realized how important it was for me to care about the underlying mission of the place I work at. It is important for me to wake up every day and be excited about the company I am working at and the impact that company is having. All jobs will have really exciting parts and parts that are tedious or boring. When the underlying company mission speaks to you, all of those tasks are much more enjoyable.\nCompany structure and culture Nielsen was a difficult place to work because of how large a company it was and how hard it was to get anything done. Data Scientists at Nielsen sit so far away from the product. During my time at Nielsen there has been a lot of work to fix this, but this is an inherent problem of working at such a large corporation. There is so much that needs to happen for any change to be realized. With a large company it is impossible to make changes quickly. After David Kenny came in as the CEO, I thought he was saying all of the correct things, but did not see that change filtering its way down to my level. I think this was due to a lack of change at the middle management or upper management level. Here is an article from James Whittaker that talks about how important it is to have change in the middle leadership of a company.\nNielsen is stuck with a culture problem where people are not empowered to make a real difference. There are a lot of great people at Nielsen who say all of the correct things, but overall the company is too large with too much going on. For Nielsen to be successful it is necessary for them to shrink down their scope immensely. They need to put their employees in the best situations to succeed and be empowered. Nielsen is much to centralized as an organization.\nI believe that the a company will have the most success when it is organized around decentralized cross functional teams that oversee a project from beginning to end. Currently at Nielsen, different departments over see a project during different parts of it\u0026rsquo;s life cycle. When a certain segment of a project is finished, one team throws it over the wall to another department. It would be a much better structure to have cross functional teams that own a product from beginning to end.\nPeople talk a lot about work-life balance and Nielsen has a great work life balance. Nielsen allows employees to work from home and has very good hours. I believe less focus should be on work-life balance and more focus should be on work-life integration. Your work should give you energy and make you thrive in the other parts of your life. You should not feel the necessity to separate the two and protect one from the other. At Nielsen it does not seem like most people are passionate about their job and that their job is gives them energy. This is why they find work-life balance to be so important. When you are not passionate about your job, you need to ensure a work-life balance. On the other hand when you are excited about your job, your job will give you energy and make all the other parts of your life better. People are not passionate at Nielsen because it is difficult to be passionate about consumer insights.\nWhy I am excited about working at Change From the above it might be pretty clear about why I am so excited to be working at Change. Change is different from Nielsen in all of the aspects I have talked about above. I am extremely passionate about improving american democracy and helping democratic candidates make better decisions using data. Change is a start-up where everyone works together and sits close to the product. All of the people at change are extremely passionate about what they do and care a ton about the company\u0026rsquo;s mission. Change is literally the size of one cross functional team. I could not be more excited about starting this new chapter in my journey.\n","permalink":"https://judahgnewman.netlify.com/writing/industry/nielsen/leaving/","tags":["Reflections"],"title":"Leaving Nielsen"},{"categories":["Books"],"contents":"The Omnivore\u0026rsquo;s Dilemma by Michael Pollan was an absolutely fascinating read. I enjoyed learning more about our food insutry and how the human food chain has developed. The central question of the book was where does our food come from? What seems like such a simple question has become quite complex in our modern society. I learned a lot by watching Michael try to answer this question.\n  While this book dealt explicitly with our food chain, it raises a lot of questions about the complexity of our society today. Food is something that on the surface is simple and essential to our day to day society. It is crazy how certain processes have developed without careful thought about the pros and cons of certain approaches.\n  The book brought up a number of times about how the government subsidizes the production of corn. It argues that this has been very bad for the farming industry and our food chain in general because the demand for food is inelastic. This means that even as the price of food drops, for the most part the demand of food is stable. I am curious for someone to take a careful look at the economics of our food chain and if subsidizing farmers makes economic sense.\n  One other part of the book that was very interesting to me was the discussions about how we can understand our own behavior in the context of evolution and food. Our actions are mostly dependent on the time when we evolved which is when we were hunter and gatherers. Can we better understand ourselves if we think about the hunter/gatherer context and what kind of traits would have been necessary to thrive in that context.\n  Quotes  As a relatively new nation drawn from many different immigrant populations, each with its own culture of food, Americans have never had a single, strong, stable culinary tradition to guide us.\n  Indeed, there is every reason to believe that corn has succeeded in domesticating us.\n  The economics of a family farm are very different than a firm’s: When prices fall, the firm can lay off people, idle factories, and make fewer widgets. Eventually the market finds a new balance between supply and demand. But the demand for food isn’t elastic; people don’t eat more just because food is cheap. And laying off farmers doesn’t help to reduce supply.\n  To the contrary, a healthy sense of all we don’t know—even a sense of mystery—keeps us from reaching for oversimplifications and technological silver bullets.\n  Like fresh air and sunshine, Joel believes transparency is a more powerful disinfectant than any regulation or technology.\n  Is the individual the crucial moral entity in nature as we’ve decided it should be in human society? We simply may require a different set of ethics to guide our dealings with the natural world, one as well suited to the particular needs of plants and animals and habitats (where sentience counts for little) as rights seem to suit us and serve our purposes today.\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2019/omnivore/","tags":["Nature","Non-Fiction"],"title":"The Omnivore's Dilemma"},{"categories":["Talks"],"contents":"I attended ODSC (Open Data Science Conference) West in October 2019. Here are the notes that I took from every session that I attended while I was there.\nDay 1, Tuesday October 29, 2019 [Interpretable Knowledge Discovery Reinforced by Visual Methods] ( https://judahgnewman.netlify.com/writing/talks/odsc/kdd_odsc/ )\n[User-Centric Design for Data Scientists] ( https://judahgnewman.netlify.com/writing/talks/odsc/user_centeric_design_odsc/ )\n[ML Flow: Platform for Complete Machine Learning Life Cycle] ( https://judahgnewman.netlify.com/writing/talks/odsc/mlflow_odsc/ )\nDay 2, Wednesday October 30, 2019 [Implict Deep Learning] ( https://judahgnewman.netlify.com/writing/talks/odsc/implicit_learning_odsc/ )\n[Towards a Blend of Machine Learning and Microeconomics] ( https://judahgnewman.netlify.com/writing/talks/odsc/ml_microecon/ )\n[Building Intelligent Agents That Can Interpret, Generate and Learn from Natural Language] ( https://judahgnewman.netlify.com/writing/talks/odsc/agents_language_odsc/ )\n[Principled Methods for Analyzing Weight Matrices of Modern Production Quality Neural Networks] ( https://judahgnewman.netlify.com/writing/talks/odsc/inspecting_deep_learning_odsc/ )\n[Tackling Climate Change with Machine Learning] ( https://judahgnewman.netlify.com/writing/talks/odsc/climate_change_odsc/ )\nDay 3, Thursday October 31, 2019 [Key Notes] ( https://judahgnewman.netlify.com/writing/talks/odsc/keynotes_odsc/ )\n[AI Neuroscience: Can we understand the neural networks we train?] ( https://judahgnewman.netlify.com/writing/talks/odsc/uber_ai_neuroscience_odsc/ )\n[Trouble shooting Deep Neural Networks] ( https://judahgnewman.netlify.com/writing/talks/odsc/troubleshoot_nn_odsc/ )\n[Declarative Data Visualization with Vega-Lite and Altair] ( https://judahgnewman.netlify.com/writing/talks/odsc/vega_altair_odsc/ )\nDay 4, Friday November 1, 2019 General thoughts What is my point of differentation,\nalso there is so many different things and how do I specilize\nhaving a long career and pursuing my interests\nwhat makes a good talk\n move through slides quickly mostly illustration do not actually need to read anything shorter talk is generally easier or better  ","permalink":"https://judahgnewman.netlify.com/writing/talks/odsc/odsc/","tags":["Conferences"],"title":"ODSC"},{"categories":["Talks"],"contents":"Talk given by Dominik Moritz at Apple/CMU and Kanit \u0026ldquo;Ham\u0026rdquo; Wongsuphasawat at Apple\nMotivation So what is declarative data visualization? As programmers we are used to imperative programming. Imperative programming is specifying how to do a problem. For example you say, put a red circle here and put a blue circle there. It couples the specification with the execution. In declarative programming we only specify what should be done. We would say map x and y to specific position, not how to max x and y. This allows us to separate specification from execution. Now the compiler only handles how to do something, not what to do.\nUsing this approach allows us to focus on the data and the relationships in the data rather than how to visualize something. The speakers than talked through an example with matplotlib. Making a plot is easy in matplotlib, but once we start wanting to add any form of complexity, it gets really tough. Once we start trying to multiple variables and adding legends or titles, the code becomes quite unruly.\nDeclarative Data visualization is an alternative to matplotlib that should make it easier to work with data. Declarative data viz is inspired by the work of leland wilkerson, the grammar of graphics. But what is a grammar? It simply defines primitive building blocks that can be used to make data visualizations.\nData Visualizartion Building Blocks  Data: The input to the visualization Transformations: Different ways to transform the data (eg. filter, aggregation, binning) Mark: A data-representative graphic, (eg. area , point, line, bar) Encoding: A mapping between data and mark properties (eg. color, size) Scale: Functions that map data values to visual values Guides: Axis \u0026amp; legends that help us visualize scale  Using these building blocks the authors have created two libraries Vega-Lite (Javascript) and Altair(Python). These libraries using a grammar of graphics and the above building blocks. These building blocks make it much easier to build powerful and interactive data visualizations. ggplot which many people are already familiar with is a similar style library for R.\nThe rest of the talk was showing examples of vega and altair. Here is a link to web interface for vega-lite and github repo with examples of tutorials. In the tutorials they show how easy it is to do things that are quite hard in matplotlib. The showcase how easy it can be to take an altair visualization and then put it into a webpage. Lastly, they showed how easy it is to layer different plots together.\nMy thoughts Data Visualization in general is a very difficult task. There are so many different use cases and finding the right level of abstraction is difficult. In the talk they bring up how matplotlib has histogram has a level of abstraction. This is nice because it makes it really easy to create a histogram. But the speakers argued that histogram is the wrong layer of abstraction, it is not a fundamental building block. But instead using fundamental building blocks, it might make one type of graph harder, but it makes the entire eco system easier to interact with. It also makes it easier to build more complex visualizations.\nThis is a classic example of identifying the right layer of abstraction. Doing user studies can be really difficult because user\u0026rsquo;s might not even know what is possible. If you were trying to design a new visualization tool, studying users might give you only part of the story. This is why it can be important to try and predict what the user would want to do, rather than just observe.\n","permalink":"https://judahgnewman.netlify.com/writing/talks/odsc/vega_altair_odsc/","tags":["Design"],"title":"Declarative Data Visualization with Vega-Lite and Altair"},{"categories":["Talks"],"contents":"Talk given by Josh Tobin works at Open Ai and Full stack Deep Learning. Talk Resources\nMotivation Troubleshooting neurel nets is hardest part of building a deep model. Even the best practitioners spend a long time trouble shooting. Josh argues that the vast majority of what the best practitioners do to troubleshoot their models can be broken down to a decision tree. During this talk he presents that decision tree. Generally, 80-90% of our time goes into debugging and tuning while only 10-20 percent is deriving math or building implemnations. Troubleshooting is defined as helping your model achieve optimal performance.\nBut why is troubleshooting Deep Learning so hard? Imagine that you are trying to replicate the results from a paper, but for some reason the lose you are achieving is worse than what the paper reports. It is very difficult to tell if you have a bug and where that bug might be. In software engineering bugs are generally quite loud. You will know immediately when something is breaking. In deep learning though, bugs are essentially silent. There are many possible causes of the error. It could be a case of simply having the wrong hyper parameters. Model performance can be very sensitive to hyper parameters.\nStrategy for Deep Learning Troubleshooting Before we delve into the decision tree to follow, there is a certain mindset we need to have in mind when we are troubleshooting Deep Learning. We need to have a pessimistic mindet. It is going to be very difficult to disambiguate errors. We should not assume that things are going to work. By being pessimistic we are going to start as simple as possible, and then slowly ramp up complexity. By starting simple and slowly making changes we are able to understand why is our performance changing.\nStart Simple The first step in starting simple is to choose a simple architecture.\n If our input is images → start with le net then move to res net If our input is sequences → start with a LSTM and then slowly move to transformer If our input is something else → start with a fully connected neural net with one hidden layer, where you go after that is problem dependent  But what about multiple inputs? → For example two images and sentence\n First map each into a lower dimenstional feature space For images conv net and sentence LSTM Then you flatten the outputs and concatenate them Finally send it through a fully connected layer to get an ouput  Some other good principles to starting simple\n  Use sensible defaults\n  Normalize inputs: do not pass raw images, substract the mean and divide by variance\n  Consider simplyfing the problem\n use a subset of you\u0026rsquo;re data set if you have many classes just start with a couple create a simpler synthetic training set    Implement and then Debug The first step is really just getting your model to run. Here is a list of the most common bugs that might break your model at the very beginning.\n Incorrect shapes for tensors: A lot of libraries will silently broadcast tensor without your knowledge Preprocessing your inputs incorrectly Incorrect input to your loss function → softmaxed outputs to a loss that expecits logits Forgetting to set up train mode for the net correctly  toggling training / evaluation with batch normalization depedencies   Numerical instability → inf/Nan  often stems from using an exp, log or div operation. Use off the shelf softmax rather than writing your own    Some general tips\n Use a light weight implementation: You should have the minimum number of possible line of code for v1. Ideally less than 200 lines of code. Each additional line of code is new chance for bugs Use of the shelf components when you can Build complicated data pipelines later in your development process. You should not spend time doing this when you are still debugging. Begin with a dataset that you can load into memory.  Try to see if you can overfit a single batch. This will help catch a very large number of bugs. You should be able to the drive the error of your model to zero on a single batch. If you are unable to do this, it will help you find the source of potential bugs.\nIt is helpful to compare your results to known results. You might have an error if your model is performing worse than the best known results. Ideally you could compare your model with an official model implemntation evaluated on a similar dataset to yours. This would even allow you to compare your code with their code. A less ideal solution would be to compare with an unoffical model implementation on github. But be wary, lots of repos will have bugs in them. Worst case you can compare your results to the results from a paper with no code associated with it. It is also always good to compare with super simple baselines. How does your model compare to the average of outputs, or a linear regression?\nEvaluate model, then decide what to do next A good framework for doing model evaluation is thinking in terms of the bias variance decomposition. This allows you to think about underfitting and overfitting. We can look at the gap between our performance on our training set, our validation set and our test set. The gap between our training set and our validation set is how much our model is overfitting. Test error can be broken down into the following components\n test error = irreducible error + bias + variance + distribution shift + val overfitting\n By breaking error into its parts we can identify what is causing our error and prioritize our improvements. Distribution shift happens when our training or validation data comes from a different distribution than our test set. For example our training data could come from the day time and our test could be from the night time. We can handle this by using two validation sets. One validation set from our training distribution and one from the test data. The difference in our error between these two validation sets is our error due to distribution shift.\nImprove model or data The first place is to address under-fitting. Here are some things to try ranked from what you should try first to what you should try last.\n make your model bigger, wider or deeper reduce regularization do error anlysis choose a different model architecture tune hyper parameters add features  Second you shoud address over-fitting. Once again a list of what you should try from first to last\n add more training data add normalization do data augmentation add regularization do an error analysis choose a different model architecture tune hyper parameters Use early stopping Remove features Reduce Model size  After you have addressed over and underfitting you should move on to distribution shift. One note about over and under fitting, it is possible that combating one will exacerbate the other. You should not just treat them as isolation. You might fix underfitting then do some things to fix overfitting and then need to do some more to fix underfitting. Below is a list for what to try ranked from first to last to address distribution shift.\n do an error analysis: look at specific errors and prioritize places for more specific data collection Synthesize more data rather than collecting it Domain adaptation techniques. This is a new area of research that is not quite advanced enough  So how do we do an error analysis? We should look at our errors on the train-validation set and the test-validation set. We can go through a set of errors and manually classify them into different categories. We can then hopefully understand what is driving our errors and how many come from each category. We want to understand how much error does each category introduce and how costly are the possible solutions to fix it. We can then prioritize, if we want to collect more data.\nIt is important to ocassionaly re-balance your datasets. Switch what is your validation set and every so often check up on the held out test set.\nTuning Hyper Parameters There are so many differeny hyper parameters that tuning them can be quite daunting and difficult. You should first start with learning rate and batch size. These have been shown to be two of the more important hyper parameters. If this does not get you where you want then look at the others. Rather than doing a grid search you should do a coarse-to-fine random search. Begin by looking in a large range and then narrow in on smaller regions where you are getting best performance. You can also consider bayesian hyper-parameter optimization but this can be hard to implement and you should only invest in it as your code base matures.\nConclusion So why is trouble shooting so hard? Because there are many competing sources of error. We have to be pessimistic in our approach. It should be an iterative process where we start as simple as possible and then add complexity. Following the steps that were discussed in this talk should make it easier.\nMy thoughts Wow, this talk had such a good structure. It was broken down into itemized parts that were easily digestiable. This talk presented not just a good way to troubleshoot deep learning but a good framework for problem solving in general. Start simple and slowly add in complexity. Break the problem down into different parts.\nIn his presentation he essentially used topic sentences and transition sentences. At the beginning of every section was an introduction and at the end was a summary. He had one underlying example that he returned to trhough out the talk to ground his higher level theory in a concerete example.\nThis talk and the one before it mafe me reflect that we should have more transparency into the traning process of models. It should be easier to replicate them. People should report common bugs in development and how to solve them.\n","permalink":"https://judahgnewman.netlify.com/writing/talks/odsc/troubleshoot_nn_odsc/","tags":["Deep Learning"],"title":"Trouble shooting Deep Neural Networks"},{"categories":["Talks"],"contents":"Talk given by Jason Yosinski. He works at Uber AI Labs and Recursion Pharmaceuticals\nMotivation We train and use neural netwroks but we have a very minimal grasp on how they actual work. We have created systems that can beat humans players in games and actually create robots that can work. We have made huge advances in creating systems that can do impressive things. Most of the this improve has been driven by increase in computation and increase in amounts of data. Yet, scientific understaning is not connected to progress. When a computer becomes two times faster, that does not mean we understand it twice as much.\nCurrently we are building models that are more powerful than we can understand. Often we build things that are more complex than we can understand. But understanding falling two far behind what we do is dangerous. We need to work hard to keep our understanding closer to our progress.\n Lets look at our understanding of a deep learning system. Alex Net was one of the first breakthrough networks in computer vision. At this point we can define the code for Alex Net in about ten lines of keras code. We define the size of the layers and the Relu between them. But this code is not actually the model itself, it is the skeleton of the model. The model consists of the actual matrices. Our model consists of the 60 million numbers that are in our matrices. It turns out then when we have all of the right numbers in the right places, these matrices can accomplish some pretty incredible things.\nFor the example of Alex Net, we understand the outer most part of the system. The input is image which is a bunch of pixels. The output is a classification of what is in the image. The difficulty comes from understanding the middle of the network. We can begin to understand the network by feeding images through the network and seeing what happens. We can plot the different parts of the network.\nJason developed a visualization toolkit that allows you to visualize how a neurel net is being activated when you feed an image through it. Jason during the talk demoed this tool to help us understand how the neurel net works. He showed how the first layer understand the lowest level of abstraction. So a transition from white to dark or essentially and edge. Then as we go further down the network we get to higher and higher levels of abstraction. SO one layer might have a detector for faces or for text.\nThe very first layer looks for patterns in pixel space. Then the second layer looks for patterns in the previous layer, not the pixel layer. By stacking these layers we slowly see more and more patterns. The first layer sees patters in an 11 x 11 pixel box. The second layer than has a 3 x 3 filter so it can see patterns in a 33 x 33 pixel space. Here is an article that outlies Alexnet\u0026rsquo;s architecture for reference.\nSo now that we understand to some extent what it learns, can we understand why it learns these features. We neved told the system to learn faces or text or edges. We told it to learn how to classify pictures. We can conjecture that it learns these different abstractions because without them, it cannot accomplish the task we want it to.\nImagine a seatbelt. A seatbelt is nothing but a grey stripe. But there are grey stripes everywhere. So it cannout just learn, grey stripe, otherwise it would classify so many things as a seatbelt. It had to learn that a seatbelt is gray stripe beneath a face. It is learning to detect a face, because that is an abstraction that will help it detect many other things. Or imagine a camera lense. If the system called everything that was a black circle a camera lens it would not be succesfuully. It should only label a black circle with text in the middle a camera lens.\nUsing this visualization tool we were able to understand a little about the middle layers of Alex Net. But we only looked at couple of neurons. It would take so long to understand one feature decoder. Understanding all of the neurons would take forver.\n After publishing the paper on all of these findings, Jason was frustrated but entraced. We had improved our understanding of the system to some degree, but there still was so much to learn. He decided to try a different approach. Rather than passing a photo through a network and inspecting the neurons, lets ask a network what it thinks a guerilla is. We could start with a photo of white noice and feed it through the network. We could then slowly update the photo so that the network assigns a high probability of it being a guerilla. So at first it would have no idea what the photo was. Then we would back propogate through to the photo so that it thought it was more a guerilla. If we do this many times eventually the network will think it is a guerilla with very high probability and this should give us some understanding of what the network believes is a guerilla.\nBut when they did this, they did not get a photo that looks anything like a guerilla. Yet the network says with very high probability it is a guerilla. He then wrote a paper about why this approach failed. It is great that in science when something fails it is a paper. In engineering when somethings fails, then you failed.\nThe next approach was to try the same thing but regularize the image space so no pixels are too extreme. Also make it so pixels do not change too quickly. They combined a bunch of regularizers (l1, l2, and spatial smoothness) and started getting better results. He showed examples with a hartebeast and a school bus. For the hartebeeast the networks does not learn hooves, because lots of animals have hooves. It does learn the horns and the shape of the top of the hartebeast are what defines a hartebeast. For a school buss it learns to look for patterns of yellow and windows and wheels.\nKnow we kind of have an understanding of the network is learning. The regularizer allowed us to enfore a prior to keep images in a specific region.\n Rather than using regularization though, we can use GANS. One network that determines whether or a not an image looks real and then another network that says does it look like a specific class. We then are also changing the input from being a specic class to being a caption.\nWhat if we wanted to replace a network with a brain? We could then see how the brain work using a similar process. Unfortunately we cannot do back propogation through a brain.\nBut a group in harvard and Japan did something similar with Monkeys. They showed monkeys a blank image and then saw what neurons fired. They then altered that photo to make it excite those neurons even more. They did this iteratively until eventually they created a photo that looked like a monkey. The monkey was trying to recognize another monkey.\n There must recent research is about trying to to understand what is happening during the training process. When we train an algorithm all we see is that the loss is going down. But what does that actually mean? We want to understand how is the network actually changing.\nThere must recent paper LCA: loss change allocation for neurel network training does exactly that. They examine the matrices over time to see how they are changing during the training process. It provides a rich visuilizations of how each neuron is changing.\nThey take away three things from this visualization.\n Training is very noisy with different neurons going in different directions It seems like some layers actually make the loss worse There seems to be some levels of micro learning that is snychronized with nearby neurons being updated in similar ways.  This visualization gives us much more visibility into what is happening during training.\nMy thoughts Wow this was a really cool talk. I especially loved the iterative process of the research. We are interested in something, lets look at it and try and understand it. Okay, now that we have improved our understanding a little bit what are the next steps.\nThe idea that some of the neurons or layers are actually hurting the model in terms of loss is really odd to me. I am not sure what to make of that. The way Jason explained it was that it was because they were losing out to other layers. If we knew that one layer was hurting the model in terms of loss could we improve it by removing that layer?\nI am interested in how some of these visualizations, both the most recent ones about the training process and the ones used to visualize a trained model, could be used to improve the model training process. Is it possible that by providing these with end-users and them having a better understanding of the model is working we could improve the model building process. COuld we use these tools to better understand errors. If we look at an error and use this tool could we understand why the model is making an error and help make the model more robust?\n","permalink":"https://judahgnewman.netlify.com/writing/talks/odsc/uber_ai_neuroscience_odsc/","tags":["Deep Learning"],"title":"AI Neuroscience: Can we understand the neural networks we train?"},{"categories":["Talks"],"contents":"AI Lifecycle Model Management: Monitoring for Risk, Bias and Fairness Talk given by Sepideh Seifzadeh\nThe first keynote was about how can we as data scientists monitor for risk, bias and fairness. The speaker motivated the tech by saying that AI and data regulation is coming. There is more data being collected, Machine Learning capabilities have increased and the cost of storage is now a lot lower.\nThe speaker highlighted how we do not have defined mechanisms in place to handle when their are problems with AI. When AI makes a mistake who do we blame? Who is liable if a self driving car hurts someone. She then referenced the COMPAS example of Machine Learning having higher false positive rates of a black defendant re-committing a crime.\nThe rest of the talk was focused on what can we do to better handle all of these difficulties. The speaker highlighted a lot of different tools IBM has developed to help with this process.\nMy thoughts Wow, I struggled with this talk a ton. It really felt like a sales pitch and not a talk dissecting the issues. Here are a couple of larger thoughts though.\nThe speaker said regulation for machine learning is coming and that is how she motivated the talk. I struggled with this motivation a lot. For me all of the things she talked about are not issues because there is going to be regulation, but rather because they are fundamental questions about how we use technology. It was kind of hard for me that the motivation was that regulation is coming. I also think using that motivation leads to a very different approach. Rather than getting at the fundamental questions and trying to answer them, we are worried about some body having rules for how we have to act. Would it be good for there to be some regulation or some principles established around all of this? Yes, 100% yes. But focusing on regulation and that us a motivation is fairly troubling to me.\nIt is interesting to see how aggressively IBM is moving into this space. I do not know much about IBM as a company but it seems like they are really trying to market themselves as place that cares about fairness and transparency and helping data scientists achieve that. To me having data scientists that work in accountable and transparent ways is not just about tools but about a shift in the way we think. Tools can obviously make that whole process easier though. It will be interesting to see how this space continues to develop. I personally believe a start up is probably better equipped to make good offerings in this space rather than IBM.\nAI and Security, Lessons, Challenges and Future Directions Talk given by Dawn Song Professor at Berekely and founder of Oasis labs\nThis talk was motivated about all of the advancements we have made with Deep Learning. Deep Learning is now in use in personal assistants and other places all through out our lives. As deep learning has grown as a field so has the security attacks aganist automated systems. The speaker referenced the Mirai botnet attack and how the growth in technology use makes us more susceptible to attackers.\nFrom now on we should always know that there will be bad attackers. We should always assume that there will be someone who wants to abuse technology and use it for their own purposes. As we automate more systems that makes it easier for attackers to have power. When something is not automated, it is harder for an attacker to gain control or have as large of an impact.\nThe speaker highlighted there are two main ways to attack:\n An integrity attack → cause a system to produce incorrect or specific outcomes a confidentiality attack → Get sensitive information  When we build systems, we need to make them secure. The speaker than used the example of self driving cars. There has been research that shows how easy it is to make a self-driving car think a stop sign is actually something different. Dawn showed a video of how you could convince a car that a stop sign is actually a speed limit sign.\nShe then presented another example of tricking a visual question answering system. The input is an image and question and then the system generates an answer. By simply adding noise to the image, the researchers were able to change the answer to the question.\nThese were both examples of integrity attacks. But there has also been research about confidentiality attacks. We need to build systems that maintain different privacy and were users have control over their data.\nDifferential privacy is defined at the following:\n We add one extra point to our data set, for example we add John\u0026rsquo;s data point An algorithm is differentially private if the output of the algorithm is the same with and without John\u0026rsquo;s data point. If John\u0026rsquo;s data point changed the output of the algorithm a lot, then it is not differentially private.  There was a morning paper write up about memorization in NLP systems and how we can extract secrets from training data.\nThere are lots of challenges to deploy algorithms which are differentially private. It can be hard to make these algorithms usable by non-experts and for them to integrate with our existing environments.\nOasis labs, is a company that has been putting together lots of different tools to help companies build secure systems.\nThere are many important questions about security still to answer and it is a large challenge that requires community effort.\nMy thoughts AI and ML is really just a tool. Anyone who talks about how great AI and ML is, is oversimplifying everything. Its a tool that can be used for both good and bad.\nOne big key though, that when something is automated you are giving more power to the system. By giving power to the system you are reducing friction to a bad actor. A bad actor is no able to more easily take control. Tech in general is something that reduces friction and makes it easier to accomplish things. By making it easier to accomplish tasks it also makes it easier for bad actors to inflict more damage.\nThis kind of work about the weaknesses in ML systems is so important. It highlights how these systems are not intelligent but just doing complicated pattern recognition.\nTo me this kind of work should but a large doubt in the mind of a lot of researchers. This work should not only motivate you to think about security, but recognize the limitations of our current approaches. If our current approaches can be fooled so easily, are these even the right approaches. Can we even call these systems intelligent?\nWhile it is important for some people to be focusing on the security of the current systems we develop there should also be other researchers looking at this and saying, WOW → we need a totally different approach.\nGetting Specific About Algorithmic Bias Talk given by Rachel Thomas founding director of the Center for Applied Data Ethics at USF and founder of fast.ai\nRachel began her talk by using the Gender Shades example of image classification doing much worse with african-american faces, specifically african-american women. Furthermore, amazon\u0026rsquo;s facial recognition algorithm matched 28 members ofr congress to mugshots of criminals. The rate of incidence was much higher for congressman of color.\nRachel than talked about how bias or fairness are too abstract concepts. We need a more structure way to think about these things. She calls out the paper called A framework for understanding unintended consequences of machine learning by Harini Suresh and John Guttag.\nThe face recognition example above is something called representation bias. Identifying the specific bias gives us a better way to conceptualize how we deal with bias. This was an example of representation bias because the training sets the algorithm used had more white men in them. It was also an example of evaluation bias. The bench mark data sets that were used to evaluate performance also had more light skinned men.\nNow that we understand what type of bias is present we can do a better job getting rid of that bias. We can create more representative data sets which have more people of color in them.\n The next example she used is the compass algorithm which had a false positive rate twice as high for black defendants. It had a false positive rate of 40 percent predicting that black defendants would re-offend when they did not. It was possible to get the same performance as the compass algorithm using a linear classifier with only three variables. Using more complex algorithms does not always increase performance that much but often decreases transparency and understanding.\nEven though race was not an input into the compass algorithm, the algorithm found it as a latent variable. We have to be careful of this when we use complex algorithms.\nWhen we implement machine learning systems we also have to be very careful about feedback loops. A feedback loop happens when the machine learning system impacts the next round of data that you see. The most intuitive example of this is a recommendation system. The recommendation system predict what content people would want to see and then shows that to them. But then we are using the content that people view to train the recommendation system. The recommendation system is then impacting what content people look at.\nAnother example of bias we have to be careful about is historical bias. Getting more data would not have mitigated the compass algorithm bias. The algorithm was biased because we have historically incarcerated higher numbers of black people. This is a historical and cultural bias that is ingrained in our systems that if we continue to use data naively will continue to be a part of our society. This highlights the necessity of working closely with domain experts. We have to work with the people who intricately know how these systems work. We should also involve the people who will actually be impacted by these systems into the design process.\n The third case study Rachel discussed was online ad delivery. Latanya Sweeney, a professor at Harvard wrote a research paper about how the ads shown when you google certain names are different. When you googled a name associated with african-americans the add asked if you wanted to do a background check and see if they had been arrested. If you googled a more neutral sounding name, even for someone who had actually been arrested, it showed a more neutral ad.\nThe company responded by saying that they were allowing people to post both and then doing A/B testing. They would show the add that gets people to click on it more.\nThis example show cases how we have to be more carefully about what we optimize and the impacts it could have.\n The fourth example Rachel referenced is a paper by Sendhil Mullainathon and Ziad Obermeyer looking at a system that predicted the likelihood of a stroke. The system that was implemented found that the following things where most predictive of if you were you going to have a stroke\n have you previously had a stroke? do you have cardiovascular disease have you had an accidental injury? Do you have a benign breast lump? Have you had a colonoscopy?  The first two make sense as being predictive but why would the last three be predictive? It is because we are not actually measuring if someone has had a stroke. We are measuring if you had a stroke and then went to the hospital and reported. The last three predictive variables are measures of how much you use healthcare. This is an example of measurement bias in action.\n We see historical bias all throughout society. Here are just three examples that are related to racial historical bias:\n When doctors are shown identical files for black and white patients they are less likely to give helpful recommendations to black patients If you apply to an apartment on craigslist with a name that is thought to be associated with being an african-american you are less likely to get a response When bargaining for a car, black people are offered higher prices on average  This might lead you to ask why does algorithmic bias matter, humans are biased too. Algorithmic bias is even more important than human bias because machine learning amplifies bias. We use machine learning differently than how humans are used. Machine learning is used at scale to do many things at once. It is much easier to appeal to a human being than to appeal to an algorithm. Also people are more likely to assume algorithms are fair. We recognize that humans are subjective, but we are less likely to see that in an automated system.\n So know that we understand a lot more structurally about bias and fairness, how can we work towards a solution.\nOne good place to start is to analyze a project and find all the different biases and how they could be in that project. There are lots of different frameworks to try and do this but Rachel provided some questions she thinks are important to ask.\n Should we even build this? Sometimes the answer is just not to build something What bias are there in the data? All data is biased and it is important to recognize that Can the code and data be audited? Is it open source? What are the error rates for different subgroups? How accurate would a very simple alternative be? How do we handle appeals or mistakes? How diverse is the team that built it?  My thoughts This was one of the best talks I have ever seen. Rachel Thomas is such a good speaker and she created such a great talk. She did such a good job of having examples and depth but also driving home high level concepts. She also made the talk understandable to everyone.\nI want to in the next coming weeks do what she suggested and examine a project and look at all the potential bias or fairness issues.\n","permalink":"https://judahgnewman.netlify.com/writing/talks/odsc/keynotes_odsc/","tags":["Machine Learning"],"title":"ODSC Keynotes"},{"categories":["Books"],"contents":"I read this book because of a recommendation from my friend Hawkins. I had previously read a couple of other books by Ursula Le Guin. I absolutely loved both Left Hand of Darkness and The Dispossessed. Both were sci-fi books that were actually commentaries on human society. I cannot get enough of books like that. Hawk had mentioned that this book was about a communist society and what that would look like. I did not really see that at all in the first book but maybe that happens later in the series.\nThis book read much more like a young adult novel than the other books I had read by Le Guin. That is because this book was written as a young adult novel. I wonder if knowing that before I read the book would have changed my perspective.\nThe book was quite enjoyable to read though. It reminded me of reading Harry Potter and my childhood a lot. I was frustrated through out the book about how simple the characters seemed and how rudimentary their motivations were. There can be a lot of power though in simple characters. This book spoke to me in terms of self discovery and how we change over time. It also showed how traumatic events can really shape a person and their development.\nLe Guin wrote an afterward that accompanied later versions of the book. The afterword explained how the novel was created. She explained how in a lot of ways this book broke the traditional trends of the literature without anyone knowing it. She explained that the main characters in the novel had dark skin and people did not even realize that. By writing a simple young adult novel she was able to hide some powerful things. This book might have set up the ways she later wrote very harsh critiques of society but disguised them as sci-fi books. The afterword really changed the way I thought about the book.\nThere are quite a number of sci-fi series where I only read the first book and did not continue with the series (The Fifth Season, The Three Body Problem). I will hopefully continue to read this series because I do really love Le Guin\u0026rsquo;s writing.\nQuotes  \u0026ldquo;To light a candle is to cast a shadow\u0026rdquo;\n  \u0026ldquo;And the truth is that as a man’s real power grows and his knowledge widens, ever the way he can follow grows narrower: until at last he chooses nothing, but does only and wholly what he must do\u0026rdquo;\n  \u0026ldquo;He has to find out what it means to be himself. That requires not a war but a search and a discovery.\u0026rdquo;\n ","permalink":"https://judahgnewman.netlify.com/writing/books/2019/wizard_from_earth_sea/","tags":["Science Fiction"],"title":"A Wizard from Earthsea"},{"categories":["Talks"],"contents":"Talk given by David Rolnick University of Pennsylvania and founder of Climate Change AI\nMotivation Climate change as an important issues does not need that much motivation. We have all seen what has been happening California recently with the wild fires. There has been increasingly severe impacts from human influence on our climate system.\nOne thing that is important to recognize about climate change is that different parts of the world are impacted differently. When we talk about climate change in aggregate we often lose sight of how hard it is going to hit certain parts of the world.\nClimate change is part of a large system. Because it is part of a large system it is not just going to get worse linearly. There will actually be feedback loops and it will start getting worse at a quicker rate. The warmer it gets, the faster it will get warmer.\nSo what does it mean to tackle climate change? It is not like there is an on/off switch. People often ask the question is it too late. That is a silly question because of course it is too late. We can already see the impact. We do get to decide though how bad it gets.\nThere are two separate approaches to fighting climate change. There is mitigation which means reducing the affect that we are having. The other approach is adaption which means increasing our resiliency to changing conditions.\nBut how does machine learning relate? Machine learning allows us to take advantage of large data sets to do better planning and make our systems have less of an impact on the climate system. Machine Learning is really just a tool. It can be used to make climate change worse, or it can be used to make climate change better.\nThey have founded Climate Change AI which is a group of volunteers from academia and industry trying to facilitate applications of machine learning to address climate change. The following paper is essentially their manifesto.\nThe rest of the talk mostly just covers the paper which I plan to read so look for my write up on the paper at some point.\nBelow are some example opportunities of place where ML can be applied and then how to get involved.\nExample Opportunuities We can apply Machine Learning systems to improve our electrical systems. Our electrical systems are very complicated and involve systems generating electricity, system transmitting electricity and then systems using electricity.\nOne way to improve our electrical systems is if we can better forecast supply and demand. We are currently really bad at storing energy in batteries. Forecasts of how much electricity we are going to supply and how much electricity is going to be demanded would be very helpful.\nTransportation is another sector where we could us ML. We could use ML to help with freight routing and consolidation. We could use ML to reduce freight emissions by optimizing bundling and shipment. There are also many problems where ML can be used related to electric vehicles.\nIn the talk he mentioned lots of other examples. I decided not to write them all out here.\nYou can get involved through their website.\nMy Thoughts I was kind of disappointed by this talk. I am not sure what I expected out of it, but it was essentially just a laundry list of all the different ways ML can help with Climate Change. I would love to work on combating climate change but it is daunting to just start a problem from complete scratch. The website and project is clearly new so I am excited to see how it develops and how the community forms.\n","permalink":"https://judahgnewman.netlify.com/writing/talks/odsc/climate_change_odsc/","tags":["Machine Learning","Nature"],"title":"Tackling Climate Change with Machine Learning"},{"categories":["Talks"],"contents":"Talk given by Michael Mahoney and Charles H. Martin\nTalk Slides\nGithub\nMotivation What most practitioners of deep learning do is train models. Training models that do something is the easy part. The hard part is actually evaluating and testing those models. Most people evaluate their models by tieing them to a certain set of data and seeing the accuracy on that data. This has problem because it is tied to a given set of data and the potential problems with that data.\nThe speakers explain a new approach of directly inspecting the weight matrices in deep learning algorithms. Looking directly at this weight matrices could also help with problems of reproducibility. We can analyze these weight matrices to understand what makes a model start of the art and what makes a model generalize well.\nIdeally we could develop some sort of theory about these weight matrices that we could use to guide us. We also want to be able to use that theory to help in practice with creating these deep learning models.\nMethods The speakers demonstrate their approach by examining the weights in state of the art pre-trained models that have been released to the public. We can look at how the weight matrices have changed over time.\nthe speakers argue for looking at the singular value density for the weight matrices of the matrices. We can examine these histograms of the singular values using Random Matrix Theory (RMT). At the beginning of training we should see a certain pattern that corresponds to a random matrix. As the model gets trained though we see a different pattern emerge.\nWe see the entropy of this matrix go down over time. This makes sense because maximum entropy means we have a completely random matrix. As the entropy decrease this corresponds to a move away from random structure.\nThe speakers than examined different models and saw how models changed to develop a theory of what made a good model. What can we see in VGG 19 that makes it better than VGG 11?\nDeep learning systems that display longer tails in this singular value histogram also known as empirical spectral density have better performance and are better able to generalize. When we have alphas that are closer to 2 the better the model performs.\nAlpha is a parameter value in the Marchenko-Pastur bulk decay which is a formula for long tailed random matrices. The speakers have created a tool that examines the singular values and gives a heuristic using this alpka to get a sense if the model is performing well.\nThe speakers than presented a graph with test accuracy plotted correlated with the alphas and they show the lower the alpha the better the test accuracy.\nMy Thoughts This talk was really cool and raised a lot of interesting ideas. My notes do not do it justice. I highly recommend checking it out.\nOne of the things that was cool is both of the speakers come from a none deep learning background. They took what they learned from other backgrounds and applied it here. Having people from other fields and having them apply their knowledge is so helpful. Physics is essentially everything tbh.\n","permalink":"https://judahgnewman.netlify.com/writing/talks/odsc/inspecting_deep_learning_odsc/","tags":["Deep Learning"],"title":"Principled Methods for Analyzing Weight Matrices of Modern Production Quality Neural Networks"},{"categories":["Talks"],"contents":"Talk given by Jacob Andreas\nMotivation Ideally we have systems that understand us in our natural language and can do things. We are beginning to have autonomous agents in the world around us. Both agents in the virtual world such a personal assistants and agents in the physical world like Roombas. We want to build automated agents that can take actions based on natural language\nHow do we design agents? We can break down the agent design problem in the following way. We have a context which consists of our environment and our available actions. And then we have our data which consists of our instructions and supervision.\nSo imagine we want to teach an agent to navigate our house. The environment would be where in the house is the agent and what do they see nearby. The available actions would be the movements the agent could make. The instructions would be a certain task we wanted to agent to accomplish. For example, \u0026ldquo;Go sit on the sofa\u0026rdquo;. Lastly, the supervision would be whether or not a series of actions accomplishes the instructions.\nEvery time an agent takes an action it changes the state. Each action is a transition from one state to another state. The language of the instructions corresponds to a series of actions. It is essentially a machine translation problem of translating instructions into a set of actions.\nDoes this approach work? We can get fairly far with this approach. One problem with it though is that it treats each state as independent. But generally we will actually need to understand the previous state.\nWe might have to enrich the state space so that it includes something about completion of certain parts of the instruction. We can do this by adding a state space which corresponds an action to a given part of the instruction. Essentially attention in this machine translation problem. This only works though when the instructions are a series of actions.\nWhat if the instructions were instead a goal?\nConstraints approach Another approach is that we read the instructions and make constraints of our final state. To do this we take advantage of linguistic structure. Lets look at an example.\n\u0026ldquo;Sit on the chair next to the bed\u0026rdquo;\nWhat we would do is parse this text into a set of constraints for the final space. The constraints could be:\n Be sitting Be on a chair Be next to the bed  We then just have to get a series of actions that brings us to a final state that meets those constraints.\nBut how do we get a language of constraints that is powerful enough to represent everything? How do we know what being sitting is or being next to a chair is?\nHow can we get all the benefits of the constraint approach with needing all of these pre learned things?\nWe can change the approach to rather than predicting individual actions we predict the full action set. We have no intermediate logical structure but rather text to a full action set.\nWhat else can we do with these same tools? Rather than just training agents we can also do instruction generation. We can take a set of actions and map it to language. We can imagine this is really helpful for navigational guidance.\nMy Thoughts This talk had a really nice flow and was incredibly well motivated. it started out simple and then built up in complexity. It followed a logical thinking pattern and told a story.\nThis talk seemed to follow a similar recent trend of adding more structure to models. Adding more structure to models can actually help a lot. What can be difficult is how do we pick what structure to add, what level of abstractions do we use?\nOne thing I was thinking about during this talk was how we often break problems down into smaller problems to solve them. This is often necessary. It is almost impossible to think about really complex problems. But I was playing around with the idea of how using proxy problems can bring us down the wrong rabbit hole.\nFor example imagine we want to build an intelligent system. So far our approach has been to build very narrowly intelligent system. The hope is by starting here and doing this well, we will eventually learn how to do the harder thing. Is it possible though, that by optimizing the simpler thing, we actually are not taking the best approaches to solve the larger problem.\nIn the end I would argue yes that is possible, but what else can we really do? We need to have intermediate approaches. We just should keep in mind the larger issue we are actually trying to solve and understand we are using a proxy which might not generalize to the larger issue.\n","permalink":"https://judahgnewman.netlify.com/writing/talks/odsc/agents_language_odsc/","tags":["Reinforcement Learning"],"title":"Building Intelligent Agents That Can Interpret, Generate and Learn from Natural Language"},{"categories":["Talks"],"contents":"Talk given by Michael I Jordan\nWhat is Machine Learning? The talk started by Professor Jordan defining what Machine Learning is. He said that machine learning is not some new field of research but it is rather the engineering extension of statistics. He calls back to how there was chemistry for a long time before there was chemical engineering. There was physics and electrical theory before there was electrical engineering.\nMachine Learning is a new branch of engineering and we are still very early on in it\u0026rsquo;s development. There are many issues that we still need to answer. How do we think about robustness and fairness of systems?\nThere was been an insane amount of hype about Machine Learning but it is still in its infancy. Essentially all of Machine Learning is still just pattern recognition. Deep learning is pattern recognition on steroids. But pattern recognition is not good enough to make decisions. We often need more the pattern recognition to make decisions\nWhere is Machine Learning Going? Currently Machine Learning has really been limited to individual systems solving individual tasks. Where it becomes exciting is when we start to think about complex systems interacting. A large interconnected web of data, agents and decisions. We want our overall system to be intelligent.\nAn example of an intelligent system is a market. A market is a large decentralized system that organizes resources. Markets are scalable, robust, adaptive and have a very long lifetime.\nAn example of a market is Uber. It is a platform that brings together consumers and producers. There is a lot of Machine Learning that makes the Uber platform possible. We could imagine a similar platform but for many other use cases as well. Medicine, Commerce etc..\nImagine if we thought of Youtube as a connection between producers and consumers. That would completely change how Youtube was monetized and how we interacted with the platform. We have seen that people are willing to give up a little bit of privacy for value. It is essentially a market where agents interact with each other by recommendation systems. We can then use Microeconomics in conjecture with Machine Learning to think about how these systems function.\nML and Decision Making In Machine Learning decision marking is often simplified to just a threshold. If our value is larger than some threshold we make one decision and if it is smaller we make another decision. But real world decisions have consequences. Also, imagine we are not thinking about just one decision put a set of decisions? Decisions over time? Decisions where there is scarcity?\nTo ground this in an example imagine we are recommending movies to people. What happens if we recommend the same movie to every individual. No problem. But what about if we recommend the same restaurant? or the same street to drive on. Then we have a series problem. Instead what we can do is create a market. We can connect producers and consumers.\nProfessor Jordan is arguing that markets are the future because with a market you are providing a real service. This is a sustainable business model rather than the advertising business models we currently have where organizations are harvesting people\u0026rsquo;s data. They key comes from the connection between the digital world and the real world.\nCurrently there is not any economic value in the music industry. It is is very difficult for most people to make a living being a musician. Instead how could we create a market for music? We can provide data to musicians to let them learn where their audience is. A Musician can give a show where they have an audience. There is actual already a company doing this: united masters\nMy thoughts I have a lot of mixed feelings about this talk. When I initially heard it I loved it. But when I was writing up my notes, I was not loving it as much. The talk was given in a short amount of time and did not go into much of the actual research around this motivation. I love the idea of machine learning not being some new field but rather just an engineering discipline.\nI am not sure I agree with the whole argument that most digital systems right now are not creating value and that it showed be a market. Spotify is creating value not by giving you a place to listen to music, but by making it so so easy. It also has a lot of services such as recommendation that are really nice. Do I wish that more of the value in the eco system went to the people actually producing the music than Spotify? For sure. But this is the whole shift that the internet has brought about. Suppliers are not the people who are in control anymore. They are a commodity.\nI think there is a very careful discussion that needs to be fleshed here about how the digital world intersects with the physical world. How do these two worlds interact and how does value in one, translate to value in another? Michael Jordan was arguing that Uber\u0026rsquo;s business model is a good one, but the problem is that Uber\u0026rsquo;s business model does not have the same return on investment as a software as a service company. Software as a service companies have essentially zero marginal cost. Uber has quite a high marginal cost of every transaction. Uber is also constantly worried about bringing suppliers and demanders (drivers and users) onto the platform. So far we have not seen that Uber\u0026rsquo;s business model is successful.\nSo while I do agree that complex systems are where things are headed with many different interactions. I am not sure I agree that markets are a better business proposition than other approaches.\n","permalink":"https://judahgnewman.netlify.com/writing/talks/odsc/ml_microecon/","tags":["Economics","Machine Learning"],"title":"Towards a Blend of Machine Learning and Microeconomics"},{"categories":["Talks"],"contents":"Talk given by Laurent El Ghaoui : Professor at BAIR and chief scientist at Sumup Analytics\nThis talk was about a new area of Deep Learning research that is still very much theory. In this approach rather than specifying the architecture of a given model, the data tells us the structure of our model. This approach provides for new notation and new conceptual ways for us to think about deep learning. This approach is notationally much simpler and having this abstraction in notation allows us to do a lot of more powerful things.\nmy thoughts I Struggled to really understand what was going on in this talk. I also left a little bit early to attend another talk. I find it really difficult to wrap my head around theory if it is not motivated really well and given examples. Otherwise I need to work through the theory on my own. Concrete examples are just so helpful to ground what you are talking about.\n","permalink":"https://judahgnewman.netlify.com/writing/talks/odsc/implicit_learning_odsc/","tags":["Deep Learning"],"title":"Implict Deep Learning "},{"categories":["Talks"],"contents":"Talk given by Jules Damji: git hub repo\nMotivation Machine learning development is complex. This is not due to the underlying theory but rather all of the different stages in development. Each stage has its own requirements and goals. We have developed tools and best practices for this for traditional software development, but not Machine Learning.\nHere are the differences between traditional software and machine learning\nTraditional Software:\n Meet a functional specification Quality depends on the code One software stack  Machine Learning\n Optimize a metric (accuracy) Constantly experiment to improve it Quality depends on input data and parameters Compare and combine many libraries models \u0026amp; algorithms  What is ML development ML Development follows the below life cycle:\nRaw Data \u0026ndash;\u0026gt; Data Prep \u0026ndash;\u0026gt; Training \u0026ndash;\u0026gt; Deploy, Repeat\nEach of these stages has its own tools and its own work environment. This difficulty manifests itself in different ways.\n Tracking Parameters: If you just want to track one thing this is easy. But as the number of things we want to track grows this problem explodes. Scale: Often the scale of the problem we are working on is very large and we need something that is still efficient and quick Model Exchange: How do you give a model that you developed to someone else and make sure they have all the necessary information? Governance: How do we track the a model is doing what we want it to and that it is being used properly?  Large industry leading companies have built custom machine learning problems to solve these problems:\n Facebook: FBLearner Uber: Michelangelo AirBnb: BigHead Google: TFX  Introduction to Mlflow But these platforms are tied to companies infrastructure and limited to a subset of algorithms. If you do not work at one of these companies you are out of luck.\nMlflow is a tool built by DataBricks designed to provide similar benefits in an open manner that anyone can use. Mlflow was designed with the following design principles:\n Modular Design: You do not have to use all of it and you are not locked in. It is made up of distinct components and you can use what you want. Design is API first  APIs are very intuitive to developers and allow them to construct immensely powerful things.    Components of Mlflow There are currently four different components of Mlflow. They are listed out below with details about each one of them.\n1. Mlflow Tracking A simple API that allows the developer to track different things during their ML development.It provides both an API for your code and UI to visualize what you are tracking. It allows you to track all of the following things\n parameters: inputs to your code metrics: measurements of performance tags and notes: extra information artifacts: files, data and models source: what code ran? version  2. Mlflow Projects Mlflow projects provides a packing format that makes it reproducible to run a model on any platform. It is essentially a directory that has code, configuration, dependencies and data. We can run our models either remotely or locally. We use a yaml file to specify all of the necessary information.\n3. Mlflow Models Mlflow models is a format the supports diverse deployment tools. It creates an abstraction in between model code and deployment tools that can be translated. It allows us to specify a model that then can be deployed in a lot of different ways.\n4. Mlflow Registry A repository of named, versioned models with tags and comments. This tool allows us to tag different versions of a model and see how a model changes over time. We can track a model through development, staging, production and even retirement. We can easily load a specific version of a model and look at model lineage.\nMy Thoughts First, I love all of this work so much. Mlflow is such a cool tool. I am all behind it. I have not had much experience using it yet but I cannot wait to start using it regularly. I imagine it is going to become an integral part of all of my developments moving forward.\nI specifically love how they engage with the open source community and want to get feedback. Jules was constantly asking for feedback and wanting people to contribute to the project. I love building things that are open and transparent. When he gives these talks he is making the tool known to more people and getting feedback at the same time.\nThere were a couple parts of his talk I did kind of disagree with though. Specifically I sort of disagreed with the whole motivation section and setting up data science as very different from traditional software development. I believe the two are actually a lot more similar then we often say and are doing a disservice by treating them as such distinct things. Software engineering is a much more developed field and we can take a lot of the learnings from it and apply it to data science or machine learning development. I might have to write about this in more length at some point.\nOne thing that I really want to see is a tool that tracks the exploratory part of the data science life cycle. Ml flow seems to be a tool that is designed only once you have reached a model. But to me that most important part of the life cycle happens before that. How did you decided on the model that you did? What did you find that was interesting in the data? What are the assumptions that you made? In the end tuning parameters or metrics do not help you answer the most important questions.\nI talked a little bit about this with Jules after the talk. He suggested having a notebook that tracks this and making it an artifact. He also mentioned using tags and notes. I will be interested to try this out with my next project.\nIn general, I struggle with hands on tutorials. I find they rarely give you enough time to truly learn on your own. It either should give me like a full half hour to work on implementing something, or should not really have me write any code.\n","permalink":"https://judahgnewman.netlify.com/writing/talks/odsc/mlflow_odsc/","tags":["Machine Learning"],"title":"ML Flow: Platform for Complete Machine Learning Life Cycle"},{"categories":["Talks"],"contents":"By Annie Darmofal and Katie Malone\nThey both work at Tempus\nMotivation Even if we build the most incredible thing, if it is not designed with users in mind, no body will use it. If we have built something that nobody will use, is it even incredible. If nobody is using what we have built than that is a failing.\nWe can build tools that users actually care about and will use if we take a user-centered design approach.\nWhat is User-Centered Design? User centered design is a frame work for problem solving where users are the vocal point. Users are at the center of everything. We explicitly acknowledge that are users are human which introduces a lot of new social and emotional factors. We care about how our tools make users feel.\nUser centered define can be broken down into the following five step iterative process.\n Empathize - We begin in an exploratory phase that is very open where we are just trying to understand Define - We synthesize the results of empathizing into more concrete segments of work. We still keep users at the center of everything by using user stories. We are essentially making technical requirements from a users perspective. Ideate - We bring users into the ideation phase having them help us brainstorm how to solve the problem. Prototype - We want to get our solution back in front of the user as quickly as possible so that we can get their feedback. Test - Iterate, Iterate Iterate. We testing something out, see how the user responds and then try something new out.  In the end the strict process or framework is not that important. It is all about having the correct mindset.\nPrinciples of User-Center Design   Understand your users, tasks and environments\n It is important to recognize you are unique and that other people are different from you.Work to identify how they are different and how they are similar. Understand the environments, that your users work in. What are there constraints?    Keep users involved throughout the process\n It is unrealistic for you to have solved the entire problem up front. Constantly get feedback from your users as you go. This makes it much less likely you do a lot of work that you did not need to do. you do not need all of the answers up front    Consider the whole user experience\n It is critical to zoom out. Do not just think about the individual task but identify the larger picture. Ask your users what marks success in their job? This question is so important, because it gives us the context for what we are trying to achieve We want to understand the entire workflow rather than a specific solution    Evaluate\n We need a way to measure if what we are doing are is successful. Here are some heuristics for usability taken from Jakob Nielsen\u0026rsquo;s paper 10 usability heuristics for user interface design   Visibility of system status  do users get feedback on their progress? how much longer do they need to wait? This is critical to building trust in the user   Have Consistency and Standards  example is a search bar   Error Prevention  try to prevent user errors from the start. If you know a system has constraints build them in   Aesthetic and Minimalist Design  you should not be able to take anything else away and still solves the problem Ask yourself does what you are adding help guide the user or distract them?   Help users recognize, diagnose and recover from errors  explain to user how to fix their error      Iteration\n You should always be learning and striving to make a better product    my thoughts I love so many of the tenants and focus of this talk. User centric design is such a key to making products that actually have an impact. I believe academia would benefit so much from taking this approach. So much of academia is not user friendly and almost purposely inaccessible. I know there is a section of academia that focus on study design techniques, but really user centered design should under-pin everything.\nThis talk seemed mostly to be about software design. Or their approaches just seemed to be about design in general. I was hoping they would talk specifically about what this means for a data scientist. One thing I was thinking is that you could apply this by saying data scientists have to use algorithms that are interpretable. If the algorithm is not interpretable good change the user will not understand it and it will not be accessible. A user centered design approach would mean making models that are transparent and anyone can understand and interact with.\n","permalink":"https://judahgnewman.netlify.com/writing/talks/odsc/user_centeric_design_odsc/","tags":["Design"],"title":"User-Centric Design for Data Scientists"},{"categories":["Talks"],"contents":"Talk given by Boris Kovalerchuk , Slides\nMotivation Boris motivated the need for visual techniques with an example on the Iris data set. He showed three different models for the Iris data set, shown below.\nHe highlighted that the problem with each of these analytical approaches is that they generalize to areas we have seen before. Ideally, instead we would have a model that classified points in space we had seen before and refused to classify points in the unknown space.\nBut how do we know which spaces we have seen and which we have not? This can be done easily visually in 2 dimensions, but what about in higher dimensions.\nVisualization is important because it allows us to answer questions in ways we cannot analytically. It also is easier to communicate and identify patterns. Good visualizations communicate information quickly and efficiently.\nBut visualizing data from high dimensions is very difficult. Anything larger than 3-dimensions is almost immediately intractable. How do we visualize high dimension data without loss of information?\nTalk Outline The goal of the talk was to highlight how can we use visualizations to aid ML\nBoris identified four different ways visualization can be used in conjunction with Machine Learning.\n Visualize ML Models Discover ML models using visual means Explain models using visualization Combining visual and analytical approaches for discovery  Overall, Visual Knowledge discovery allows us to make less of a trade off between interpretability and accuracy\nVisualize a trained model Boris provided a couple different examples of how to visualize a model once it is already trained. He used the examples of how to visualize an association rules model and Tensor Board.\nHe argued that visualization can give models some explanability, but not the necessary depth of understanding. For example, a Decision Tree visualization can tell us what cutoffs a model is using. But it does not explain to user why it is using those cut offs.\nBy providing this proxy it is actually doing us a disservice. We believe we know why it it is outputting certain results, but we do not actually know.\nBoris argues we should use analytical approaches from the beginning\nUsing A Visual Approach from the start Using visualization as an approach is very intuitive to people in low dimensional spaces. If we had 2-d data we would almost always create a scatter plot of it. But in higher dimensions because visualizations are harder people move away from this approach. We need visualizations that work in higher dimensions.\nA common approach that people using for visualization high dimensional data is PCA (principal component analysis). PCA projects high dimensional data into 2 dimensions. But PCA is problematic because it is not interpretable. When we look at the data after PCA the coordinate plane it is on does not mean anything to us.\nBoris then referenced the Johnson Lindenstrauss Lemma which provides a mathematical formulation on how hard it is to map high dimensional spaces to low dimensional spaces. When we use PCA we will lose a lot of information.\nImagine you have a 10 dimensional cube. There will be 1024 points in this cube. When you project this cube to 2 dimensions it will get projected into only four points. This means 256 points that used to be in different parts in the space will now be identical. By projecting this high dimensional space we have lost a lot of potential information.\nThis means there is potentially a big problem using PCA or t-SNE to find outliers. It might work, but it is also possibly then when we project the data down we are losing lots of information. We might have convinced ourselves that we have found all the outliers using this approach, when really we have not. What is a dense area in 2 dimensions might actually be very spread out in N dimensions.\nThere are approaches to visualizing high dimensional data in ways that we want understand. If we want to visualize a point in seven dimensions we will have to use a graph in 2 dimensions. One example of this is the parallel coordinate plot\nIn reality we should combine the two approaches. We should use PCA and the lossey approach, but recognize that it has its weaknesses. We should combine it with other visualization approaches that do not have the same weaknesses.\nMy Thoughts I left Boris\u0026rsquo;s talk about half way through because I had to talk some calls for work. He did a good job motivating why visualization is important and why some of our current approaches are not sufficient.\nI would have really appreciated a delicate balance of highlighting the benefits of certain approaches and how they can be used in conjecture. That might not have been the focus of Boris\u0026rsquo; talk, but I believe that will always be the key. ANy approach will have weaknesses and blind spots. Its about recognizing those weaknesses and using other approaches in conjecture.\nGiving a four hour talk is just so tough. I think four hour talks almost never make sense unless a good portion of it is just personal work. Maintaining focus for that long of a time and learning new things that whole time is just unsustainable\n","permalink":"https://judahgnewman.netlify.com/writing/talks/odsc/kdd_odsc/","tags":["Design"],"title":"Interpretable Knowledge Discovery Reinforced by Visual Methods"},{"categories":["Personal"],"contents":"My friend Hawkins recently sent me the following question:\n How do you work around biased data sets to produce non discriminatory results in ML?\n I ended up writing him a fair amount while I wan on a plane. My response to him is replicated here in full.\n Hey Hawk,\nThanks for asking this question. It is a really important one and it is always nice to think about these problems in a fair amount of depth. While I would not say there is a silver bullet for solving this problem I will try and talk through a couple of different approaches and some examples.\nIn the end we are always somewhat constrained by our data. It is kind of a cliche but the saying \u0026ldquo;garbage in, garbage out\u0026rdquo; rings very true. If your data is garbage then your model is going to be garbage. While that is kind of an extreme statement it applies in smaller places to. If our data is biased in some ways a model fit on that data is going to be biased.\nThis is a very large concern because as we start automating different processes with Machine Learning or Artificial Intelligence we end up propagating that bias. Lets think of this with an example.\nImagine we built a machine learning model to identify photos of wolves and dogs. We can think of this as a pretty simple task: wolves and dogs have some serious differences. We train a model and have a very high success rate. We then put our actual model into production and after about three months, we realize our model is starting to make a ton of errors. Why was our model good in our training set, good for three months and then all of a sudden started failing? It is because our data was biased. In the orginal data all the photos of wolves had snow in them. Rather than learning to actually identify wolves vs dogs. Our classifer learned to identify either snow or no snow.\nAnother couple of examples can help us see how biased data can affect us in other ways. Imagine we want to know who is going to win the next democratic primary. We create a online survey that asks people who they are going to vote for. This data set is going to be biased. What we care about is how the population is going to vote, but we only have information about people who use the internet. We know that there are people in the population who do not go on the internet and any information we would gain from our survey would not take those people into account.\nOne more example of bias for you. Imagine we want to predict who is most likely to commit a crime. We use a bunch of data about prior arrests. This data is going to be biased in so so so many different ways. We can imagine it is going to predict certain races are going to commit more crimes.\nThis propogation of bias is a huge issue when we start to work on automating different processes. Bias can show up in many different ways and impact outputs. I am going to talk about a couple of different ways I think about combating bias using the examples above.\nOne last thing to note though. It is only possible to work on combating these biases if we know they exist. If they are in our data but we do not know they exist, then we will not be able to combat them. This is why having models that are explanaible is so so important. When a model is explainable we can understand why it is making a decision and then we can recognize when it is biased. If the model is truly a black box, it is much more difficult to identify if it is biased and especially how it is biased.\n There is a quote from statisican George Box that I have been thinking about a lot of late.\n All models are wrong, some are useful\n This quote is so powerful to me because it calls out exactly what models are. They are a simplification of the real world that makes assumptions. These assumptions are going to be sometimes wrong. The real world is an intensely complex place, and that means models are not going to fully account for that complexity. Even though our model is wrong it can still be useful and represent some part of the truth.\nLets imagine, that we have identified a bias in our data and we want to fix it. We can attempt to correct for a bias by applying additonal structure to our model. This means we are applying outside knowledge that is not contained in our data. This is essentially what we are doing whenever we make a model. We are making assumptions about how the world works and fitting our data into that. All models are making certain assumptions about how data interacts and then using that to do something that is hopefully useful.\nIn the example where we only survey people who are online, we can then use some structure to adjust our model. We know that people who are online are systematically different from those who are not online. We can look at those differences and understand how those differences correspond to their voting preferences. We can then re-calibrate our model to take into account these differences and predict for the general population rather than just internet users. This approach will not be perfect, but it should be better and get rid of some of the bias.\nEssentially, when our data is biased we add some additional structure to our model based on how we understand our data to be biased. Often this is in conjecture with getting some additional data which gives us a sense of how exactly it is biased. So we are combining our intuition of how the data is structured with both knowledge of the true problem we are trying to solve and some additional data. For example in this case we would be get census data about the entire population. This would allow us to see how our sample of internet users is different from the general population.\n Another tactic that I personally appreciate a lot is incoporating more uncertainity into our models. We should be building models that are transparent about their uncertainity and can convey their uncertainity. When a model is not going to be able to perform well because of bias it should communicate that. It gives us a better idea of where our model is going to be accurate and where it is just grasping at straws.\nThere is a concept that is very commonly discussed in economics but almost never talked about with regard to ML that is really important here. The term is external validity.\nHere is how external validity is used in economics and how I think it applies to Machine Learning. Imagine you run an experiment on college students. We would question the external validity of that experiment because does something that holds true for college students, hold true for elderly people? Essentially it helps us think about what is the scope of our findings. Where do our findings succesfully apply and where do they not?\nHere is how I think this could be applied to Machine Learning. Imagine you train an face recognition system that can recognize faces of people on the street from cameras. But in your training data set you never included any days when it was raining. Your model still works, but it has problems with external validity. It most likely will not work well for times when it is raining outside.\nTaking this vocubulary of external validity and applying it to machine learning would be a very helpful practice.\n I know I kind of went on a couple of different tangents, but what you are asking about is really a core problem of applying Machine Learning and Artificial Intelligence. I think here are a couple of the takeways that are important.\n  It is dire that we have models that are interpertable and explainable. Without this we will have very little understanding if our models are biased or how our models are biased.\n  All of modeling is using assumptions about the structure of the world to leverage data in a meanigful way. We can implement certain structures or assumptions to help us work with bias data.\n  Incorporating uncertainity into our models and transparency of that uncertainity should be a necessity. This allows us to better understand where our models are accurate and where they are not.\n  Implementing Machine Learning has the very real implication of propogating bias. If we do not want to incorporate bias into our systems we have to be constantly testing to see if bias does exist and if so how we can mitigate it.\n  I hope this is helpful. Let me know if it answers your question or if you have any follow ups.\n","permalink":"https://judahgnewman.netlify.com/writing/personal/biased_data_hawk/","tags":["Machine Learning"],"title":"How do you Solve for Biased Data?"},{"categories":["Industry"],"contents":"Project Description The Machine Learning community of practice was a one stop shop for all things machine learning at Nielsen. The goal of the organization was to provide resources related to machine learning. We recorded all of the different projects being worked on related to machine learning, who were the different machine learning experts in different fields and maintained general learning resources.\nWe hosted monthly webinars that highlighted different machine learning projects around the organization and provided a forum for practitioners to present their work and get feedback. We also had a website that contained lots of resources about general machine learning resources and machine learning information specific to Nielsen.\nMy Contribution During my time at Nielsen I was on the Machine Learning CoP Council. This meant that I was in charge of helping to make sure the Machine Learning of practice was a helpful resource through out Nielsen. During my time on the council there were two main initiatives that I led.\nStudy Groups I find that the best way to learn is learning directly from other individuals and learning together as a group. Having a group to study with makes it more enjoyable to study and holds you more accountable. While anyone can dedicate the time to work on their skills using resources from the internet, working together with a group of other people is a lot more appealing. During my time on the council I set up study groups in different Nielsen offices to improve machine learning skills. The goal of the study groups was to learn by getting hands-on experience with machine learning. The study group would identify some area at Nielsen that could be helped by machine learning and then come up with a small side project they could work on using machine learning. In this way they were working on an actual project for the business and learning more about how to apply machine learning.\nTechnical Skills for Non-Technical Leaders Some of the leaders in our data science organization recognized that their technical skills were not as strong as they wished. They reached out to ML CoP expressing that it was difficult for them to learn on there own and if we could help them set up a study group. We decided that the most helpful thing we could do was actually build content / lectures / exercises for them to work. When you are a leader in the organization there are so many different tasks demanding your time. It can difficult to find anytime to grow your own skills and if you do it can be frustrating to do it in isolation.\nWe would create lectures for a group of data science leaders once a month teaching them about different technical concepts that would help them better lead their teams. We built these lectures to help give leaders the necessary information to ask the right questions of people on their teams.\nWhat I Learned Working on the ML CoP was difficult at times. It was a side project I picked up outside of my normal work. The CoP did not have any funding and operated outside of the company hierarchy. At time it felt like no matter what we did things were going to stay the same. Overall, working on the ML CoP was very rewarding and it did seem like over time we were able to improve Nielsen\u0026rsquo;s ML skills. Here are a couple of different lessons I learned during my time on the ML CoP.\n  It can be very difficult to stay motivated when it seems like what you are doing does not make a difference. A good way to stay motivated is by creating a community that supports each other and is there for each other. The other members of the ML CoP council where the absolute best. Seeing their drive, work ethic and excitement also got me excited about other possibilities. It was also important to recognize when a project was no longer exciting and became draining. If the project was draining and we felt like it was not making a different then normally it was time to take a different approach.\n  One very important thing I learned during my time on the council was how important writing user stories is. User stories is not only important for software development but anytime you are creating a product for people. We knew that there were different users with different use cases but until we wrote out user stories we did not have much clarification about who those different users were and what they actually needed. User stories allowed us to take an abstract idea and put it into concrete details and actually actionable information. The process of writing user stories is really helpful in better understanding the value that you can provide.\n  Another important part of being on the ML CoP was learning how to identify when you want to build something yourself versus providing people with the tools to build something. We found with study groups, we just wanted to provide individuals with the infrastructure and communication to build their own study groups. By providing them with the tools they could actually build study groups that suited their needs the best. When we did the study group for leaders though, it was much more important for us to build out the entire study group ourselves. It was necessary for us to build the curriculum and the lesson plan.\n  ","permalink":"https://judahgnewman.netlify.com/writing/industry/nielsen/ml_cop/","tags":["Machine Learning"],"title":"Machine Learning Community of Practice: Nielsen"},{"categories":["Industry"],"contents":"What I Did at Nielsen My first job after completing undergrad was as a Data Scientist at Nielsen. Nielsen is a market research company that provides insight about what people buy and what media content they consume. I was a part of Nielsen\u0026rsquo;s early career program focused on fast tracking individuals to leadership roles. The program consists of four six months rotations for a total of two years. During my time at Nielsen I wrote about what I worked on and what I was learning. I wrote about each of my rotations and some side projects I took on. During my time at Nielsen I focused on building sustainable solutions and modernizing how we work. Below are the links for all the writing I did about Nielsen.\nRotation 1 - Non-coop estimation\nRotation 2 - Bayesian Inference for Ratings\nRotation 3 - Digital Pipeline Development\nMachine Learning Community of Practice\nI left Nielsen after a year and four months to pursue an opportunity at Change Research. I wrote about why I was leaving Nielsen and why I am excited about working at Change here\n","permalink":"https://judahgnewman.netlify.com/writing/industry/nielsen/my_time_at_nielsen/","tags":null,"title":"My Time At Nielsen"},{"categories":["Industry"],"contents":" Project Description Nielsen receives data from retailers about all of the items they sell. For example, Walmart sends Nielsen all of the products sold at each store in the past week. Brands, such as Johnson \u0026amp; Johnson, use this information to see how much of their product is being bought. Some retailers do not send Nielsen their data. We will refer to these retailers as non-cooperators. Non-cooperators do not send Nielsen their data because the majority of the items they sell are from their own brand. A retailer that is a good example of this is Trader Joe\u0026rsquo;s. Non-cooperators limit Nielsen\u0026rsquo;s ability to give our clients accurate information about market share because there is part of the market that Nielsen cannot see. My project was to develop a new methodology for estimating the sales of non-cooperators. My Contribution I developed a new methodology that had a 20% improvement in accuracy over Nielsen\u0026rsquo;s old methodology. Estimating non-cooperators\u0026rsquo; sales is a particular difficult task because there is good reason to believe that non-cooperating stores are fundamentally different from stores that do cooperate. My methodology was based on the principles of how kernel regression works. The methodology works in the following two steps:  Use different features to assign a similarity score between each store  Project a given store\u0026rsquo;s sales using a weighted average of similar stores.   What I Learned This rotation involved a lot of learning for me. It was my first foray into the business world. My project was completely open ended. I was tasked with a business problem and asked to solve it however I wanted to. I learned a lot about how to build a model from scratch, how to define success criteria and how to work efficiently. I discuss some of my main learnings in detail below.  Model acceptance is more complicated than just maximizing model accuracy. Models are just one part of a larger complex system. Verifying that a model is worthwhile involves understanding the role that a model plays in that larger system. We have to take into account model simplicity, model explainability as well as how the new model compares to the previous approach. This project was particular difficult because we did not have a truth set that we could use to measure model accuracy. We had to develop a proxy for model accuracy. Validating a model involves using lots of evidence besides just error rate. Building off of that point, I learned a lot about how being a good data scientist involves a lot more than being good at building models. It is of critical importance that a data scientist understands the business context and the role that their model plays in it. You have to be a good communicator and understand where different people are coming from. It does not matter if you have built the best model in the world, if it is solving the wrong problem. Building good models is only one skill that determines if you are a good data scientist.  This rotation also taught me the importance of understanding our data before beginning the modeling process. Even if a model should work in theory, if there is no signal in the data, even the best model will not be useful. Throughly exploring our data before hand and identifying the key relationships or structure is crucial. If you believe a certain modeling approach will work, find evidence in the data. Use the data to show that the assumptions you are making in the model building process are good ones. All models involve some assumptions. It is important that those assumptions are supported by data.  This rotation was the first time I was coding for the majority of the day, every day. I developed much better coding habits. I learned how to write good documentation for both myself and others. I also now understand how to better plan modularity in my code.  Meetings can be your best friend or your worst nightmare. Meetings are opportunities to hear other\u0026rsquo;s opinions and get feedback on your work. Truly two of the most important things you can do. Meetings can also suck away all your time and zap your energy. A couple of tactics I have found that make meetings run smoothly:  Clearly state the purpose and agenda of each meeting  Send materials for the meeting ahead of time  Make sure that everyone\u0026rsquo;s voice is heard and that everyone is contributing. Each person should be at the meeting for a reason.  Always take meeting notes and send them as a follow-up afterwards  Meetings are places were decisions should be made and everyone should have action items after a meeting. Make sure actions items are clear and understood.     ","permalink":"https://judahgnewman.netlify.com/writing/industry/nielsen/rotation_1/","tags":["Statistics"],"title":"Rotation 1: Non-coop estimation"},{"categories":["Industry"],"contents":"Project Description Nielsen is best known for their linear television ratings which estimate the number of people who were watching a channel at any time. This information is estimated using a representative panel of households all across the United States. Households in Nielsen\u0026rsquo;s Panel report all of the television that they watch. Each household\u0026rsquo;s viewing is weight based on its demographics to ensure that the viewing is representative of the population.\nWhile this approach gives an estimate of the number of people who are watching a given channel, there are some problems with it. When there is a show that is very widely watched, such as the Super Bowl, Nielsen\u0026rsquo;s methodology does a good job of accurately estimating the viewership. But for shows with less viewership such as more niche shows or shows during the day time, Nielsen will regularly have no one in the panel viewing those shows. We can expect that the rating is not actually zero, but because the panel does not account for everyone it does a bad job estimating these low viewership shows.\nThe computational modeling team is exploring a new approach to calculate ratings. The team is exploring using Bayesian Inference to combine panel data with other data sources to create a more accurate estimate of TV viewership. My Contribution During my time on the computational modeling team my work focused on building tools that let the team iterate faster. I built a couple of different libraries for the team that allowed them to incorporate new data into their models and evaluate their models. I focused on reducing the amount of code that team members had to write and letting them focus on modeling decisions rather than engineering.  I built a library that allowed the team to easily pull third party data. For example this library allowed the team to easily pull census data or weather data. This made it easier for the team to incorporate other data sources into their modeling process.  I built a visualization tool that allowed the team to systematically evaluate their models. This visualization tool consisted of a user interface that allowed the team to understand quickly the success of their models and where their models were failing. The interface also allowed the team to explore feature importance and how different features were driving model output.  The last tool I developed was a library that wrapped the model training process and made it seamless to publish a new model and visualize it. This made it easier for the the team to train a model, visualize it, come up with improvements and then train a new model.   What I Learned This rotation was much more structured than my first rotation. I was working with a team that had already been working on a problem and had a defined direction. I was building software to help enable the team. This was the first time I built software that other people were going to use.\n I learned a lot about how to design software that is extendable and that people will actually use. I learned the real power of object oriented programming and how to use abstract classes. I learned how to make software diagrams and create user stories. I also got first hand experience with the necessity of focusing on your user and involving them in the creation process (how to get feedback from users and incorporating that as I develop). I also learned about how to make different parts of software independent and have them interact through contracts. This rotation taught me a lot about how to build software that people will actually use.  I also learned about how to think about problems in a bayesian way. While I was not doing as much model development, I still had to understand how the team was approaching the problem and what were the advantageous of the new methodology. I learned about how MCMC (markov chain monte carlo) and SVI (stochastic variational inference) work. I really enjoy thinking about problems in terms of a data generating process now and using structural models rather than pure machine learning. A big part of being on the computational modeling team was that we were working in a new space. This meant a lot of reading of recent papers and learning as we go. It is a very different process to apply something that is well established then to apply something that is still being developed. While at times it was difficult to be building something where best practices or approaches had yet to be established, it was quite exciting.  The computational modeling team was based out of New York while I was working in Chicago. Through this process I learned how to best work with a remote team. It is so important to be friends with the people you work with and that is difficult when you are not seeing them regularly. I learned how to put time into forming remote relationships and to learn about my teammates lives outside of work.   ","permalink":"https://judahgnewman.netlify.com/writing/industry/nielsen/rotation_2/","tags":["Machine Learning"],"title":"Rotation 2: Bayesian Inference for Ratings"},{"categories":["Industry"],"contents":"Project Description Nielsen is in the process of transforming how we build our products. We are a ninety five year old company that is not technologically native. We are working on building products that are more maintainable and reducing the amount of work that we duplicate. Data science in particular is undergoing a large transformation within Nielsen. Previously, Data Science sat very far away from the final product and a large portion of the data science organization did not have the technical skills to build reliable technical products. The data science organization was very much seen as the place where research happened, but then technology would actually build products. There also would be very minimal day-to-day interactions between data science and technology. Data science would work on a project for a long time and then throw it over the wall to technology. Software development is never going to be Data Science\u0026rsquo;s strong suit. The people who are in Data Science have different skill sets and have strengths in a different field. But we can work to develop better engineering skills within Data Science and have data science writing code that goes into production systems. I am working to improve how data science contributes products for the digital and advanced tv team. Moving data scientists closer to the product involves changing how we work on a couple of different fronts. One part of it is working with data scientists to improve their engineering skills. Another part of it is creating internal systems that better facilitate development. Another part is working with tech to figure out the best division of labor between the two separate organizations. There will not be a singular panacea that solves everything, but rather we have to work with lots of different individuals to figure out how we can put them in the best situations to succeed. My Contribution  One of my big contributions this rotation is rewriting and automating our digital adjustment factors process. Digital adjustment factors is a process for how we adjust our digital numbers based on some known biases in our data. This process used to be highly manual with all of the code being stored in notebooks and someone having to run it manually. I have done all of the following things with the code in an effort to improve this process:  Taken large SQL queries and turned them into modular testable PySpark  Written both unit and integration tests for all of our code  Orchestrated the code using Airflow  The process is now callable through an API   While re-writing the adjustment factor process is a nice win, a large focus has been on using this process to develop learnings that can be used for future projects. I have written extensive documentation about how I automated this process and what best practices would be for future developers. While I was working on this project the beginning was quite slow as I was learning a fair amount. After that I was able to develop quite quickly. The hope is that by using my documentation other developers will be able to have a high velocity in their projects from the beginning.  During this rotation I have also identified that there is a large gap in how we document and track models. I believe that is gap exists across the entire data science industry and not just within Nielsen. All models involve the data scientists making many different assumptions and decisions. We should make the data science process one that is a lot more transparent and accessible to all. I have been working with Nielsen to develop better practices documenting our data science work.   What I Learned  This rotation was the first time I had to write code that was directly going into one of our existing products. I have had a fair amount of experience writing code with tests, but most of previous experience was writing unit tests. This rotation I learned how to identify and write integration tests. I learned how to incorporate system monitoring into what I am building and how to build something that you can easily debug and identify what is causing issues.  In this rotation it was very important for me to take what I was learning from one process and identify how it could be applied to many different process. I had to continually push myself to think outside of the scope of my project and focus on larger structures. I found that doing this big picture thinking was very helpful in doing better work locally. Connecting my work to larger processes helped me better understand the context of my work and make me more motivated.  I had previously tried to take aspects of agile and incorporate them into my day-to-day but this was the first time I truly worked in an agile framework with sprints and the agile ceremonies. One of the places I see people failing the most with agile is too closely trying to follow the letter of the law. Agile is much more a way of thinking about work and planning rather than an exact script to follow. It is not about doing everything by the book but finding which parts of agile help you the most.  A large part of this rotation has been working with many different teams across both data science and tech. I have had to understand how different people have found their own ideal workflows and identify the commonalities between them. It can be really easy to look at how different people do different things and recognize all the differences. While there will be lots of differences and those are important, we want to let teams and individuals locally optimize, it is just as important to identify the commonalities. The places where we find a common structure are the places where we can use tools or a structure to help people focus their time. We want people focus their time not on the commonalities but the nuisances and uniqueness of their individual space.   ","permalink":"https://judahgnewman.netlify.com/writing/industry/nielsen/rotation_3/","tags":["Software Engineering"],"title":"Rotation 3: Digital Pipeline Development"},{"categories":["Papers"],"contents":"By Sara Alspaugh, Nava Zokaei, Andrea Liu, Cindy Jin, Marti A. Hearst\nSummary This paper is a qualitative analysis of thirty interviews with data scientists about their data exploration process. The interviews focus on the different tools that they use, how they are used and what functionality data scientists still desire. The paper shows how different this exploration process can be for different data scientists and how they all use different tools. The authors define a structure for the data science workflow. The exploration phase comes in where the data scientist has a defined goal, but does not know how to solve it or what is in the data. The exploration phase consists of exploring the data and using the data to guide further direction. Exploration is how the data scientist transitions from general business problem to an action plan. There is a key tension between ease of us and power in exploration tools discussed through out the paper. Some data scientists find tools that have a lot of abstraction and are easy to use very nice, while other data scientists find them to be very limiting. The authors highlight how there is going to be this trade-off. The easier to use a tool is and the more it abstracts away from the end-user, the less customizable or powerful the tool will be. Thoughts  This process of exploration is what makes the data science process unique. Other processes have some aspects of it, but it is truly front and center in data science. So much ends up being driven by the data itself! Thats fantastic and both scary. It is fantastic because it means less is grounded in our pre-conceived assumptions. Rather than making an assumption we can let the data tell us and guide us. But it is also scary because humans are not good at reasoning about very large datasets. Large datasets operate on a scale that is different to how we think. Without being careful and having some structure, it is possible to get completely lost working with data.  One thing that I think this paper misses on, is that exploration is always happening in the data science workflow. It is not just one piece that happens and then we are done with it. There might be more exploration happening at the beginning of the workflow, but data science is constantly returning and doing more exploration. Exploration is such an iterative process. We learn something, try something out, learn something new and then return back to the data with a new understanding and direction.  In this paper, some of the data scientists call for one system where they can do everything. To me this seems like a silly ask. There is never going to be one place where you can get everything done. Different systems are designed for different niches. If one tool focus on doing everything, a different tool will come along that can do one part better. This is why platforms, like our operating system, are so powerful. They provide the infrastructure for many different things to run on. This allows the user to pick the tools that are the best for them.  In general, I do a bad job of recognizing how unique my own system of working is and that there are so many workflows out there. Everyone will have their own best way of working that maximizes them. There will be commonalties in what is useful but different people will want different things. Being able to optimize a tool for a given niche is really quite powerful.  ","permalink":"https://judahgnewman.netlify.com/writing/papers/2019/paper-futzing-and-moseying-interviews-with-professional-data-analysts-on-exploration-practices/","tags":["Software Engineering"],"title":"Futzing and Moseying: Interviews with Professional Data Analysts on Exploration Practices"},{"categories":["Papers"],"contents":"By Mosca, Abigail and Robinson, Shannon and Clarke, Meredith and Redelmeier, Rebecca and Coates, Sebastian and Cashman, Dylan and Chang, Remco\nSummary The authors of this paper use interviews of 14 data scientists to characterize how a data scientist takes business requirements and turns it into a project. The authors call this process of taking a clients needs and turning it into an analysis plan **_initialization. _**The authors specifically focus on how data scientists interact with non-technical clients. In the paper, clients asks are broken into three categories: high specificity, medium specificity and low specificity. High specificity is when the client knows exactly what they want and low specificity is when the client has a minimal idea of what they want to learn. The authors identify different tactics that a data scientists uses during initialization depending on how specific the client\u0026rsquo;s ask is. Thoughts  A data science workflow is often quite messy at the beginning. The project regularly begins with little insight into what the data is going to reveal. As the data scientist begins to interact with the data they learn new things and are better able to make an analysis plan. Data Science is a very iterative process in this way. The data scientist does not know what they are going to find. They do some research, make some findings, and then plan next avenues of research.  Due to the data science workflow being messy, people often treat it as a very different process from an engineering workflow. Traditional engineering workflows, both software or otherwise, have best practices for planning increments of work, writing technical specifications, and defining success criteria. These best practices are things that data scientists should follow but currently do not. Just because there is a lack of clarity in the beginning, does not mean we cannot use the best practices from engineering. Using these best practices would help data scientists have more direction, ensure reliable quality code and speed up the development process.  ","permalink":"https://judahgnewman.netlify.com/writing/papers/2019/paper-defining-an-analysis-a-study-of-client-facing-data-scientists/","tags":["Software Engineering"],"title":"Defining an Analysis: A Study of Client-Facing Data Scientists"},{"categories":["Papers"],"contents":"By Mary Beth Kery, Bonnie E. John, Patrick O’Flaherty, and Amber Horvath, Brad A. Myers\nSummary The authors of the paper present a juypter lab extension called verdant that keeps track of notebook history. The goal of verdant is to to help data scientists answer questions about their research process even if they have not documented everything along the way. The need for this tool is motivated by how messy and non-linear the data scientist process is. Through out the data science process many different things are tried out and it is impossible to record everything. Verdant makes it possible for users to look back through what they did and answer key questions. The authors of the paper focus a lot on building an interface that makes information retrieval simple in easy. The tools provides a few different interfaces for information retrieval. They borrow best practices from the information foraging theory to guide their design. One of their key insights is that there is a certain structure behind all data science processes in notebooks and they use that to help guide the information they display. They use the structure of cells, artifacts that the data science has created and the passing of time to make search and retrieval easier. The authors evaluate their tool through a set of 15 user interactions with data scientists ay JuypterCon. From this set of interactions the authors verify that their tool helps with information retrieval and get feedback about how to make the tool even better. The authors show that by keeping this history and making it accesible to the data scientist they are able to answer questions about their process which they would not have been able to otherwise. Thoughts  This is another example of bringing best practices from other fields. Bringing learnings from another field is always powerful and shows how problems field\u0026rsquo;s face are often quite similar. While the actual task might be quite different, we can learn from how other fields have successfully solved essentially the same problem. Defining a hierarchical structure for the different parts of a data scientists workflow gives the authors a systematic way to structure information. Identifying this underlying structure and then leveraging it is the real key to their paper. Finding this underlying structure is easier when we limit our scope to just notebooks. Notebooks have a defined structure of cells and only a limited number of actions can happen. Finding/defining this underlying structure for a more general purpose data science workflow is a difficult task. A data scientists work happens at many different granularities. There is the scope of an individual coding session. There is the scope of a certain segment of model development such as EDA (exploratory data analysis). We could even think of different sprints as another scope. Verdant helps data scientists with information retrieval at a specific granularity: an individual working session. But what about if a data scientists wants to retrieve information from months ago? Information retrieval and information sharing over a larger scope is still an open problem. Data scientists need a better structure for documentation over the span of an entire project. This paper has key insights about how to find underlying structure in the data science workflow and how to best structure information so that it can be found later. Documentation over the entire workflow will require a more active effort from the data scientists themselves. It will have to be a more active effort of saying this what is important because there is just too much information to passively store all of it. It will need to be a tool that helps lead the data scientists towards best practices of documentation  ","permalink":"https://judahgnewman.netlify.com/writing/papers/2019/paper-towards-effective-foraging-by-data-scientists-to-find-past-analysis-choices/","tags":["Software Engineering"],"title":"Towards Effective Foraging by Data Scientists to Find Past Analysis Choices"},{"categories":["Personal"],"contents":"One of my friends and teammates asked me for some feedback about what he should be working on during the frisbee off-season. I wrote him an email explaining about how I think about getting better in the off-season before sending him some personal feedback. I thought sending this email would help contextualize how I think about getting better and reaching our goals. Some of the information in here, can be applied to really any skill, not just getting better at frisbee. The full email is below\n Hey Ben,\nHere are my thoughts about training during the off-season and making a comprehensive plan: I have noticed that a lot of players struggle to continue getting better after college. I believe a large portion of that is because players get better in college through a largely passive effort. In college you are playing so much frisbee all the time, that you are naturally going to get a ton better. After college though you end up playing a fair amount less and that makes it harder to keep getting better in the same way. I also think its because the way you grow that extra 10-20% is a lot different then the way you grow the first 80%. Growing that last 10-20% is a lot harder. But it is what separates the good players from the ones who are really great. Now that we have hit the offseason it is an opportunity to re-focus and really dial in on how we want to improve and the steps to make that happen. For both of us, I think we want to use the off-season as an opportunity to improve a lot. Using the off-season to improve can be a lot harder then using the season. We are not getting as many reps against high level competition as we did during the season. This means that getting better during the off-season requires a different approach. We have to be more concentrated in how we want to get better. I believe it is necessary to make an off-season plan. If we just go into the off season abstractly saying we want to get better, nothing will come off that. It is important to identify specifically how we want to improve, and then make a plan for how to get there. Let us start off with a goal. I think for both of us, a goal would be to make Revolver next year. Now that we have our goal let us break it down a little bit more. We want to be good enough to make Revolver. But what does that actually mean? How do we become good enough to be on Revolver. There are a lot of different players who are on Revolver who are good at different things. There are two parts of it. We have to understand who we are as players and then how that could fit in with a team like Revolver. One huge part of this is getting feedback from others. You do a really good job of this. You are constantly seeking feedback from others. While we might have an understanding of who we are as a player, what our strengths and weaknesses are, others can help provide more perspective. The other thing I want to call out, is most people think about getting better in terms of minimizing their weaknesses. While minimizing your weaknesses is good, there are plenty of high level players with weaknesses. What sets high-level players apart is that they have one strength that is just so good. Not everyone on a team needs to be able to do everything well. But if you can do one thing better than everyone else on the team, you probably have a spot on that team. Once we have identified what are the skills we want to work on and what is our personal path to making the team that we want to, we have to make a plan to get there. This is the part to me where we have to start thinking on small time scales. It is impossible for us to think about actions we take over the entire off season. Instead we have to think about what are we doing this week. What are we doing this week to reach our goals. For example, if I want to be a shut down handler defender, what I am doing this week and how does that contribute to the goal. Is it that I am doing some lifts to increase my athleticism. Is it that I am doing some footwork drills to improve my change of direction. Is it that I am watching film of other good defenders to learn what they do. When we break our goals down into weekly plans, then they become something we can actually reach. We then know, are we doing enough every week to reach our goals. There are three more points I want to touch on:  I believe one of the best ways to get better is to learn from others. That does not necessarily mean someone who is a better player than you, but it is pivotal to find people you can learn from. Often the best way to do this is to work out with players from the team you want to make. Surrounding yourselves with people you can learn from is very important.  I also believe that there are mental skills that people can work that most people do not think about. Mental skills are what separate the truly best players. Having a strong mental game is what determines your consistency of play and your ability to learn from your mistakes. Everyone should put more time into working on their mental game. We can talk more about what that looks like if you are interested. It is probably its own long soliloquy.  It is easier to do anything when you have a community supporting you. It is easier to wake up early in the morning to work out when you are going to see your friends. It is more fun to run sprints when you are doing it with people you care about. Working hard to get better is more meaningful when you do it with other people. Set up a community this off-season that will help you get better and reach your goals.   Let me know your thoughts on all of this. I am pretty amped about this off-season and excited to work on getting better. It is huge to me that I am in a place where I will be able to play year round.\n","permalink":"https://judahgnewman.netlify.com/writing/personal/personal-planning-for-the-offseason/","tags":["Frisbee"],"title":"Planning for the Offseason"},{"categories":["Papers"],"contents":"By Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru\nSummary The authors propose that machine learning models should be accompanied by documentation known as \u0026ldquo;model cards\u0026rdquo;. A model card should outline who built the model, what the purpose of the model is, how it was validated and what it\u0026rsquo;s performance is according to many different metrics. The authors focus on models that are making decisions about humans. They specifically call out models that characterize people and speak to the importance of evaluating how a model classifies different groups of people differently. The authors believe that model cards will better ensure that models are not misused and provide greater transparency into the performance details of a model. Some other benefits that the authors highlight about model cards is:  Creating model cards will force model builders to be more thoughtful and careful during the model building process. They will have to think ahead into how the model is being applied and what the weak points of the model might be.  Model cards will allow anyone to make better decisions about a model. It will provide both technical and non-technical users with insight into model performance and model use cases.  Model cards will also allow us to track how model\u0026rsquo;s change over time. When a model gets updated it\u0026rsquo;s model card should also get updated. By seeing the model card over time we can better understand how the model has changed over time.   Thoughts  I really appreciated how this paper referenced best practices from other fields. Other fields have been around for much longer and developed standards about how to document products. Machine Learning can learn from these other fields and adopt best practices for documentation and providing resources about models.  I appreciate the human-centric approach to machine learning that this paper takes. Even if a machine learning model is not making classifications directly about humans, we should still take a human centric view of it. All technology should be thought of with a human-centric approach. Why was this technology built, who is it going to impact and how is it going to impact them. We often get so wrapped up in technological progress we forget that technology is about enabling humans.  During my undergraduate studies of economics a term we talked about a lot was external validity. External validity is about generalizing our findings in one scenario to other scenarios. Very rarely is external validity talked about in the Machine Learning setting. One big reason might be because of the blackbox nature of some machine learning algorithms. When something is a black box it is less clear what circumstances it is valid for and which it is not. Even though the authors do not use the term external validity, they do use this kind of thinking. They specifically call out, if a model was trained in pristine conditions, it might not be valid for suboptimal conditions. It is critical to have more of this type of thinking in the Machine Learning community.  From my experience, even if data scientists know they should document, they will not. There is not a good tool to document that makes the process easy. Currently, documentation is a pain that feels like it slows the process down. We need to find a better way for data scientists to document their process.  ","permalink":"https://judahgnewman.netlify.com/writing/papers/2019/paper-model-cards-for-model-reporting/","tags":["Machine Learning"],"title":"Model Cards for Model Reporting"},{"categories":["Papers"],"contents":"By Roy Schwartz, Jesse Dodge, Noah A. Smith, Oren Etzioni\nSummary The authors of this post argue that a large percentage of machine learning research focuses on Red AI and make a call for more research to be centered on Green AI. The authors define Red AI and Green AI as follows:  Red AI: research that makes small improvements on the state of the art by leveraging more computation resources Green AI: research that focuses on diminishing the amount of computation necessary to reach a result. Research that is able to reach similar levels of accuracy with higher efficiency.   The authors of the paper are very careful to call out that Red AI is not bad and that there is important research would could be classified as Red AI. They argue that too much research falls into the category of Red AI and we should reward research more that falls into the category of Green AI. The authors specifically call out that access to computation resources is a luxury. When there is a necessity of large computation grows to replicate results or the ability to build upon results is only accessible to individuals with those resources. The authors believe that democratizing access is important, and when results are based on large compute that limits who can partake in the research process. In the paper the authors show that there are logarithmic returns to accuracy from an increase in the size of computation. This means if we increase computation size by a factor of 2, our accuracy will only increase by a factor of log(2). If there are truly logarithmic returns to scale of increasing compute size, then increasing compute will not be a sustainable way to continue building more accurate systems. The researchers propose FPO (total number of floating point operations) as a metric for measuring the efficiency of an algorithm. They outline the importance of having a metric to calculate efficient that is hardware independent. Thoughts  It is easy to forget than AI/ML is still such a young field. Best practices in terms of making improvements and how to build upon previous research are still being established. One difficulty in determining what research is of the highest quality is we do not know what directions will bring us the most success. It is possible that once we reach a certain level of compute or a certain amount of training data, our systems will actually have much greater capabilities. If that is the case, research that increases the amount of compute and gets higher accuracy is still important. I think the main takeaway from this paper is a call for more diversity research approaches. Because we do not necessarily know the best paths forward it is important we try lots of different paths. We should reward research approaches that are new and that take different approaches rather than publishing every paper that gets slightly better results by using more resources.  In the paper, the authors argue that we should democratize the ability to do research. If only people with a lot of resources are able to do research, then we are limiting the number of places new ideas can come from. In theory this makes a lot of sense to me. With greater diversity and more input we should be able to make more progress. I would be very interested to see if there was a paper that looked at how democratizing a resource changed research outcomes.  The authors comprehensively explain their reasoning behind why FPO is a good metric for efficiency. It was refreshing to see the authors break the efficiency problem down into smaller chunks, propose alternative solutions and then explain why they believed their solution was optimal. This process of breaking the problem of efficiency down into smaller steps was key. If you were tasked with just creating a metric that tracks efficiency that is a very open ended problem. But the problem becomes a lot easier to tackle when the authors break efficiency down into three separate values.  One part of the paper I was particularly interested in exploring more is about hyper parameter search. Rarely in a paper do the authors explain how much time they spent optimizing hyper parameters or how those hyper parameters were optimized. Papers also rarely show model results when you use non-optimal hyper parameters and how far these results deviate from the optimal results. I would be curious to hear more about how long it takes researchers to get the best hyper parameters. If there is a large difference in performance between optimal and non-optimal hyper parameters and it is very computational intensive to find the correct hyper parameters are the results replicable?  ","permalink":"https://judahgnewman.netlify.com/writing/papers/2019/green-ai/","tags":["Machine Learning"],"title":"Green AI"},{"categories":["Papers"],"contents":"By Devezer B, Nardin LG, Baumgaertner B, Buzbas EO\nSummary This paper creates a model of how scientific researchers make progress towards the truth. By examining this process under different conditions the authors were able to identify what characteristics help lead scientists towards the truth. This work was motivated by understanding the role that reproducibility has in the research process. Through experiments with different conditions the authors were able to reach some conclusions about how reproducibility and other characteristics affect a communities ability to find the truth. The authors experiment had the following set-up:\n There is data that was generated from a certain process with a certain amount of noise There is infinitely many scientists and one of them is selected at random to propose a hypothesis for how the data was generated. If that hypothesis or model, is more likely than the current hypothesis it replaces the current hypothesis. This process of scientists trying to find the truth can be modeled using a Markov chain.  This experiment allowed the authors to test how varying different characteristics such as, how the scientists make hypothesis, or how much noise is in the data generating processing, affects the following outcomes:\n How quickly does the group of scientists find the correct model? How much time do the scientists have the correct model as the global hypothesis?  The authors found the following results:\n Even if our results are reproducible that does not mean that we have found the truth Having a population of scientists with heterogeneous research approaches performs better than a homogenous population It is more important to have innovative research early on in the research process. Later, we want research that is focused on refining the current hypothesis  Thoughts  The core argument of this paper can be boiled down to the following observation, having reproducible results is different from finding the truth. In this paper we update our hypothesis by comparing it to another hypothesis. In the setting from the paper, it is quite clear how just because something is reproducible does not mean it is the global truth. But in real world settings this is often quite hazy. What does it mean for something to be locally true versus globally true?\u0026rsquo; I see a lot of parallels in this paper to the idea of local optimum and global optimum in optimization. If we are focused too narrowly we can end up at a local optimum and not a global optimum. The equivalent would be if we are focused too much on reproducibility we will similar only reach local results. I am not sure how well this parallel holds though. I do not imagine research as one large optimization problem with local optimum and one global optimum. I see research as using small narrow truths to help us build one larger truth. I personally prefer thinking about research results in a Bayesian perspective. Each piece of research is just a piece of evidence helping us update our beliefs. With any piece of research we have never fully proofed anything but we have provided some evidence that a certain direction might be correct. When we reproduce that result we have provided even more evidence. When we are unable to reproduce that result we have provided some evidence that direction might be incorrect. If we view research in this way then no piece of research is proving a definitive truth. Rather research is proving evidence in favor of a certain view point which is helping us update our opinions. This mindset about research leads to a great understanding of the limitations of each piece of research and how different pieces of research interact with each other.  Research often gets boiled down to the paper where it was published. But research is so much more than just one paper that was published at a certain time. As the authors get at in this paper, research is a process of gradually trying to get closer and closer to the truth. Thinking of research in terms of papers rather than the greater process of trying to understand the truth, can lead to dangerous practices. Each experiment, paper, blog post is in a dialogue with all of the other research before and after it. I love this intiative called ML Retrospectives being rolled out at NeurIPS 2019. This initiative specifically calls out that research is a lot more that just papers. Research is a never ending dialogue and there should be other dialogues for discussion, communication and sharing.  ","permalink":"https://judahgnewman.netlify.com/writing/papers/2019/scientific-discovery-in-a-model-centric-framework-reproducibility-innovation-and-epistemic-diversity/","tags":["Statistics"],"title":"Scientific discovery in a model-centric framework: Reproducibility, innovation, and epistemic diversity"},{"categories":["Papers"],"contents":"By Jessica Horan-Block \u0026amp; Elizabeth Tuttle Newman\nSummary This paper by two attorneys at Bronx Defenders argues that quick, decisive, and aggressive action can return children to parents after abuse allegations much faster than traditional approaches. The authors outline how minority and low income parents are more frequently accused of abuse. Parents will bring their children to the Emergency Room in response to accidental injuries or strange physical developments. When the parents explanation for how the injury occurred is not deemed sufficient by the physician, they will be accused of abuse. For non-minority affluent families very rarely are the parents accused of abused. Instead the focus is on consoling them for their child\u0026rsquo;s injury.\nThe author\u0026rsquo;s argue that being separated from your parents and having to go through family court is dramatic for children and can hurt their development. Through early actions immediately after the abuse allegation, children\u0026rsquo;s return to their parents can be expedited. A more traditional approach was to wait for a lot more information to be gathered before taking any action. Bronx defenders now takes the approach of putting the burden on the ACS (Administration for Children\u0026rsquo;s Services) to prove that abuse was the most likely cause of the injury rather than waiting until later and having to prove that abuse was not likely. Even if this approach does not return the child to their parent it puts the parents in front of the court much sooner and the lawyers are able to learn what the case ACS is making against the parents.\nThoughts  The paper has many in depth examples about specific cases and how they were approached. Using specific examples is so helpful in illustrating their points and helps ground their arguments. All papers should use specific examples more. It is so important to be able to connect general theory with actual implementation. It is very powerful that the authors show their learning process. Rather than just explaining their argument, they outline what their old approach was and how the new approach compares to it. This shows why they had previously taken another approach and how they transitioned to a new approach. This depiction of the learning process makes their arguments compelling. I enjoyed how the paper was written directly to other practitioners. I have often read papers that seemed to be aimed at the void. Having a direct \u0026amp; specific audience helps guide what are the important points and how can the information from this paper be incorporated by other individuals. Papers are not written just for the sake of knowledge but rather to share knowledge with others. Writing to a specific audience can help direct the focus and direction of a paper. The ACS, the children\u0026rsquo;s parents, and the legal system should all have the best interests of the family in mind. Yet there are constant examples of where all of these groups are at odds. I was struck reading this paper by how distant the ACS seemed to be from the children and the families. This was truly an example of an outsider coming into a community and telling the community that it knows what is best for it. This is always a recipe for disaster and misalignment.  Full Disclosure: This Paper was written by my Sister-in-law\n","permalink":"https://judahgnewman.netlify.com/writing/papers/2019/accidents-happen-exposing-fallacies-in-child-protection-abuse-cases-and-reuniting-families-through-aggressive-litigation/","tags":["Policy","Race"],"title":"Accidents Happen: Exposing Fallacies in Child Protection Abuse Cases and Reuniting Families Through Aggressive Litigation"},{"categories":null,"contents":"What I am Currently Reading  Books Recently Read  Good Reads Regular Internet Reading  Business\n Stratechery: Ben Thompson ($120 a year) Matthew Ball Eugene Wei Exponential View: Azeem Azhar ($85 a year)  Tech\n Pointer Julia Evans Will Larson Tushar Chandra Fast AI : Rachel Thomas \u0026amp; Jeremy Howard  Research\n Andrew Gelman The Morning Paper: Adrian Colyer  Essays\n The New Yorker ($50 a year) Wait but Why: Tim Urban Paul Graham  Here is a spreadsheet that records the articles I enjoyed every week.\n","permalink":"https://judahgnewman.netlify.com/reading/","tags":null,"title":"Reading"},{"categories":["Books"],"contents":" It is that time of year again when every website is publishing some piece of content trying to sum up 2018. Whether it is Facebook, posting a video that is your year in review, or a march madness style bracket to determine what person, place, or thing won 2018, everybody is attempting to summarize the past year.\nA lot has happened over the past year. In 2018, I graduated college and started my first full time job. I moved to a new apartment. My brother got married and my two remaining grandparents died. I spent more time on an airplane then I had in any previous year. As always, the only constant is that things are changing.\nOne big change that happened in 2018 is that I read more this year than any other year previously. In total, I read 38 books over the past year. A large part in this increase was due to getting a Kindle for the holidays last year. Another big part was now that I have graduated, I have a lot more free time to be reading. There is no better way to sum up my 2018 then a review of the different books I read this year.\n Here is the list of all the books I read this year. I also included some of the more meaningful quotes and ideas from certain books. Some of the books do not have quotes because they are graphic novels, some because they are hardcovers which I lent to other people and some because they are books from the library and so I cannot look back at my highlights.\nBefore we delve into the list though, here are some high level stats about the books I read.\nI read 38 books by 34 different authors. 11 of those authors were female, 7 of the authors were black, and 7 of those authors were asian. 20 of the books were fiction while 18 of them were non-fiction. Of the 20 that were fiction, 11 of the books were science fiction (My brother’s tastes influence me, it would seem).\n The Opposite of Loneliness — Marina Keegan Thoughts on why so many people do consulting post-graduation:\n “What bothers me is this idea of validation, of rationalization. The notion that some of us (regardless of what we tell ourselves) are doing this because we’re not sure what else to do and it’s easy to apply to and it will pay us decently and it will make us feel like we’re still successful. I just haven’t met that many people who sound genuinely excited about these jobs. That’s super depressing! I don’t understand why no one is talking about it.”\n Ender’s Game and Ender’s Shadow — Orson Scott Card  “Human beings may be miserable specimens, in the main, but we can learn, and, through learning, become decent people.”\n  “You, however, understand the profound truth that you must reveal your stupidity openly. To hold your stupidity inside you is to embrace it, to cling to it, to protect it …. But when you expose your stupidity, you give yourself the chance to have it caught, corrected, and replaced with wisdom”\n Between the World and Me — Ta-Nehisi Coates  “I have raised you to respect every human being as singular, and you must extend that same respect into the past. Slavery is not an indefinable mass of flesh. It is a particular, specific enslaved woman, whose mind is active as your own, whose range of feeling is as vast as your own; who prefers the way the light falls in one particular spot in the woods, who enjoys fishing where the water eddies in a nearby stream, who loves her mother in her own complicated way, thinks her sister talks too loud, has a favorite cousin, a favorite season, who excels at dressmaking and knows, inside herself, that she is as intelligent and capable as anyone.”\n  “My work is to give you what I know of my own particular path while allowing you to walk your own”\n The Fire Next Time — James Baldwin  “This is because white Americans have supposed “Europe” and “civilization” to be synonyms — which they are not — and have been distrustful of other standards and other sources of vitality, especially those produced in America itself, and have attempted to behave in all matters as though what was east for Europe was also east for them.”\n Tinkers — Paul Harding  “He resented his resentment because it was a sign of his own limitations of spirit and humility”\n Slaughterhouse-Five — Kurt Vonnegut  “But she did look back, and I love her for that, because it was so human”\n Kindred — Octavia Butler  “I realized that I knew less about loneliness than I had thought — and much less than I would know when he went away”\n The Wind’s Twelve Quarter’s \u0026amp; The Left Hand of Darkness — Ursula K. Le Guin  “To learn which questions are unanswerable, and not to answer them: this skill is most needful in times of stress and darkness”\n  “No, I don’t mean love, when I say patriotism. I mean fear. The fear of the other. And its expressions are political, not poetical: hate, rivalry, aggression. It grows in us, that fear. It grows in us year by year”\n Annihilation, Authority \u0026amp; Acceptance — Jeff VanderMeer  “Because our minds process information almost solely through analogy and categorization, we are often defeated when presented with something that fits no category and lies outside of the realm of our analogies”\n Secular Buddhism — Stephan Batchelor Pachinko — Min Jin Lee  “That to live without forgiveness was a kind of death with breathing and movement”\n Dreams from My Father — Barack Obama  “For me, it’s been a relatively quiet period, less a time of discovery than of consolidation, of doing the things that we tell ourselves we finally must do to grow up”\n The Alchemist — Paulo Coehlo  “The story of one person is the story of everyone”\n *Hillbilly Elegy_ — J.D. Vance*  “The best way to look at this might be to recognize that you probably can’t fix these things. They’ll always be around. But maybe you can put your thumb on the scale a little for the people at the margins.”\n Us vs. Them: The Failure of Globalism — Ian Bremmer  “Social media allows us to follow those we agree with and ignore those we don’t, enabling us to deprive ourselves of opportunities to deepen our thinking and change our minds”\n Bad Feminist — Roxanne Gay Factfulness — Hans Rosling The Best We Could Do — Thi Bui Are You My Mother? — Allison Bechdel The Remains of The Day — Kazuo Ishiguro One Hundred Year’s of Solitude — Gabriel Garcia Marquez *The Forest Unseen_ — David George Haskell*  “Our cultural and scientific memories of what a “normal” forest should look like arose at a peculiar moment in history”\n  Likewise, our naming imposes tidy categories on nature. These may not reflect life’s complicated genealogies and reproductive exchanges.\n  “The snails’ hermaphroditic embrace, seemingly so alien to most humans, is a reminder that sexuality in nature is more malleable and diverse than we might at first suppose”\n Artemis — Andy Weir The Inevitable — Kevin Kelly  This circular expansion of both problems and solutions hides a steady accumulation of small net benefits over time.\n  I think we’ll be surprised by how many of the things we assumed were “natural” for humans are not really natural at all.\n  In other words, science is a method that chiefly expands our ignorance rather than our knowledge.\n When Breath Becomes Air — Paul Kalanthi Little Fires Everywhere — Celeste Ng  “What made someone a mother? Was it biology alone, or was it love?”\n Radical Markets — Eric Posner \u0026amp; Glen Weyl  “Open markets embody the idea that by cooperating as broadly as possible, we can all benefit from each other”\n  “But because no good mechanisms exist for making collective decisions, these aspects of life are often extremely frustrating, leading many people to avoid them when possible”\n  “Failing to recognize data as labor could thus create what Lanier calls “fake unemployment,” where jobs dry up not because humans are not useful but because the valuable inputs they supply are treated as byproducts of entertainment rather than as socially valued work”\n What We Were Promised — Lucy Tan  “Why do our minds fixate on the kinds of love we’re not getting instead of the kinds of love we are? We expect it to be the thing we want it to be. And we’re blind to every other form of it.”\n  “It frightened her, the ease with which she could arrive in a place without any obvious intention”\n White Tears — Hari Kunzru  “There is no clear border between life and non-life. Once you realize that, so much else unravels”\n The Fifth Season — N.K. Jemisin  “Survival doesn’t mean rightness”\n  “It’s a gift if it makes us better. It’s a curse if we let it destroy us. You decide that — not the instructors, or the Guardians, or anyone else.”\n Surely Your Joking Mr Feynman — Richard Feynman  “You have no responsibility to live up to what other people think you ought to accomplish. I have no responsibility to be like they expect me to be. It’s their mistake, not my failing”\n Becoming — Michelle Obama  “As if growing up is finite. As if at some point you become something and that’s the end”\n  “Failure is a feeling long before it becomes an actual result. It’s vulnerability that breeds with self-doubt and then is escalated, often deliberately, by fear”\n  “This is what a control freak learns inside the compressed otherworld of college, maybe above all else: There are simply other ways of being.”\n  “None of this was his fault, but it wasn’t equal, either, and for any woman who lives by the mantra that equality is important, this can be a little confusing”\n  “It was similar to when the Sidwell mom had asked Malia if she feared for her life at tennis practice. What can you do, really, but go out and hit another ball?”\n Killing Commendatore — Haruki Murakami  “Thelonious Monk did not get those unusual chords as a result of logic or theory. He opened his eyes wide, and scooped those chords out from the darkness of his consciousness.”\n  “Namely, to determine to ‘stop thinking’ about something is itself a thought — as long as one follows that path, that something continues to exist. In the end, to stop thinking about something means to stop thinking about stopping thinking.”\n Information, Incentives and Education Policy — Derek A. Neal ","permalink":"https://judahgnewman.netlify.com/writing/books/a-year-in-review-the-books-i-read-in-2018/","tags":["Reflections"],"title":"A Year in Review: The Books I Read in 2018"},{"categories":null,"contents":"Current Project Agent Based Simulations\nResearch Interests  Applied AI and ML Tech regulation The structure of intelligence Climate Change, Inequality / Poverty / Health  Paper Reading A necessary part of producing quality research is reading lots of other research. Any piece of research is not done in isolation but in dialogue with what came before it. It is not only important to read the research but think about it critically and relate it to your own thoughts. I try to read at least one research paper a week, summarize the paper and write some of my own thoughts about it. It is important to read research papers from a diverse set of fields. By reading other research papers we also learn what good research looks like and how to best communicate findings. You can view all of the papers that I am reading and my thoughts about them here\nHere are some specific posts where my thoughts about the paper touch on some more general themes:\n Applying Learnings from other fields Academia and Industry Bayesian View of Research  Previous Research Human Computer Interaction + Machine Learning While machine learning is a very powerful tool, it is currently inaccessible to most people. Current algorithms learn through methods very different than how humans natively learn. To teach a human how to recognize a cat you might explain what a cat is and show the human a couple of photos. To teach the machine you provide it with thousands of photos and tell it whether or not the photo has a cat in it. While the human is able to understand the core qualities of a cat from a few examples, it takes the machine thousands of examples and labels to understand what actually constitutes a cat.\nDuring my undergraduate studies, I had the absolute pleasure of researching supervised by professor Blase Ur. We explored whether humans could teach machines through a more complex and native interaction than just providing labels. Is there potential for humans to teach computers, through explanations for example? We studied this by conducting a human research study where we asked participants to provide labels and explanations for an image classification task. We characterized the different types of information participants provided and how it could be used to train an image classification system. We also did a follow up experiment demonstrating how the explanations could be used to jump start the training process.\nI was first author on our paper that was presented at INTERACT 2019 in Paphos, Cyprus. Here is a link to the paper and my slides for the presentation\n Paper Slides  ","permalink":"https://judahgnewman.netlify.com/research/","tags":null,"title":"Research"},{"categories":null,"contents":"I am a builder, problem solver, and a teammate. I push myself and those around me to think differently, to challenge the status quo and to focus on personal growth. I currently work as a Data Scientist at Change Research where we are improving American democracy through better public opinion insight. I grew up in New York City, went to college in Chicago where I studied computer science and economics and I now live in the Bay Area. I spend a lot of my time playing and training for Ultimate Frisbee. I am an avid cook, mediocre meditator and a so-so journaler.\nResume\nContact Email: judah.newman@gmail.com\nPhone: 917-734-0588\nLocation: Oakland, CA\n","permalink":"https://judahgnewman.netlify.com/about/","tags":null,"title":"Judah Newman"}]