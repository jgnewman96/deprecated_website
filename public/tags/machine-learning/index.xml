<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Welcome</title>
    <link>https://judahgnewman.netlify.com/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on Welcome</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Sep 2020 16:03:09 -0700</lastBuildDate>
    
	<atom:link href="https://judahgnewman.netlify.com/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Reliance on Metrics is a Fundamental Challenge for AI</title>
      <link>https://judahgnewman.netlify.com/writing/papers/2020/reliance_on_metrics/</link>
      <pubDate>Thu, 24 Sep 2020 16:03:09 -0700</pubDate>
      
      <guid>https://judahgnewman.netlify.com/writing/papers/2020/reliance_on_metrics/</guid>
      <description>By Rachel Thomas and David Uminsky
Paper Motivation Optimizing a metric is central to current AI approaches. Metrics can lead to a myopic focus on short term goals leading to unexpected negative consequences. This reliance on metrics is a fundamental challenge of modern AI
Paper Contribution The paper reviews current examples of metrics gone wrong. The authors find the following problematic aspects of metrics
 Any metric is just a proxy for what we really care about Metrics can, and will be gamed Metrics tend to overemphasize short-term concerns Many online metrics are gathered in highly addictive environments  The authors create a framework to mitigate the harms of overemphasizing metrics by suggesting</description>
    </item>
    
    <item>
      <title>A Framework for Understanding Unintended Consequences of Machine Learning</title>
      <link>https://judahgnewman.netlify.com/writing/papers/2020/ml_harms_framework/</link>
      <pubDate>Sun, 13 Sep 2020 15:20:03 -0700</pubDate>
      
      <guid>https://judahgnewman.netlify.com/writing/papers/2020/ml_harms_framework/</guid>
      <description>By Harini Suresh and John Guttag
Paper Motivation We need a better understanding of the potential unwanted consequences from machine learning. We often hear about “biased data“ but this term has become a catch all that prevents nuanced understanding.
Paper Contribution The authors propose a framework that partitions harms from machine learning into six different categories. The paper covers what contributes to each harm and how to remedy them. The authors argue it is critical that the solutions we develop are from application specific understandings rather than abstract notions of fairness.</description>
    </item>
    
    <item>
      <title>Equality of Opportunity in Supervised Learning</title>
      <link>https://judahgnewman.netlify.com/writing/papers/2020/equalized_odds_supervised_learning/</link>
      <pubDate>Fri, 28 Aug 2020 11:23:08 -0700</pubDate>
      
      <guid>https://judahgnewman.netlify.com/writing/papers/2020/equalized_odds_supervised_learning/</guid>
      <description>Paper Motivation Model builders may want to ensure their model is fair with regard to a protected attribute. (E.g. Wanting to make sure our model is not discriminating against people because of their race). Previous research has shown that quantifying a model&amp;rsquo;s fairness is not straight forward. Even if the protected attribute is not an input to the model it is still possible that the model is discriminatory.
Paper Contribution The authors present a methodology of adjusting a learned predictor to remove discrimination according to their definition of discrimination.</description>
    </item>
    
    <item>
      <title>A Fair Algorithm is not Sufficient</title>
      <link>https://judahgnewman.netlify.com/writing/research/fair_ai_is_not_enough/</link>
      <pubDate>Sun, 19 Jul 2020 10:52:00 -0500</pubDate>
      
      <guid>https://judahgnewman.netlify.com/writing/research/fair_ai_is_not_enough/</guid>
      <description>For the past two weeks, I have been working through Me &amp;amp; White Supremacy, Layla Saed&amp;rsquo;s month long interrogation of white supremacy. I am reading the book, journaling each day and then meeting with a group of co-workers weekly to discuss our experience. While journaling, I noticed my tendency to gravitate toward simple solutions. The chapters about tone policing and cultural appropriation were quite challenged. I was looking for specific guidelines about what my behavior should look like.</description>
    </item>
    
    <item>
      <title>150 Succesful Machine Learning Models: 6 Lessons Learned at Booking.com</title>
      <link>https://judahgnewman.netlify.com/writing/papers/2020/ml_at_booking/</link>
      <pubDate>Mon, 18 May 2020 16:21:18 -0500</pubDate>
      
      <guid>https://judahgnewman.netlify.com/writing/papers/2020/ml_at_booking/</guid>
      <description>By Lucas Bernardi, Tehmis Mavridis, Pablo Estevez
Summary This paper from Booking is not focused on algorithmic advances but on how the company has derived value from using Machine Learning. The authors argue that more papers similar to this one need to be published.
The authors highlight some parts of the Machine Learning process that are specific to booking. They do this to explain why there situation is different then other technology firms.</description>
    </item>
    
    <item>
      <title>Building Models and the Scientific Method</title>
      <link>https://judahgnewman.netlify.com/writing/personal/model_building/</link>
      <pubDate>Sat, 21 Dec 2019 17:18:01 -0600</pubDate>
      
      <guid>https://judahgnewman.netlify.com/writing/personal/model_building/</guid>
      <description>In the past year there has been a large focus on what is wrong with the fields of Artificial Intelligence and Machine Learning. Coverage has focused on limitations of current deep learning and machine learning systems. Pieces have highlighted the dangers of implementing systems without understanding their implications or who they are going to impact. These pieces have highlighted areas where these fields need to grow. Machine Learning is still a relatively young field that needs to grow and learn.</description>
    </item>
    
    <item>
      <title>ODSC Keynotes</title>
      <link>https://judahgnewman.netlify.com/writing/talks/odsc/keynotes_odsc/</link>
      <pubDate>Thu, 31 Oct 2019 08:22:50 -0700</pubDate>
      
      <guid>https://judahgnewman.netlify.com/writing/talks/odsc/keynotes_odsc/</guid>
      <description>AI Lifecycle Model Management: Monitoring for Risk, Bias and Fairness Talk given by Sepideh Seifzadeh
The first keynote was about how can we as data scientists monitor for risk, bias and fairness. The speaker motivated the tech by saying that AI and data regulation is coming. There is more data being collected, Machine Learning capabilities have increased and the cost of storage is now a lot lower.
The speaker highlighted how we do not have defined mechanisms in place to handle when their are problems with AI.</description>
    </item>
    
    <item>
      <title>Tackling Climate Change with Machine Learning</title>
      <link>https://judahgnewman.netlify.com/writing/talks/odsc/climate_change_odsc/</link>
      <pubDate>Wed, 30 Oct 2019 15:58:02 -0700</pubDate>
      
      <guid>https://judahgnewman.netlify.com/writing/talks/odsc/climate_change_odsc/</guid>
      <description>Talk given by David Rolnick University of Pennsylvania and founder of Climate Change AI
Motivation Climate change as an important issues does not need that much motivation. We have all seen what has been happening California recently with the wild fires. There has been increasingly severe impacts from human influence on our climate system.
One thing that is important to recognize about climate change is that different parts of the world are impacted differently.</description>
    </item>
    
    <item>
      <title>Towards a Blend of Machine Learning and Microeconomics</title>
      <link>https://judahgnewman.netlify.com/writing/talks/odsc/ml_microecon/</link>
      <pubDate>Wed, 30 Oct 2019 09:51:58 -0700</pubDate>
      
      <guid>https://judahgnewman.netlify.com/writing/talks/odsc/ml_microecon/</guid>
      <description>Talk given by Michael I Jordan
What is Machine Learning? The talk started by Professor Jordan defining what Machine Learning is. He said that machine learning is not some new field of research but it is rather the engineering extension of statistics. He calls back to how there was chemistry for a long time before there was chemical engineering. There was physics and electrical theory before there was electrical engineering.</description>
    </item>
    
    <item>
      <title>ML Flow: Platform for Complete Machine Learning Life Cycle</title>
      <link>https://judahgnewman.netlify.com/writing/talks/odsc/mlflow_odsc/</link>
      <pubDate>Tue, 29 Oct 2019 13:53:00 -0700</pubDate>
      
      <guid>https://judahgnewman.netlify.com/writing/talks/odsc/mlflow_odsc/</guid>
      <description>Talk given by Jules Damji: git hub repo
Motivation Machine learning development is complex. This is not due to the underlying theory but rather all of the different stages in development. Each stage has its own requirements and goals. We have developed tools and best practices for this for traditional software development, but not Machine Learning.
Here are the differences between traditional software and machine learning
Traditional Software:
 Meet a functional specification Quality depends on the code One software stack  Machine Learning</description>
    </item>
    
    <item>
      <title>How do you Solve for Biased Data?</title>
      <link>https://judahgnewman.netlify.com/writing/personal/biased_data_hawk/</link>
      <pubDate>Fri, 25 Oct 2019 16:44:33 -0700</pubDate>
      
      <guid>https://judahgnewman.netlify.com/writing/personal/biased_data_hawk/</guid>
      <description>My friend Hawkins recently sent me the following question:
 How do you work around biased data sets to produce non discriminatory results in ML?
 I ended up writing him a fair amount while I wan on a plane. My response to him is replicated here in full.
 Hey Hawk,
Thanks for asking this question. It is a really important one and it is always nice to think about these problems in a fair amount of depth.</description>
    </item>
    
    <item>
      <title>Machine Learning Community of Practice: Nielsen</title>
      <link>https://judahgnewman.netlify.com/writing/industry/nielsen/ml_cop/</link>
      <pubDate>Fri, 25 Oct 2019 13:40:46 -0700</pubDate>
      
      <guid>https://judahgnewman.netlify.com/writing/industry/nielsen/ml_cop/</guid>
      <description>Project Description The Machine Learning community of practice was a one stop shop for all things machine learning at Nielsen. The goal of the organization was to provide resources related to machine learning. We recorded all of the different projects being worked on related to machine learning, who were the different machine learning experts in different fields and maintained general learning resources.
We hosted monthly webinars that highlighted different machine learning projects around the organization and provided a forum for practitioners to present their work and get feedback.</description>
    </item>
    
    <item>
      <title>Rotation 2: Bayesian Inference for Ratings</title>
      <link>https://judahgnewman.netlify.com/writing/industry/nielsen/rotation_2/</link>
      <pubDate>Fri, 25 Oct 2019 13:40:46 -0700</pubDate>
      
      <guid>https://judahgnewman.netlify.com/writing/industry/nielsen/rotation_2/</guid>
      <description>Project Description Nielsen is best known for their linear television ratings which estimate the number of people who were watching a channel at any time. This information is estimated using a representative panel of households all across the United States. Households in Nielsen&amp;rsquo;s Panel report all of the television that they watch. Each household&amp;rsquo;s viewing is weight based on its demographics to ensure that the viewing is representative of the population.</description>
    </item>
    
    <item>
      <title>Model Cards for Model Reporting</title>
      <link>https://judahgnewman.netlify.com/writing/papers/2019/paper-model-cards-for-model-reporting/</link>
      <pubDate>Sat, 05 Oct 2019 22:08:21 +0000</pubDate>
      
      <guid>https://judahgnewman.netlify.com/writing/papers/2019/paper-model-cards-for-model-reporting/</guid>
      <description>By Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru
Summary The authors propose that machine learning models should be accompanied by documentation known as &amp;ldquo;model cards&amp;rdquo;. A model card should outline who built the model, what the purpose of the model is, how it was validated and what it&amp;rsquo;s performance is according to many different metrics. The authors focus on models that are making decisions about humans.</description>
    </item>
    
    <item>
      <title>Green AI</title>
      <link>https://judahgnewman.netlify.com/writing/papers/2019/green-ai/</link>
      <pubDate>Mon, 30 Sep 2019 03:40:49 +0000</pubDate>
      
      <guid>https://judahgnewman.netlify.com/writing/papers/2019/green-ai/</guid>
      <description>By Roy Schwartz, Jesse Dodge, Noah A. Smith, Oren Etzioni
Summary The authors of this post argue that a large percentage of machine learning research focuses on Red AI and make a call for more research to be centered on Green AI. The authors define Red AI and Green AI as follows:  Red AI: research that makes small improvements on the state of the art by leveraging more computation resources Green AI: research that focuses on diminishing the amount of computation necessary to reach a result.</description>
    </item>
    
  </channel>
</rss>